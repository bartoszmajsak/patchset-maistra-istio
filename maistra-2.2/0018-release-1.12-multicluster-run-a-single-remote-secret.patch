From 25209af8b7baedc36cbbddf5e64b600d7afb9fd1 Mon Sep 17 00:00:00 2001
From: Istio Automation <istio-testing-bot@google.com>
Date: Fri, 5 Nov 2021 04:08:15 -0700
Subject: [release-1.12] multicluster: run a single remote secrets controller
 (#35842)

* multicluster: run a single remote secrets controller

Change-Id: I90f21a478bf4b507de8658d06b8abdd9dec1a73e

* fix tests

Change-Id: I5819c5c1324497c71c7a40bd586c06d07623eda0

* fmt

Change-Id: I6da4f7c4457afe02a98bab384bea95a46683beb8

* lnt

Change-Id: I4829ec203910ceccbfadbca25b9c4231f1301409

* ignore secrets that would break the local cluster

Change-Id: I1d8eeae648fedf8e75ee47aab5e8293d83e407ee

* debug dump

Change-Id: Ib0a9c3b5ca904d1fa79981fa3ab123d085c204f7

* move credential secrets init out of startfunc

Change-Id: I05bf9f77a7bd7f78ce8e02024f4d80d9ce91b5d9

* rename secrets to credentials

Change-Id: I582375506aa4830ecb902796502d7066ab4c48db

* register kube registry handler

Change-Id: If482e59756e3dc801c32059b4eeab4608b2071d9

* move metric to top level controller

Change-Id: Ib5a1051d0cdffa84b1e02ad2bec92559472311ec

* rename secretcontroller to remotesecrets

Change-Id: I3fcdfac898dca2d4ead3e3d50385c1f1437ca27d

* fix rename

Change-Id: I78671ee7a9b5407d215c25d04e4dd058f497e172

* fmt

Change-Id: I7c01bb6f69656534683945b5a847bb01f1e8fbe9

* fumpt

Change-Id: I8a0fc082d2abfae919ad67a7258429017f363787

* bad refactor

Change-Id: I5b563eb72ae4e66d1c739981c3a2f113fe42e86c

* remove local cluster init from bootstrap

Change-Id: Ib31f6fcb71d8f115f916eec99926bdd43b3252ab

* cleanup stopCh and clusterID params

Change-Id: Ib5b9aa9831b0e59574d6d5f2791a39775909d428

* renames and fix cyclic import

Change-Id: Ib8a94d85d0fe3d96d8dcce8d4796a59f265ea8a1

* init first

Change-Id: I10d711e2faaa922f1abd93fe9a105c70391e1f1e

* fix uts

Change-Id: I28bdbe199cd41610b6cb27c9ecec479801898311

* gen

Change-Id: Ic19b5db47f3afda107f32993b17771f6eb146d41

* lint

Change-Id: I8d614b5c330a487a6b5850f5b5853fc13d111d78

* address more comments

Change-Id: I96d838dc38fe5137f51385d2a904c52ca2fad0b2

* debug log

Change-Id: I1d4cf19fe78c1fe3ef8837e888017aa0b6823ebf

* debug log 2

Change-Id: I3f5927741e5d5c4f74cfbf4cb24c4efdad490f8f

* dont trigger Run from aggregate registry

Change-Id: I288dfa20affa29c45cf3678c264b40db04418258

* more debug log

Change-Id: Idb469492445361fde640e2ce7ded9320eefac9fb

* more debug log

Change-Id: I18f5c1bad01495db5e8b8ec0eb1ee7ab028876e7

* fix informer init race

Change-Id: I9accf3655f28dfc0aff1684946c832d049f96c0a

* cleanup debug logs

Change-Id: I26d6ac5e1c349bf86d9f5e6896f72cd691ac543c

* address nmittler comments

Change-Id: Ida82a787b22fb8614a1030196071fb08280268c9

* fix ut

Change-Id: Id9630475202290f86bd90bf50be99e458e104f7d

* deffered Run on service registry via aggregate

Change-Id: Iadd0a032bf4ef24bf2b441188b629ed2a653220c

* add unit test and use AddAndRun

Change-Id: I00f3438856c7915b5c1bfc858ab6d296409373da

* Update pilot/pkg/bootstrap/server.go

Co-authored-by: John Howard <howardjohn@google.com>
Change-Id: I0bce1e8ba3815b57d89c0b81fab64e2214cfe572

* debug cross cluster workload entry

Change-Id: I6347b31cf23c7cbdfdd1c19b29789d0d7b9a91c3

* fix vms

Change-Id: I7bed4594b12593439ac68abd74962fc27d930b37

Co-authored-by: Steven Landow <landow@google.com>
Co-authored-by: John Howard <howardjohn@google.com>
---
 .../analyzers/multicluster/meshnetworks.go    |   4 +-
 istioctl/pkg/multicluster/remote_secret.go    |   4 +-
 .../pkg/multicluster/remote_secret_test.go    |   8 +-
 pilot/pkg/bootstrap/server.go                 |  75 +++---
 pilot/pkg/bootstrap/servicecontroller.go      |  31 +--
 .../kube/leak_test.go                         |   0
 .../kube/multicluster.go                      | 104 +++-----
 .../{secrets => credentials}/kube/secrets.go  |  24 +-
 .../kube/secrets_test.go                      |  26 +-
 pilot/pkg/{secrets => credentials}/model.go   |   2 +-
 .../serviceregistry/aggregate/controller.go   |  46 +++-
 .../aggregate/controller_test.go              |  69 ++++-
 .../kube/controller/controller.go             |   4 +-
 .../kube/controller/multicluster.go           |  82 ++----
 .../kube/controller/multicluster_test.go      |  40 ++-
 pilot/pkg/xds/fake.go                         |  10 +-
 pilot/pkg/xds/sds.go                          |  24 +-
 pilot/pkg/xds/sds_test.go                     |  14 +-
 .../secretcontroller.go                       | 252 +++++++++++-------
 .../secretcontroller_test.go                  |  20 +-
 .../framework/components/istio/operator.go    |   1 +
 21 files changed, 478 insertions(+), 362 deletions(-)
 rename pilot/pkg/{secrets => credentials}/kube/leak_test.go (100%)
 rename pilot/pkg/{secrets => credentials}/kube/multicluster.go (57%)
 rename pilot/pkg/{secrets => credentials}/kube/secrets.go (92%)
 rename pilot/pkg/{secrets => credentials}/kube/secrets_test.go (93%)
 rename pilot/pkg/{secrets => credentials}/model.go (98%)
 rename pkg/kube/{secretcontroller => multicluster}/secretcontroller.go (68%)
 rename pkg/kube/{secretcontroller => multicluster}/secretcontroller_test.go (92%)

diff --git a/galley/pkg/config/analysis/analyzers/multicluster/meshnetworks.go b/galley/pkg/config/analysis/analyzers/multicluster/meshnetworks.go
index 991d355384..a2059dfd14 100644
--- a/galley/pkg/config/analysis/analyzers/multicluster/meshnetworks.go
+++ b/galley/pkg/config/analysis/analyzers/multicluster/meshnetworks.go
@@ -27,7 +27,7 @@
 	"istio.io/istio/pkg/config/resource"
 	"istio.io/istio/pkg/config/schema/collection"
 	"istio.io/istio/pkg/config/schema/collections"
-	"istio.io/istio/pkg/kube/secretcontroller"
+	"istio.io/istio/pkg/kube/multicluster"
 )
 
 // MeshNetworksAnalyzer validates MeshNetworks configuration in multi-cluster.
@@ -57,7 +57,7 @@ func (s *MeshNetworksAnalyzer) Metadata() analysis.Metadata {
 // Analyze implements Analyzer
 func (s *MeshNetworksAnalyzer) Analyze(c analysis.Context) {
 	c.ForEach(collections.K8SCoreV1Secrets.Name(), func(r *resource.Instance) bool {
-		if r.Metadata.Labels[secretcontroller.MultiClusterSecretLabel] == "true" {
+		if r.Metadata.Labels[multicluster.MultiClusterSecretLabel] == "true" {
 			s := r.Message.(*v1.Secret)
 			for c := range s.Data {
 				serviceRegistries = append(serviceRegistries, provider.ID(c))
diff --git a/istioctl/pkg/multicluster/remote_secret.go b/istioctl/pkg/multicluster/remote_secret.go
index d753ad8ac4..dbb445b1e1 100644
--- a/istioctl/pkg/multicluster/remote_secret.go
+++ b/istioctl/pkg/multicluster/remote_secret.go
@@ -42,7 +42,7 @@
 	"istio.io/istio/pkg/config/constants"
 	"istio.io/istio/pkg/config/labels"
 	"istio.io/istio/pkg/kube"
-	"istio.io/istio/pkg/kube/secretcontroller"
+	"istio.io/istio/pkg/kube/multicluster"
 )
 
 var (
@@ -141,7 +141,7 @@ func createRemoteServiceAccountSecret(kubeconfig *api.Config, clusterName, secNa
 				clusterNameAnnotationKey: clusterName,
 			},
 			Labels: map[string]string{
-				secretcontroller.MultiClusterSecretLabel: "true",
+				multicluster.MultiClusterSecretLabel: "true",
 			},
 		},
 		Data: map[string][]byte{
diff --git a/istioctl/pkg/multicluster/remote_secret_test.go b/istioctl/pkg/multicluster/remote_secret_test.go
index 1c285f9836..e40d69ca28 100644
--- a/istioctl/pkg/multicluster/remote_secret_test.go
+++ b/istioctl/pkg/multicluster/remote_secret_test.go
@@ -33,7 +33,7 @@
 
 	"istio.io/istio/operator/pkg/object"
 	"istio.io/istio/pkg/kube"
-	"istio.io/istio/pkg/kube/secretcontroller"
+	"istio.io/istio/pkg/kube/multicluster"
 	"istio.io/istio/pkg/test"
 	"istio.io/istio/pkg/test/env"
 )
@@ -652,7 +652,7 @@ func TestCreateRemoteKubeconfig(t *testing.T) {
 						clusterNameAnnotationKey: fakeClusterName,
 					},
 					Labels: map[string]string{
-						secretcontroller.MultiClusterSecretLabel: "true",
+						multicluster.MultiClusterSecretLabel: "true",
 					},
 				},
 				Data: map[string][]byte{
@@ -782,7 +782,7 @@ func TestCreateRemoteSecretFromPlugin(t *testing.T) {
 						clusterNameAnnotationKey: fakeClusterName,
 					},
 					Labels: map[string]string{
-						secretcontroller.MultiClusterSecretLabel: "true",
+						multicluster.MultiClusterSecretLabel: "true",
 					},
 				},
 				Data: map[string][]byte{
@@ -809,7 +809,7 @@ func TestCreateRemoteSecretFromPlugin(t *testing.T) {
 						clusterNameAnnotationKey: fakeClusterName,
 					},
 					Labels: map[string]string{
-						secretcontroller.MultiClusterSecretLabel: "true",
+						multicluster.MultiClusterSecretLabel: "true",
 					},
 				},
 				Data: map[string][]byte{
diff --git a/pilot/pkg/bootstrap/server.go b/pilot/pkg/bootstrap/server.go
index 54f080f8c4..15cfcaa186 100644
--- a/pilot/pkg/bootstrap/server.go
+++ b/pilot/pkg/bootstrap/server.go
@@ -41,12 +41,12 @@
 	"k8s.io/client-go/tools/cache"
 
 	"istio.io/api/security/v1beta1"
+	kubecredentials "istio.io/istio/pilot/pkg/credentials/kube"
 	"istio.io/istio/pilot/pkg/features"
 	istiogrpc "istio.io/istio/pilot/pkg/grpc"
 	"istio.io/istio/pilot/pkg/keycertbundle"
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/networking/plugin"
-	kubesecrets "istio.io/istio/pilot/pkg/secrets/kube"
 	"istio.io/istio/pilot/pkg/server"
 	"istio.io/istio/pilot/pkg/serviceregistry/aggregate"
 	kubecontroller "istio.io/istio/pilot/pkg/serviceregistry/kube/controller"
@@ -65,6 +65,7 @@
 	istiokeepalive "istio.io/istio/pkg/keepalive"
 	kubelib "istio.io/istio/pkg/kube"
 	"istio.io/istio/pkg/kube/inject"
+	"istio.io/istio/pkg/kube/multicluster"
 	"istio.io/istio/pkg/security"
 	"istio.io/istio/pkg/spiffe"
 	"istio.io/istio/security/pkg/k8s/chiron"
@@ -116,8 +117,7 @@ type Server struct {
 
 	kubeClient kubelib.Client
 
-	multicluster      *kubecontroller.Multicluster
-	secretsController *kubesecrets.Multicluster
+	multiclusterController *multicluster.Controller
 
 	configController  model.ConfigStoreCache
 	ConfigStores      []model.ConfigStoreCache
@@ -314,7 +314,7 @@ func NewServer(args *PilotArgs, initFuncs ...func(*Server)) (*Server, error) {
 
 	s.initDiscoveryService(args)
 
-	s.initSDSServer(args)
+	s.initSDSServer()
 
 	// Notice that the order of authenticators matters, since at runtime
 	// authenticators are activated sequentially and the first successful attempt
@@ -335,7 +335,7 @@ func NewServer(args *PilotArgs, initFuncs ...func(*Server)) (*Server, error) {
 	// The k8s JWT authenticator requires the multicluster registry to be initialized,
 	// so we build it later.
 	authenticators = append(authenticators,
-		kubeauth.NewKubeJWTAuthenticator(s.environment.Watcher, s.kubeClient, s.clusterID, s.multicluster.GetRemoteKubeClient, features.JwtPolicy))
+		kubeauth.NewKubeJWTAuthenticator(s.environment.Watcher, s.kubeClient, s.clusterID, s.multiclusterController.GetRemoteKubeClient, features.JwtPolicy))
 	if features.XDSAuth {
 		s.XDSServer.Authenticators = authenticators
 	}
@@ -512,32 +512,30 @@ func (s *Server) WaitUntilCompletion() {
 }
 
 // initSDSServer starts the SDS server
-func (s *Server) initSDSServer(args *PilotArgs) {
-	if s.kubeClient != nil {
-		if !features.EnableXDSIdentityCheck {
-			// Make sure we have security
-			log.Warnf("skipping Kubernetes credential reader; PILOT_ENABLE_XDS_IDENTITY_CHECK must be set to true for this feature.")
-		} else {
-			s.addStartFunc(func(stop <-chan struct{}) error {
-				sc := kubesecrets.NewMulticluster(s.kubeClient, s.clusterID, args.RegistryOptions.ClusterRegistriesNamespace, stop)
-				sc.AddEventHandler(func(name, namespace string) {
-					s.XDSServer.ConfigUpdate(&model.PushRequest{
-						Full: false,
-						ConfigsUpdated: map[model.ConfigKey]struct{}{
-							{
-								Kind:      gvk.Secret,
-								Name:      name,
-								Namespace: namespace,
-							}: {},
-						},
-						Reason: []model.TriggerReason{model.SecretTrigger},
-					})
-				})
-				s.XDSServer.Generators[v3.SecretType] = xds.NewSecretGen(sc, s.XDSServer.Cache, s.clusterID)
-				s.secretsController = sc
-				return nil
+func (s *Server) initSDSServer() {
+	if s.kubeClient == nil {
+		return
+	}
+	if !features.EnableXDSIdentityCheck {
+		// Make sure we have security
+		log.Warnf("skipping Kubernetes credential reader; PILOT_ENABLE_XDS_IDENTITY_CHECK must be set to true for this feature.")
+	} else {
+		creds := kubecredentials.NewMulticluster(s.clusterID)
+		creds.AddEventHandler(func(name string, namespace string) {
+			s.XDSServer.ConfigUpdate(&model.PushRequest{
+				Full: false,
+				ConfigsUpdated: map[model.ConfigKey]struct{}{
+					{
+						Kind:      gvk.Secret,
+						Name:      name,
+						Namespace: namespace,
+					}: {},
+				},
+				Reason: []model.TriggerReason{model.SecretTrigger},
 			})
-		}
+		})
+		s.XDSServer.Generators[v3.SecretType] = xds.NewSecretGen(creds, s.XDSServer.Cache, s.clusterID)
+		s.multiclusterController.AddHandler(creds)
 	}
 }
 
@@ -848,10 +846,7 @@ func (s *Server) pushContextReady(expected int64) bool {
 
 // cachesSynced checks whether caches have been synced.
 func (s *Server) cachesSynced() bool {
-	if s.secretsController != nil && !s.secretsController.HasSynced() {
-		return false
-	}
-	if s.multicluster != nil && !s.multicluster.HasSynced() {
+	if s.multiclusterController != nil && !s.multiclusterController.HasSynced() {
 		return false
 	}
 	if !s.ServiceController().HasSynced() {
@@ -1060,6 +1055,7 @@ func (s *Server) getIstiodCertificate(*tls.ClientHelloInfo) (*tls.Certificate, e
 // initControllers initializes the controllers.
 func (s *Server) initControllers(args *PilotArgs) error {
 	log.Info("initializing controllers")
+	s.initMulticluster(args)
 	// Certificate controller is created before MCP controller in case MCP server pod
 	// waits to mount a certificate to be provisioned by the certificate controller.
 	if err := s.initCertController(args); err != nil {
@@ -1074,6 +1070,17 @@ func (s *Server) initControllers(args *PilotArgs) error {
 	return nil
 }
 
+func (s *Server) initMulticluster(args *PilotArgs) {
+	if s.kubeClient == nil {
+		return
+	}
+	s.multiclusterController = multicluster.NewController(s.kubeClient, args.Namespace, s.clusterID)
+	s.XDSServer.ListRemoteClusters = s.multiclusterController.ListRemoteClusters
+	s.addStartFunc(func(stop <-chan struct{}) error {
+		return s.multiclusterController.Run(stop)
+	})
+}
+
 // maybeCreateCA creates and initializes CA Key if needed.
 func (s *Server) maybeCreateCA(caOpts *caOptions) error {
 	// CA signing certificate must be created only if CA is enabled.
diff --git a/pilot/pkg/bootstrap/servicecontroller.go b/pilot/pkg/bootstrap/servicecontroller.go
index 4eb44e65d6..3c35387b61 100644
--- a/pilot/pkg/bootstrap/servicecontroller.go
+++ b/pilot/pkg/bootstrap/servicecontroller.go
@@ -25,7 +25,6 @@
 	"istio.io/istio/pilot/pkg/serviceregistry/provider"
 	"istio.io/istio/pilot/pkg/serviceregistry/serviceentry"
 	"istio.io/istio/pkg/config/host"
-	"istio.io/istio/pkg/kube/secretcontroller"
 	"istio.io/pkg/log"
 )
 
@@ -83,7 +82,7 @@ func (s *Server) initKubeRegistry(args *PilotArgs) (err error) {
 	args.RegistryOptions.KubeOptions.SystemNamespace = args.Namespace
 	args.RegistryOptions.KubeOptions.MeshServiceController = s.ServiceController()
 
-	mc := kubecontroller.NewMulticluster(args.PodName,
+	s.multiclusterController.AddHandler(kubecontroller.NewMulticluster(args.PodName,
 		s.kubeClient,
 		args.RegistryOptions.ClusterRegistriesNamespace,
 		args.RegistryOptions.KubeOptions,
@@ -92,34 +91,8 @@ func (s *Server) initKubeRegistry(args *PilotArgs) (err error) {
 		args.Revision,
 		s.shouldStartNsController(),
 		s.environment.ClusterLocal(),
-		s.server)
+		s.server))
 
-	// initialize the "main" cluster registry before starting controllers for remote clusters
-	s.addStartFunc(func(stop <-chan struct{}) error {
-		writableStop := make(chan struct{})
-		go func() {
-			<-stop
-			close(writableStop)
-		}()
-		if err := mc.AddMemberCluster(args.RegistryOptions.KubeOptions.ClusterID, &secretcontroller.Cluster{
-			Client: s.kubeClient,
-			Stop:   writableStop,
-		}); err != nil {
-			return fmt.Errorf("failed initializing registry for %s: %v", args.RegistryOptions.KubeOptions.ClusterID, err)
-		}
-		return nil
-	})
-
-	// Start the multicluster controller and wait for it to shutdown before exiting the server.
-	s.addTerminatingStartFunc(mc.Run)
-
-	// start remote cluster controllers
-	s.addStartFunc(func(stop <-chan struct{}) error {
-		s.XDSServer.ListRemoteClusters = mc.InitSecretController(stop).ListRemoteClusters
-		return nil
-	})
-
-	s.multicluster = mc
 	return
 }
 
diff --git a/pilot/pkg/secrets/kube/leak_test.go b/pilot/pkg/credentials/kube/leak_test.go
similarity index 100%
rename from pilot/pkg/secrets/kube/leak_test.go
rename to pilot/pkg/credentials/kube/leak_test.go
diff --git a/pilot/pkg/secrets/kube/multicluster.go b/pilot/pkg/credentials/kube/multicluster.go
similarity index 57%
rename from pilot/pkg/secrets/kube/multicluster.go
rename to pilot/pkg/credentials/kube/multicluster.go
index 49b29ddd99..c4b7adcde3 100644
--- a/pilot/pkg/secrets/kube/multicluster.go
+++ b/pilot/pkg/credentials/kube/multicluster.go
@@ -17,104 +17,69 @@
 import (
 	"fmt"
 	"sync"
-	"time"
 
-	"istio.io/istio/pilot/pkg/secrets"
+	"istio.io/istio/pilot/pkg/credentials"
 	"istio.io/istio/pkg/cluster"
-	"istio.io/istio/pkg/kube"
-	"istio.io/istio/pkg/kube/secretcontroller"
+	"istio.io/istio/pkg/kube/multicluster"
 	"istio.io/pkg/log"
-	"istio.io/pkg/monitoring"
 )
 
+type eventHandler func(name string, namespace string)
+
 // Multicluster structure holds the remote kube Controllers and multicluster specific attributes.
 type Multicluster struct {
-	remoteKubeControllers map[cluster.ID]*SecretsController
+	remoteKubeControllers map[cluster.ID]*CredentialsController
 	m                     sync.Mutex // protects remoteKubeControllers
-	secretController      *secretcontroller.Controller
 	localCluster          cluster.ID
-	stop                  <-chan struct{}
+	eventHandlers         []eventHandler
 }
 
-var _ secrets.MulticlusterController = &Multicluster{}
-
-var (
-	clusterType = monitoring.MustCreateLabel("cluster_type")
-
-	clustersCount = monitoring.NewGauge(
-		"istiod_managed_clusters",
-		"Number of clusters managed by istiod",
-		monitoring.WithLabels(clusterType),
-	)
-
-	localClusters  = clustersCount.With(clusterType.Value("local"))
-	remoteClusters = clustersCount.With(clusterType.Value("remote"))
-)
-
-func init() {
-	monitoring.MustRegister(clustersCount)
-}
+var _ credentials.MulticlusterController = &Multicluster{}
 
-func NewMulticluster(client kube.Client, localCluster cluster.ID, secretNamespace string, stop <-chan struct{}) *Multicluster {
+func NewMulticluster(localCluster cluster.ID) *Multicluster {
 	m := &Multicluster{
-		remoteKubeControllers: map[cluster.ID]*SecretsController{},
+		remoteKubeControllers: map[cluster.ID]*CredentialsController{},
 		localCluster:          localCluster,
-		stop:                  stop,
 	}
-	// init gauges
-	localClusters.Record(1.0)
-	remoteClusters.Record(0.0)
-
-	// Add the local cluster
-	m.addMemberCluster(client, localCluster)
-	sc := secretcontroller.StartSecretController(client,
-		func(k cluster.ID, c *secretcontroller.Cluster) error {
-			m.addMemberCluster(c.Client, k)
-			return nil
-		},
-		func(k cluster.ID, c *secretcontroller.Cluster) error {
-			m.updateMemberCluster(c.Client, k)
-			return nil
-		},
-		func(k cluster.ID) error { m.deleteMemberCluster(k); return nil },
-		secretNamespace,
-		time.Millisecond*100,
-		stop)
-	m.secretController = sc
-	return m
-}
 
-func (m *Multicluster) HasSynced() bool {
-	return m.secretController.HasSynced()
+	return m
 }
 
-func (m *Multicluster) addMemberCluster(clients kube.Client, key cluster.ID) {
-	log.Infof("initializing Kubernetes credential reader for cluster %v", key)
-	sc := NewSecretsController(clients, key)
+func (m *Multicluster) ClusterAdded(cluster *multicluster.Cluster, _ <-chan struct{}) error {
+	log.Infof("initializing Kubernetes credential reader for cluster %v", cluster.ID)
+	sc := NewCredentialsController(cluster.Client, cluster.ID)
 	m.m.Lock()
-	m.remoteKubeControllers[key] = sc
-	remoteClusters.Record(float64(len(m.remoteKubeControllers) - 1))
+	m.remoteKubeControllers[cluster.ID] = sc
+	for _, onCredential := range m.eventHandlers {
+		m.remoteKubeControllers[cluster.ID].AddEventHandler(onCredential)
+	}
 	m.m.Unlock()
+	return nil
 }
 
-func (m *Multicluster) updateMemberCluster(clients kube.Client, key cluster.ID) {
-	m.deleteMemberCluster(key)
-	m.addMemberCluster(clients, key)
+func (m *Multicluster) ClusterUpdated(cluster *multicluster.Cluster, stop <-chan struct{}) error {
+	if err := m.ClusterDeleted(cluster.ID); err != nil {
+		return err
+	}
+	if err := m.ClusterAdded(cluster, stop); err != nil {
+		return err
+	}
+	return nil
 }
 
-func (m *Multicluster) deleteMemberCluster(key cluster.ID) {
+func (m *Multicluster) ClusterDeleted(key cluster.ID) error {
 	m.m.Lock()
 	delete(m.remoteKubeControllers, key)
-	remoteClusters.Record(float64(len(m.remoteKubeControllers) - 1))
 	m.m.Unlock()
+	return nil
 }
 
-func (m *Multicluster) ForCluster(clusterID cluster.ID) (secrets.Controller, error) {
+func (m *Multicluster) ForCluster(clusterID cluster.ID) (credentials.Controller, error) {
 	if _, f := m.remoteKubeControllers[clusterID]; !f {
 		return nil, fmt.Errorf("cluster %v is not configured", clusterID)
 	}
 	agg := &AggregateController{}
-	agg.controllers = []*SecretsController{}
+	agg.controllers = []*CredentialsController{}
 
 	if clusterID != m.localCluster {
 		// If the request cluster is not the local cluster, we will append it and use it for auth
@@ -129,7 +94,8 @@ func (m *Multicluster) ForCluster(clusterID cluster.ID) (secrets.Controller, err
 	return agg, nil
 }
 
-func (m *Multicluster) AddEventHandler(f func(name string, namespace string)) {
+func (m *Multicluster) AddEventHandler(f eventHandler) {
+	m.eventHandlers = append(m.eventHandlers, f)
 	for _, c := range m.remoteKubeControllers {
 		c.AddEventHandler(f)
 	}
@@ -138,11 +104,11 @@ func (m *Multicluster) AddEventHandler(f func(name string, namespace string)) {
 type AggregateController struct {
 	// controllers to use to look up certs. Generally this will consistent of the local (config) cluster
 	// and a single remote cluster where the proxy resides
-	controllers    []*SecretsController
-	authController *SecretsController
+	controllers    []*CredentialsController
+	authController *CredentialsController
 }
 
-var _ secrets.Controller = &AggregateController{}
+var _ credentials.Controller = &AggregateController{}
 
 func (a *AggregateController) GetKeyAndCert(name, namespace string) (key []byte, cert []byte, err error) {
 	// Search through all clusters, find first non-empty result
diff --git a/pilot/pkg/secrets/kube/secrets.go b/pilot/pkg/credentials/kube/secrets.go
similarity index 92%
rename from pilot/pkg/secrets/kube/secrets.go
rename to pilot/pkg/credentials/kube/secrets.go
index d2f1240350..1c71731e22 100644
--- a/pilot/pkg/secrets/kube/secrets.go
+++ b/pilot/pkg/credentials/kube/secrets.go
@@ -35,7 +35,7 @@
 	k8stesting "k8s.io/client-go/testing"
 	"k8s.io/client-go/tools/cache"
 
-	"istio.io/istio/pilot/pkg/secrets"
+	"istio.io/istio/pilot/pkg/credentials"
 	"istio.io/istio/pkg/cluster"
 	"istio.io/istio/pkg/kube"
 	"istio.io/pkg/log"
@@ -61,7 +61,7 @@
 	GatewaySdsCaSuffix = "-cacert"
 )
 
-type SecretsController struct {
+type CredentialsController struct {
 	secrets informersv1.SecretInformer
 	sar     authorizationv1client.SubjectAccessReviewInterface
 
@@ -78,9 +78,9 @@ type authorizationResponse struct {
 	authorized error
 }
 
-var _ secrets.Controller = &SecretsController{}
+var _ credentials.Controller = &CredentialsController{}
 
-func NewSecretsController(client kube.Client, clusterID cluster.ID) *SecretsController {
+func NewCredentialsController(client kube.Client, clusterID cluster.ID) *CredentialsController {
 	informer := client.KubeInformer().InformerFor(&v1.Secret{}, func(k kubernetes.Interface, resync time.Duration) cache.SharedIndexInformer {
 		return informersv1.NewFilteredSecretInformer(
 			k, metav1.NamespaceAll, resync, cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc},
@@ -99,7 +99,7 @@ func(options *metav1.ListOptions) {
 		)
 	})
 
-	return &SecretsController{
+	return &CredentialsController{
 		secrets: informerAdapter{listersv1.NewSecretLister(informer.GetIndexer()), informer},
 
 		sar:                client.AuthorizationV1().SubjectAccessReviews(),
@@ -115,7 +115,7 @@ func toUser(serviceAccount, namespace string) string {
 const cacheTTL = time.Minute
 
 // clearExpiredCache iterates through the cache and removes all expired entries. Should be called with mutex held.
-func (s *SecretsController) clearExpiredCache() {
+func (s *CredentialsController) clearExpiredCache() {
 	for k, v := range s.authorizationCache {
 		if v.expiration.Before(time.Now()) {
 			delete(s.authorizationCache, k)
@@ -125,7 +125,7 @@ func (s *SecretsController) clearExpiredCache() {
 
 // cachedAuthorization checks the authorization cache
 // nolint
-func (s *SecretsController) cachedAuthorization(user string) (error, bool) {
+func (s *CredentialsController) cachedAuthorization(user string) (error, bool) {
 	key := authorizationKey(user)
 	s.mu.Lock()
 	defer s.mu.Unlock()
@@ -139,7 +139,7 @@ func (s *SecretsController) cachedAuthorization(user string) (error, bool) {
 }
 
 // cachedAuthorization checks the authorization cache
-func (s *SecretsController) insertCache(user string, response error) {
+func (s *CredentialsController) insertCache(user string, response error) {
 	s.mu.Lock()
 	defer s.mu.Unlock()
 	key := authorizationKey(user)
@@ -166,7 +166,7 @@ func DisableAuthorizationForTest(fake *fake.Clientset) {
 	})
 }
 
-func (s *SecretsController) Authorize(serviceAccount, namespace string) error {
+func (s *CredentialsController) Authorize(serviceAccount, namespace string) error {
 	user := toUser(serviceAccount, namespace)
 	if cached, f := s.cachedAuthorization(user); f {
 		return cached
@@ -195,7 +195,7 @@ func (s *SecretsController) Authorize(serviceAccount, namespace string) error {
 	return resp
 }
 
-func (s *SecretsController) GetKeyAndCert(name, namespace string) (key []byte, cert []byte, err error) {
+func (s *CredentialsController) GetKeyAndCert(name, namespace string) (key []byte, cert []byte, err error) {
 	k8sSecret, err := s.secrets.Lister().Secrets(namespace).Get(name)
 	if err != nil {
 		return nil, nil, fmt.Errorf("secret %v/%v not found", namespace, name)
@@ -204,7 +204,7 @@ func (s *SecretsController) GetKeyAndCert(name, namespace string) (key []byte, c
 	return extractKeyAndCert(k8sSecret)
 }
 
-func (s *SecretsController) GetCaCert(name, namespace string) (cert []byte, err error) {
+func (s *CredentialsController) GetCaCert(name, namespace string) (cert []byte, err error) {
 	strippedName := strings.TrimSuffix(name, GatewaySdsCaSuffix)
 	k8sSecret, err := s.secrets.Lister().Secrets(namespace).Get(name)
 	if err != nil {
@@ -290,7 +290,7 @@ func extractRoot(scrt *v1.Secret) (cert []byte, err error) {
 		GenericScrtCaCert, TLSSecretCaCert, found)
 }
 
-func (s *SecretsController) AddEventHandler(f func(name string, namespace string)) {
+func (s *CredentialsController) AddEventHandler(f func(name string, namespace string)) {
 	handler := func(obj interface{}) {
 		scrt, ok := obj.(*v1.Secret)
 		if !ok {
diff --git a/pilot/pkg/secrets/kube/secrets_test.go b/pilot/pkg/credentials/kube/secrets_test.go
similarity index 93%
rename from pilot/pkg/secrets/kube/secrets_test.go
rename to pilot/pkg/credentials/kube/secrets_test.go
index d426e8c9f4..a57e891298 100644
--- a/pilot/pkg/secrets/kube/secrets_test.go
+++ b/pilot/pkg/credentials/kube/secrets_test.go
@@ -28,6 +28,7 @@
 	"istio.io/istio/pilot/pkg/util/sets"
 	cluster2 "istio.io/istio/pkg/cluster"
 	"istio.io/istio/pkg/kube"
+	"istio.io/istio/pkg/kube/multicluster"
 )
 
 func makeSecret(name string, data map[string]string) *corev1.Secret {
@@ -99,7 +100,7 @@ func TestSecretsController(t *testing.T) {
 		wrongKeys,
 	}
 	client := kube.NewFakeClient(secrets...)
-	sc := NewSecretsController(client, "")
+	sc := NewCredentialsController(client, "")
 	stop := make(chan struct{})
 	t.Cleanup(func() {
 		close(stop)
@@ -245,13 +246,12 @@ func allowIdentities(c kube.Client, identities ...string) {
 }
 
 func TestForCluster(t *testing.T) {
-	stop := make(chan struct{})
-	defer close(stop)
 	localClient := kube.NewFakeClient()
 	remoteClient := kube.NewFakeClient()
-	sc := NewMulticluster(localClient, "local", "", stop)
-	sc.addMemberCluster(remoteClient, "remote")
-	sc.addMemberCluster(remoteClient, "remote2")
+	sc := NewMulticluster("local")
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "local", Client: localClient}, nil)
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "remote", Client: remoteClient}, nil)
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "remote2", Client: remoteClient}, nil)
 	cases := []struct {
 		cluster cluster2.ID
 		allowed bool
@@ -272,14 +272,13 @@ func TestForCluster(t *testing.T) {
 }
 
 func TestAuthorize(t *testing.T) {
-	stop := make(chan struct{})
-	defer close(stop)
 	localClient := kube.NewFakeClient()
 	remoteClient := kube.NewFakeClient()
 	allowIdentities(localClient, "system:serviceaccount:ns-local:sa-allowed")
 	allowIdentities(remoteClient, "system:serviceaccount:ns-remote:sa-allowed")
-	sc := NewMulticluster(localClient, "local", "", stop)
-	sc.addMemberCluster(remoteClient, "remote")
+	sc := NewMulticluster("local")
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "local", Client: localClient}, nil)
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "remote", Client: remoteClient}, nil)
 	cases := []struct {
 		sa      string
 		ns      string
@@ -332,9 +331,10 @@ func TestSecretsControllerMulticluster(t *testing.T) {
 	localClient := kube.NewFakeClient(secretsLocal...)
 	remoteClient := kube.NewFakeClient(secretsRemote...)
 	otherRemoteClient := kube.NewFakeClient()
-	sc := NewMulticluster(localClient, "local", "", stop)
-	sc.addMemberCluster(remoteClient, "remote")
-	sc.addMemberCluster(otherRemoteClient, "other")
+	sc := NewMulticluster("local")
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "local", Client: localClient}, nil)
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "remote", Client: remoteClient}, nil)
+	_ = sc.ClusterAdded(&multicluster.Cluster{ID: "other", Client: otherRemoteClient}, nil)
 
 	// normally the remote secrets controller would start these
 	localClient.RunAndWait(stop)
diff --git a/pilot/pkg/secrets/model.go b/pilot/pkg/credentials/model.go
similarity index 98%
rename from pilot/pkg/secrets/model.go
rename to pilot/pkg/credentials/model.go
index f5d798e357..189d3dc373 100644
--- a/pilot/pkg/secrets/model.go
+++ b/pilot/pkg/credentials/model.go
@@ -12,7 +12,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
-package secrets
+package credentials
 
 import "istio.io/istio/pkg/cluster"
 
diff --git a/pilot/pkg/serviceregistry/aggregate/controller.go b/pilot/pkg/serviceregistry/aggregate/controller.go
index 8e30adfae2..1f65c38dd5 100644
--- a/pilot/pkg/serviceregistry/aggregate/controller.go
+++ b/pilot/pkg/serviceregistry/aggregate/controller.go
@@ -41,7 +41,7 @@
 
 // Controller aggregates data across different registries and monitors for changes
 type Controller struct {
-	registries []serviceregistry.Instance
+	registries []*registryEntry
 	storeLock  sync.RWMutex
 	meshHolder mesh.Holder
 	running    *atomic.Bool
@@ -49,6 +49,12 @@ type Controller struct {
 	handlers model.ControllerHandlers
 }
 
+type registryEntry struct {
+	serviceregistry.Instance
+	// stop if not nil is the per-registry stop chan. If null, the server stop chan should be used to Run the registry.
+	stop <-chan struct{}
+}
+
 type Options struct {
 	MeshHolder mesh.Holder
 }
@@ -56,24 +62,42 @@ type Options struct {
 // NewController creates a new Aggregate controller
 func NewController(opt Options) *Controller {
 	return &Controller{
-		registries: make([]serviceregistry.Instance, 0),
+		registries: make([]*registryEntry, 0),
 		meshHolder: opt.MeshHolder,
 		running:    atomic.NewBool(false),
 	}
 }
 
-// AddRegistry adds registries into the aggregated controller
-func (c *Controller) AddRegistry(registry serviceregistry.Instance) {
+func (c *Controller) addRegistry(registry serviceregistry.Instance, stop <-chan struct{}) {
 	c.storeLock.Lock()
 	defer c.storeLock.Unlock()
 
-	c.registries = append(c.registries, registry)
+	c.registries = append(c.registries, &registryEntry{Instance: registry, stop: stop})
 
 	// Observe the registry for events.
 	registry.AppendServiceHandler(c.handlers.NotifyServiceHandlers)
 	registry.AppendWorkloadHandler(c.handlers.NotifyWorkloadHandlers)
 }
 
+// AddRegistry adds registries into the aggregated controller.
+// If the aggregated controller is already Running, the given registry will never be started.
+func (c *Controller) AddRegistry(registry serviceregistry.Instance) {
+	c.addRegistry(registry, nil)
+}
+
+// AddRegistryAndRun adds registries into the aggregated controller and makes sure it is Run.
+// If the aggregated controller is running, the given registry is Run immediately.
+// Otherwise, the given registry is Run when the aggregate controller is Run, using the given stop.
+func (c *Controller) AddRegistryAndRun(registry serviceregistry.Instance, stop <-chan struct{}) {
+	if stop == nil {
+		log.Warnf("nil stop channel passed to AddRegistryAndRun for registry %s/%s", registry.Provider(), registry.Cluster())
+	}
+	c.addRegistry(registry, stop)
+	if c.Running() {
+		go registry.Run(stop)
+	}
+}
+
 // DeleteRegistry deletes specified registry from the aggregated controller
 func (c *Controller) DeleteRegistry(clusterID cluster.ID, providerID provider.ID) {
 	c.storeLock.Lock()
@@ -88,6 +112,8 @@ func (c *Controller) DeleteRegistry(clusterID cluster.ID, providerID provider.ID
 		log.Warnf("Registry %s is not found in the registries list, nothing to delete", clusterID)
 		return
 	}
+
+	c.registries[index] = nil
 	c.registries = append(c.registries[:index], c.registries[index+1:]...)
 	log.Infof("Registry for the cluster %s has been deleted.", clusterID)
 }
@@ -276,9 +302,17 @@ func (c *Controller) GetProxyWorkloadLabels(proxy *model.Proxy) labels.Collectio
 
 // Run starts all the controllers
 func (c *Controller) Run(stop <-chan struct{}) {
+	c.storeLock.RLock()
 	for _, r := range c.GetRegistries() {
-		go r.Run(stop)
+		// prefer the per-registry stop channel
+		registryStop := stop
+		if s := r.(*registryEntry).stop; s != nil {
+			registryStop = s
+		}
+		go r.Run(registryStop)
 	}
+	c.storeLock.RUnlock()
+
 	c.running.Store(true)
 	<-stop
 	log.Info("Registry Aggregator terminated")
diff --git a/pilot/pkg/serviceregistry/aggregate/controller_test.go b/pilot/pkg/serviceregistry/aggregate/controller_test.go
index b218b79042..8d2b0ce4ad 100644
--- a/pilot/pkg/serviceregistry/aggregate/controller_test.go
+++ b/pilot/pkg/serviceregistry/aggregate/controller_test.go
@@ -19,8 +19,10 @@
 	"fmt"
 	"reflect"
 	"testing"
+	"time"
 
 	"github.com/google/go-cmp/cmp"
+	"go.uber.org/atomic"
 
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	"istio.io/istio/pilot/pkg/model"
@@ -30,6 +32,7 @@
 	"istio.io/istio/pkg/cluster"
 	"istio.io/istio/pkg/config/host"
 	"istio.io/istio/pkg/config/labels"
+	"istio.io/istio/pkg/test/util/retry"
 )
 
 type mockMeshConfigHolder struct {
@@ -425,6 +428,10 @@ func TestGetDeleteRegistry(t *testing.T) {
 			Controller: &mock.Controller{},
 		},
 	}
+	wrapRegistry := func(r serviceregistry.Instance) serviceregistry.Instance {
+		return &registryEntry{Instance: r}
+	}
+
 	ctrl := NewController(Options{})
 	for _, r := range registries {
 		ctrl.AddRegistry(r)
@@ -443,7 +450,7 @@ func TestGetDeleteRegistry(t *testing.T) {
 		t.Fatalf("Expected length of the registries slice should be 2, got %d", l)
 	}
 	// check left registries are orders as before
-	if !reflect.DeepEqual(result[0], registries[0]) || !reflect.DeepEqual(result[1], registries[2]) {
+	if !reflect.DeepEqual(result[0], wrapRegistry(registries[0])) || !reflect.DeepEqual(result[1], wrapRegistry(registries[2])) {
 		t.Fatalf("Expected registries order has been changed")
 	}
 }
@@ -494,3 +501,63 @@ func TestSkipSearchingRegistryForProxy(t *testing.T) {
 		}
 	}
 }
+
+func runnableRegistry(name string) *RunnableRegistry {
+	return &RunnableRegistry{
+		Instance: serviceregistry.Simple{
+			ClusterID: cluster.ID(name), ProviderID: "test",
+			Controller: &mock.Controller{},
+		},
+		running: atomic.NewBool(false),
+	}
+}
+
+type RunnableRegistry struct {
+	serviceregistry.Instance
+	running *atomic.Bool
+}
+
+func (rr *RunnableRegistry) Run(stop <-chan struct{}) {
+	rr.running.Store(true)
+	<-stop
+}
+
+func expectRunningOrFail(t *testing.T, ctrl *Controller, want bool) {
+	// running gets flipped in a goroutine, retry to avoid race
+	retry.UntilSuccessOrFail(t, func() error {
+		for _, registry := range ctrl.registries {
+			if running := registry.Instance.(*RunnableRegistry).running.Load(); running != want {
+				return fmt.Errorf("%s running is %v but wanted %v", registry.Cluster(), running, want)
+			}
+		}
+		return nil
+	}, retry.Timeout(50*time.Millisecond), retry.Delay(0))
+}
+
+func TestDeferredRun(t *testing.T) {
+	stop := make(chan struct{})
+	defer close(stop)
+	ctrl := NewController(Options{})
+
+	t.Run("AddRegistry before aggregate Run does not run", func(t *testing.T) {
+		ctrl.AddRegistry(runnableRegistry("earlyAdd"))
+		ctrl.AddRegistryAndRun(runnableRegistry("earlyAddAndRun"), nil)
+		expectRunningOrFail(t, ctrl, false)
+	})
+	t.Run("aggregate Run starts all registries", func(t *testing.T) {
+		go ctrl.Run(stop)
+		expectRunningOrFail(t, ctrl, true)
+		ctrl.DeleteRegistry("earlyAdd", "test")
+		ctrl.DeleteRegistry("earlyAddAndRun", "test")
+	})
+	t.Run("AddRegistry after aggregate Run does not start registry", func(t *testing.T) {
+		ctrl.AddRegistry(runnableRegistry("missed"))
+		expectRunningOrFail(t, ctrl, false)
+		ctrl.DeleteRegistry("missed", "test")
+		expectRunningOrFail(t, ctrl, true)
+	})
+	t.Run("AddRegistryAndRun after aggregate Run starts registry", func(t *testing.T) {
+		ctrl.AddRegistryAndRun(runnableRegistry("late"), nil)
+		expectRunningOrFail(t, ctrl, true)
+	})
+}
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller.go b/pilot/pkg/serviceregistry/kube/controller/controller.go
index be82bbd0ea..5a5d979367 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller.go
@@ -287,7 +287,7 @@ type Controller struct {
 }
 
 // NewController creates a new Kubernetes controller
-// Created by bootstrap and multicluster (see secretcontroller).
+// Created by bootstrap and multicluster (see multicluster.Controller).
 func NewController(kubeClient kubelib.Client, options Options) *Controller {
 	c := &Controller{
 		opts:                        options,
@@ -825,6 +825,7 @@ func (c *Controller) syncEndpoints() error {
 
 // Run all controllers until a signal is received
 func (c *Controller) Run(stop <-chan struct{}) {
+	st := time.Now()
 	if c.opts.NetworksWatcher != nil {
 		c.opts.NetworksWatcher.AddNetworksHandler(c.reloadNetworkLookup)
 		c.reloadMeshNetworks()
@@ -838,6 +839,7 @@ func (c *Controller) Run(stop <-chan struct{}) {
 		log.Errorf("one or more errors force-syncing resources: %v", err)
 	}
 	c.initialSync.Store(true)
+	log.Infof("kube controller for %s synced after %v", c.opts.ClusterID, time.Since(st))
 	// after the in-order sync we can start processing the queue
 	c.queue.Run(stop)
 	log.Infof("Controller terminated")
diff --git a/pilot/pkg/serviceregistry/kube/controller/multicluster.go b/pilot/pkg/serviceregistry/kube/controller/multicluster.go
index d184d11fe0..5b6b78746c 100644
--- a/pilot/pkg/serviceregistry/kube/controller/multicluster.go
+++ b/pilot/pkg/serviceregistry/kube/controller/multicluster.go
@@ -36,7 +36,7 @@
 	"istio.io/istio/pkg/config/schema/collections"
 	"istio.io/istio/pkg/config/schema/gvk"
 	kubelib "istio.io/istio/pkg/kube"
-	"istio.io/istio/pkg/kube/secretcontroller"
+	"istio.io/istio/pkg/kube/multicluster"
 	"istio.io/istio/pkg/webhooks"
 	"istio.io/istio/pkg/webhooks/validation/controller"
 )
@@ -46,6 +46,8 @@
 	webhookName = "sidecar-injector.istio.io"
 )
 
+var _ multicluster.ClusterHandler = &Multicluster{}
+
 type kubeController struct {
 	*Controller
 	workloadEntryStore *serviceentry.ServiceEntryStore
@@ -76,9 +78,8 @@ type Multicluster struct {
 	revision          string
 
 	// secretNamespace where we get cluster-access secrets
-	secretNamespace  string
-	secretController *secretcontroller.Controller
-	syncInterval     time.Duration
+	secretNamespace string
+	syncInterval    time.Duration
 }
 
 // NewMulticluster initializes data structure to store multicluster information
@@ -135,41 +136,39 @@ func (m *Multicluster) close() (err error) {
 	for _, clusterID := range clusterIDs {
 		clusterID := clusterID
 		g.Go(func() error {
-			return m.DeleteMemberCluster(clusterID)
+			return m.ClusterDeleted(clusterID)
 		})
 	}
 	err = g.Wait()
 	return
 }
 
-// AddMemberCluster is passed to the secret controller as a callback to be called
+// AddCluster is passed to the secret controller as a callback to be called
 // when a remote cluster is added.  This function needs to set up all the handlers
 // to watch for resources being added, deleted or changed on remote clusters.
-func (m *Multicluster) AddMemberCluster(clusterID cluster.ID, rc *secretcontroller.Cluster) error {
+func (m *Multicluster) ClusterAdded(cluster *multicluster.Cluster, clusterStopCh <-chan struct{}) error {
 	m.m.Lock()
 
 	if m.closing {
 		m.m.Unlock()
-		return fmt.Errorf("failed adding member cluster %s: server shutting down", clusterID)
+		return fmt.Errorf("failed adding member cluster %s: server shutting down", cluster.ID)
 	}
 
-	client := rc.Client
-	clusterStopCh := rc.Stop
+	client := cluster.Client
 
 	// clusterStopCh is a channel that will be closed when this cluster removed.
 	options := m.opts
-	options.ClusterID = clusterID
+	options.ClusterID = cluster.ID
 	// the aggregate registry's HasSynced will use the k8s controller's HasSynced, so we reference the same timeout
-	options.SyncTimeout = rc.SyncTimeout
+	options.SyncTimeout = cluster.SyncTimeout
 
 	log.Infof("Initializing Kubernetes service registry %q", options.ClusterID)
 	kubeRegistry := NewController(client, options)
-	m.opts.MeshServiceController.AddRegistry(kubeRegistry)
-	m.remoteKubeControllers[clusterID] = &kubeController{
+	m.remoteKubeControllers[cluster.ID] = &kubeController{
 		Controller: kubeRegistry,
 	}
 	// localCluster may also be the "config" cluster, in an external-istiod setup.
-	localCluster := m.opts.ClusterID == clusterID
+	localCluster := m.opts.ClusterID == cluster.ID
 
 	m.m.Unlock()
 
@@ -191,25 +190,22 @@ func (m *Multicluster) AddMemberCluster(clusterID cluster.ID, rc *secretcontroll
 		} else if features.WorkloadEntryCrossCluster {
 			// TODO only do this for non-remotes, can't guarantee CRDs in remotes (depends on https://github.com/istio/istio/pull/29824)
 			if configStore, err := createConfigStore(client, m.revision, options); err == nil {
-				m.remoteKubeControllers[clusterID].workloadEntryStore = serviceentry.NewServiceDiscovery(
+				m.remoteKubeControllers[cluster.ID].workloadEntryStore = serviceentry.NewServiceDiscovery(
 					configStore, model.MakeIstioStore(configStore), options.XDSUpdater,
-					serviceentry.DisableServiceEntryProcessing(), serviceentry.WithClusterID(clusterID),
+					serviceentry.DisableServiceEntryProcessing(), serviceentry.WithClusterID(cluster.ID),
 					serviceentry.WithNetworkIDCb(kubeRegistry.Network))
-				m.opts.MeshServiceController.AddRegistry(m.remoteKubeControllers[clusterID].workloadEntryStore)
 				// Services can select WorkloadEntry from the same cluster. We only duplicate the Service to configure kube-dns.
-				m.remoteKubeControllers[clusterID].workloadEntryStore.AppendWorkloadHandler(kubeRegistry.WorkloadInstanceHandler)
+				m.remoteKubeControllers[cluster.ID].workloadEntryStore.AppendWorkloadHandler(kubeRegistry.WorkloadInstanceHandler)
+				m.opts.MeshServiceController.AddRegistryAndRun(m.remoteKubeControllers[cluster.ID].workloadEntryStore, clusterStopCh)
 				go configStore.Run(clusterStopCh)
 			} else {
-				return fmt.Errorf("failed creating config configStore for cluster %s: %v", clusterID, err)
+				return fmt.Errorf("failed creating config configStore for cluster %s: %v", cluster.ID, err)
 			}
 		}
 	}
 
-	// TODO make the aggregate controller keep clusters tied to their individual stop channels
-	if m.opts.MeshServiceController.Running() {
-		// if serviceController isn't running, it will start its members when it is started
-		go kubeRegistry.Run(clusterStopCh)
-	}
+	// run after ServiceHandler and WorkloadHandler are added
+	m.opts.MeshServiceController.AddRegistryAndRun(kubeRegistry, clusterStopCh)
 
 	// TODO only create namespace controller and cert patch for remote clusters (no way to tell currently)
 	if m.startNsController && (features.ExternalIstiod || localCluster) {
@@ -220,7 +216,7 @@ func (m *Multicluster) AddMemberCluster(clusterID cluster.ID, rc *secretcontroll
 			leaderelection.
 				NewLeaderElection(options.SystemNamespace, m.serverID, leaderelection.NamespaceController, m.revision, client).
 				AddRunFunction(func(leaderStop <-chan struct{}) {
-					log.Infof("starting namespace controller for cluster %s", clusterID)
+					log.Infof("starting namespace controller for cluster %s", cluster.ID)
 					nc := NewNamespaceController(client, m.caBundleWatcher)
 					// Start informers again. This fixes the case where informers for namespace do not start,
 					// as we create them only after acquiring the leader lock
@@ -241,7 +237,7 @@ func (m *Multicluster) AddMemberCluster(clusterID cluster.ID, rc *secretcontroll
 		// operator or CI/CD
 		if features.InjectionWebhookConfigName != "" {
 			// TODO prevent istiods in primary clusters from trying to patch eachother. should we also leader-elect?
-			log.Infof("initializing webhook cert patch for cluster %s", clusterID)
+			log.Infof("initializing webhook cert patch for cluster %s", cluster.ID)
 			patcher, err := webhooks.NewWebhookCertPatcher(client, m.revision, webhookName, m.caBundleWatcher)
 			if err != nil {
 				log.Errorf("could not initialize webhook cert patcher: %v", err)
@@ -264,7 +260,7 @@ func (m *Multicluster) AddMemberCluster(clusterID cluster.ID, rc *secretcontroll
 			leaderelection.
 				NewLeaderElection(options.SystemNamespace, m.serverID, leaderelection.ServiceExportController, m.revision, client).
 				AddRunFunction(func(leaderStop <-chan struct{}) {
-					log.Infof("starting service export controller for cluster %s", clusterID)
+					log.Infof("starting service export controller for cluster %s", cluster.ID)
 					serviceExportController := newAutoServiceExportController(autoServiceExportOptions{
 						Client:       client,
 						ClusterID:    m.opts.ClusterID,
@@ -286,17 +282,17 @@ func (m *Multicluster) AddMemberCluster(clusterID cluster.ID, rc *secretcontroll
 	return nil
 }
 
-func (m *Multicluster) UpdateMemberCluster(clusterID cluster.ID, rc *secretcontroller.Cluster) error {
-	if err := m.DeleteMemberCluster(clusterID); err != nil {
+func (m *Multicluster) ClusterUpdated(cluster *multicluster.Cluster, stop <-chan struct{}) error {
+	if err := m.ClusterDeleted(cluster.ID); err != nil {
 		return err
 	}
-	return m.AddMemberCluster(clusterID, rc)
+	return m.ClusterAdded(cluster, stop)
 }
 
-// DeleteMemberCluster is passed to the secret controller as a callback to be called
+// RemoveCluster is passed to the secret controller as a callback to be called
 // when a remote cluster is deleted.  Also must clear the cache so remote resources
 // are removed.
-func (m *Multicluster) DeleteMemberCluster(clusterID cluster.ID) error {
+func (m *Multicluster) ClusterDeleted(clusterID cluster.ID) error {
 	m.m.Lock()
 	defer m.m.Unlock()
 	m.opts.MeshServiceController.DeleteRegistry(clusterID, provider.Kubernetes)
@@ -343,23 +339,3 @@ func (m *Multicluster) updateHandler(svc *model.Service) {
 		m.XDSUpdater.ConfigUpdate(req)
 	}
 }
-
-func (m *Multicluster) GetRemoteKubeClient(clusterID cluster.ID) kubernetes.Interface {
-	m.m.Lock()
-	defer m.m.Unlock()
-	if c := m.remoteKubeControllers[clusterID]; c != nil {
-		return c.client
-	}
-	return nil
-}
-
-func (m *Multicluster) InitSecretController(stop <-chan struct{}) *secretcontroller.Controller {
-	m.secretController = secretcontroller.StartSecretController(
-		m.client, m.AddMemberCluster, m.UpdateMemberCluster, m.DeleteMemberCluster,
-		m.secretNamespace, m.syncInterval, stop)
-	return m.secretController
-}
-
-func (m *Multicluster) HasSynced() bool {
-	return m.secretController.HasSynced()
-}
diff --git a/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go b/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go
index bc0520c6cc..fbc4eba63f 100644
--- a/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go
@@ -30,7 +30,7 @@
 	"istio.io/istio/pilot/pkg/serviceregistry/aggregate"
 	"istio.io/istio/pkg/config/mesh"
 	"istio.io/istio/pkg/kube"
-	"istio.io/istio/pkg/kube/secretcontroller"
+	"istio.io/istio/pkg/kube/multicluster"
 	"istio.io/istio/pkg/test/util/retry"
 )
 
@@ -49,7 +49,7 @@ func createMultiClusterSecret(k8s kube.Client, sname, cname string) error {
 			Name:      sname,
 			Namespace: testSecretNameSpace,
 			Labels: map[string]string{
-				secretcontroller.MultiClusterSecretLabel: "true",
+				multicluster.MultiClusterSecretLabel: "true",
 			},
 		},
 		Data: map[string][]byte{},
@@ -78,8 +78,15 @@ func verifyControllers(t *testing.T, m *Multicluster, expectedControllerCount in
 	}, retry.Message(timeoutName), retry.Delay(time.Millisecond*10), retry.Timeout(time.Second*5))
 }
 
+func initController(client kube.ExtendedClient, ns string, stop <-chan struct{}, mc *Multicluster) {
+	sc := multicluster.NewController(client, ns, "cluster-1")
+	sc.AddHandler(mc)
+	_ = sc.Run(stop)
+	cache.WaitForCacheSync(stop, sc.HasSynced)
+}
+
 func Test_KubeSecretController(t *testing.T) {
-	secretcontroller.BuildClientsFromConfig = func(kubeConfig []byte) (kube.Client, error) {
+	multicluster.BuildClientsFromConfig = func(kubeConfig []byte) (kube.Client, error) {
 		return kube.NewFakeClient(), nil
 	}
 	clientset := kube.NewFakeClient()
@@ -93,20 +100,22 @@ func Test_KubeSecretController(t *testing.T) {
 		clientset,
 		testSecretNameSpace,
 		Options{
+			ClusterID:             "cluster-1",
 			DomainSuffix:          DomainSuffix,
 			ResyncPeriod:          ResyncPeriod,
 			SyncInterval:          time.Microsecond,
 			MeshWatcher:           mesh.NewFixedWatcher(&meshconfig.MeshConfig{}),
 			MeshServiceController: mockserviceController,
 		}, nil, nil, "default", false, nil, s)
-	mc.InitSecretController(stop)
-	cache.WaitForCacheSync(stop, mc.HasSynced)
+	initController(clientset, testSecretNameSpace, stop, mc)
 	clientset.RunAndWait(stop)
 	_ = s.Start(stop)
 	go func() {
 		_ = mc.Run(stop)
 	}()
 
+	verifyControllers(t, mc, 1, "create local controller")
+
 	// Create the multicluster secret. Sleep to allow created remote
 	// controller to start and callback add function to be called.
 	err := createMultiClusterSecret(clientset, "test-secret-1", "test-remote-cluster-1")
@@ -115,7 +124,7 @@ func Test_KubeSecretController(t *testing.T) {
 	}
 
 	// Test - Verify that the remote controller has been added.
-	verifyControllers(t, mc, 1, "create remote controller")
+	verifyControllers(t, mc, 2, "create remote controller")
 
 	// Delete the mulicluster secret.
 	err = deleteMultiClusterSecret(clientset, "test-secret-1")
@@ -124,7 +133,7 @@ func Test_KubeSecretController(t *testing.T) {
 	}
 
 	// Test - Verify that the remote controller has been removed.
-	verifyControllers(t, mc, 0, "delete remote controller")
+	verifyControllers(t, mc, 1, "delete remote controller")
 }
 
 func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
@@ -137,7 +146,7 @@ func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
 		features.InjectionWebhookConfigName = webhookName
 	}()
 	clientset := kube.NewFakeClient()
-	secretcontroller.BuildClientsFromConfig = func(kubeConfig []byte) (kube.Client, error) {
+	multicluster.BuildClientsFromConfig = func(kubeConfig []byte) (kube.Client, error) {
 		return kube.NewFakeClient(), nil
 	}
 	stop := make(chan struct{})
@@ -151,20 +160,23 @@ func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
 		clientset,
 		testSecretNameSpace,
 		Options{
+			ClusterID:             "cluster-1",
 			DomainSuffix:          DomainSuffix,
 			ResyncPeriod:          ResyncPeriod,
 			SyncInterval:          time.Microsecond,
 			MeshWatcher:           mesh.NewFixedWatcher(&meshconfig.MeshConfig{}),
 			MeshServiceController: mockserviceController,
 		}, nil, certWatcher, "default", false, nil, s)
-	mc.InitSecretController(stop)
-	cache.WaitForCacheSync(stop, mc.HasSynced)
+	initController(clientset, testSecretNameSpace, stop, mc)
 	clientset.RunAndWait(stop)
 	_ = s.Start(stop)
 	go func() {
 		_ = mc.Run(stop)
 	}()
 
+	// the multicluster controller will register the local cluster
+	verifyControllers(t, mc, 1, "registered local cluster controller")
+
 	// Create the multicluster secret. Sleep to allow created remote
 	// controller to start and callback add function to be called.
 	err := createMultiClusterSecret(clientset, "test-secret-1", "test-remote-cluster-1")
@@ -173,7 +185,7 @@ func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
 	}
 
 	// Test - Verify that the remote controller has been added.
-	verifyControllers(t, mc, 1, "create remote controller")
+	verifyControllers(t, mc, 2, "create remote controller 1")
 
 	// Create second multicluster secret. Sleep to allow created remote
 	// controller to start and callback add function to be called.
@@ -183,7 +195,7 @@ func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
 	}
 
 	// Test - Verify that the remote controller has been added.
-	verifyControllers(t, mc, 2, "create remote controller")
+	verifyControllers(t, mc, 3, "create remote controller 2")
 
 	// Delete the first mulicluster secret.
 	err = deleteMultiClusterSecret(clientset, "test-secret-1")
@@ -192,7 +204,7 @@ func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
 	}
 
 	// Test - Verify that the remote controller has been removed.
-	verifyControllers(t, mc, 1, "delete remote controller")
+	verifyControllers(t, mc, 2, "delete remote controller 1")
 
 	// Delete the second mulicluster secret.
 	err = deleteMultiClusterSecret(clientset, "test-secret-2")
@@ -201,5 +213,5 @@ func Test_KubeSecretController_ExternalIstiod_MultipleClusters(t *testing.T) {
 	}
 
 	// Test - Verify that the remote controller has been removed.
-	verifyControllers(t, mc, 0, "delete remote controller")
+	verifyControllers(t, mc, 1, "delete remote controller 2")
 }
diff --git a/pilot/pkg/xds/fake.go b/pilot/pkg/xds/fake.go
index 40da78c50e..1bc268984c 100644
--- a/pilot/pkg/xds/fake.go
+++ b/pilot/pkg/xds/fake.go
@@ -36,11 +36,11 @@
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	"istio.io/istio/pilot/pkg/config/kube/ingress"
 	"istio.io/istio/pilot/pkg/controller/workloadentry"
+	kubesecrets "istio.io/istio/pilot/pkg/credentials/kube"
 	"istio.io/istio/pilot/pkg/features"
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/networking/core/v1alpha3"
 	"istio.io/istio/pilot/pkg/networking/plugin"
-	kubesecrets "istio.io/istio/pilot/pkg/secrets/kube"
 	"istio.io/istio/pilot/pkg/serviceregistry"
 	kube "istio.io/istio/pilot/pkg/serviceregistry/kube/controller"
 	v3 "istio.io/istio/pilot/pkg/xds/v3"
@@ -53,6 +53,7 @@
 	"istio.io/istio/pkg/config/schema/gvk"
 	"istio.io/istio/pkg/keepalive"
 	kubelib "istio.io/istio/pkg/kube"
+	"istio.io/istio/pkg/kube/multicluster"
 	"istio.io/istio/pkg/test"
 	"istio.io/istio/pkg/test/util/retry"
 )
@@ -168,6 +169,8 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 			Delegate: s,
 		}
 	}
+	creds := kubesecrets.NewMulticluster(opts.DefaultClusterName)
+	s.Generators[v3.SecretType] = NewSecretGen(creds, s.Cache, opts.DefaultClusterName)
 	for k8sCluster, objs := range k8sObjects {
 		client := kubelib.NewFakeClientWithVersion(opts.KubernetesVersion, objs...)
 		if opts.KubeClientModifier != nil {
@@ -193,13 +196,14 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 			client.RunAndWait(stop)
 		}
 		registries = append(registries, k8s)
+		if err := creds.ClusterAdded(&multicluster.Cluster{ID: k8sCluster, Client: client}, nil); err != nil {
+			t.Fatal(err)
+		}
 	}
 
 	if opts.DisableSecretAuthorization {
 		kubesecrets.DisableAuthorizationForTest(defaultKubeClient.Kube().(*fake.Clientset))
 	}
-	sc := kubesecrets.NewMulticluster(defaultKubeClient, opts.DefaultClusterName, "", stop)
-	s.Generators[v3.SecretType] = NewSecretGen(sc, s.Cache, opts.DefaultClusterName)
 	defaultKubeClient.RunAndWait(stop)
 
 	ingr := ingress.NewController(defaultKubeClient, mesh.NewFixedWatcher(m), kube.Options{
diff --git a/pilot/pkg/xds/sds.go b/pilot/pkg/xds/sds.go
index 22222a8c80..9c8a4c891b 100644
--- a/pilot/pkg/xds/sds.go
+++ b/pilot/pkg/xds/sds.go
@@ -22,11 +22,11 @@
 	tls "github.com/envoyproxy/go-control-plane/envoy/extensions/transport_sockets/tls/v3"
 	discovery "github.com/envoyproxy/go-control-plane/envoy/service/discovery/v3"
 
+	credscontroller "istio.io/istio/pilot/pkg/credentials"
 	"istio.io/istio/pilot/pkg/features"
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/model/credentials"
 	"istio.io/istio/pilot/pkg/networking/util"
-	"istio.io/istio/pilot/pkg/secrets"
 	"istio.io/istio/pkg/cluster"
 	"istio.io/istio/pkg/config"
 	"istio.io/istio/pkg/config/schema/gvk"
@@ -90,7 +90,7 @@ func (s *SecretGen) parseResources(names []string, proxy *model.Proxy) []SecretR
 func (s *SecretGen) Generate(proxy *model.Proxy, push *model.PushContext, w *model.WatchedResource,
 	req *model.PushRequest) (model.Resources, model.XdsLogDetails, error) {
 	if proxy.VerifiedIdentity == nil {
-		log.Warnf("proxy %v is not authorized to receive secrets. Ensure you are connecting over TLS port and are authenticated.", proxy.ID)
+		log.Warnf("proxy %v is not authorized to receive credscontroller. Ensure you are connecting over TLS port and are authenticated.", proxy.ID)
 		return nil, model.DefaultXdsLogDetails, nil
 	}
 	if req == nil || !needsUpdate(proxy, req.ConfigsUpdated) {
@@ -125,13 +125,13 @@ func (s *SecretGen) Generate(proxy *model.Proxy, push *model.PushContext, w *mod
 	for _, sr := range resources {
 		if updatedSecrets != nil {
 			if !containsAny(updatedSecrets, relatedConfigs(model.ConfigKey{Kind: gvk.Secret, Name: sr.Name, Namespace: sr.Namespace})) {
-				// This is an incremental update, filter out secrets that are not updated.
+				// This is an incremental update, filter out credscontroller that are not updated.
 				continue
 			}
 		}
 
-		// Fetch the appropriate cluster's secrets, based on the credential type
-		var secretController secrets.Controller
+		// Fetch the appropriate cluster's credscontroller, based on the credential type
+		var secretController credscontroller.Controller
 		switch sr.Type {
 		case credentials.KubernetesGatewaySecretType:
 			secretController = configClusterSecrets
@@ -176,10 +176,10 @@ func (s *SecretGen) Generate(proxy *model.Proxy, push *model.PushContext, w *mod
 }
 
 // filterAuthorizedResources takes a list of SecretResource and filters out resources that proxy cannot access
-func filterAuthorizedResources(resources []SecretResource, proxy *model.Proxy, secrets secrets.Controller) []SecretResource {
+func filterAuthorizedResources(resources []SecretResource, proxy *model.Proxy, secrets credscontroller.Controller) []SecretResource {
 	var authzResult *bool
 	var authzError error
-	// isAuthorized is a small wrapper around secrets.Authorize so we only call it once instead of each time in the loop
+	// isAuthorized is a small wrapper around credscontroller.Authorize so we only call it once instead of each time in the loop
 	isAuthorized := func() bool {
 		if authzResult != nil {
 			return *authzResult
@@ -316,13 +316,13 @@ func containsAny(mp map[model.ConfigKey]struct{}, keys []model.ConfigKey) bool {
 // but we need to push both the `foo` and `foo-cacert` resource name, or they will fall out of sync.
 func relatedConfigs(k model.ConfigKey) []model.ConfigKey {
 	related := []model.ConfigKey{k}
-	// For secrets without -cacert suffix, add the suffix
+	// For credscontroller without -cacert suffix, add the suffix
 	if !strings.HasSuffix(k.Name, GatewaySdsCaSuffix) {
 		withSuffix := k
 		withSuffix.Name += GatewaySdsCaSuffix
 		related = append(related, withSuffix)
 	}
-	// For secrets with -cacert suffix, remove the suffix
+	// For credscontroller with -cacert suffix, remove the suffix
 	if strings.HasSuffix(k.Name, GatewaySdsCaSuffix) {
 		withoutSuffix := k
 		withoutSuffix.Name = strings.TrimSuffix(withoutSuffix.Name, GatewaySdsCaSuffix)
@@ -332,7 +332,7 @@ func relatedConfigs(k model.ConfigKey) []model.ConfigKey {
 }
 
 type SecretGen struct {
-	secrets secrets.MulticlusterController
+	secrets credscontroller.MulticlusterController
 	// Cache for XDS resources
 	cache         model.XdsCache
 	configCluster cluster.ID
@@ -340,8 +340,8 @@ type SecretGen struct {
 
 var _ model.XdsResourceGenerator = &SecretGen{}
 
-func NewSecretGen(sc secrets.MulticlusterController, cache model.XdsCache, configCluster cluster.ID) *SecretGen {
-	// TODO: Currently we only have a single secrets controller (Kubernetes). In the future, we will need a mapping
+func NewSecretGen(sc credscontroller.MulticlusterController, cache model.XdsCache, configCluster cluster.ID) *SecretGen {
+	// TODO: Currently we only have a single credentials controller (Kubernetes). In the future, we will need a mapping
 	// of resource type to secret controller (ie kubernetes:// -> KubernetesController, vault:// -> VaultController)
 	return &SecretGen{
 		secrets:       sc,
diff --git a/pilot/pkg/xds/sds_test.go b/pilot/pkg/xds/sds_test.go
index 283288e4f7..0f5a72b05a 100644
--- a/pilot/pkg/xds/sds_test.go
+++ b/pilot/pkg/xds/sds_test.go
@@ -28,8 +28,8 @@
 	"k8s.io/client-go/kubernetes/fake"
 	k8stesting "k8s.io/client-go/testing"
 
+	credentials "istio.io/istio/pilot/pkg/credentials/kube"
 	"istio.io/istio/pilot/pkg/model"
-	kubesecrets "istio.io/istio/pilot/pkg/secrets/kube"
 	v3 "istio.io/istio/pilot/pkg/xds/v3"
 	"istio.io/istio/pilot/test/xdstest"
 	"istio.io/istio/pkg/config/schema/gvk"
@@ -53,16 +53,16 @@ func makeSecret(name string, data map[string]string) *corev1.Secret {
 
 var (
 	genericCert = makeSecret("generic", map[string]string{
-		kubesecrets.GenericScrtCert: "generic-cert", kubesecrets.GenericScrtKey: "generic-key",
+		credentials.GenericScrtCert: "generic-cert", credentials.GenericScrtKey: "generic-key",
 	})
 	genericMtlsCert = makeSecret("generic-mtls", map[string]string{
-		kubesecrets.GenericScrtCert: "generic-mtls-cert", kubesecrets.GenericScrtKey: "generic-mtls-key", kubesecrets.GenericScrtCaCert: "generic-mtls-ca",
+		credentials.GenericScrtCert: "generic-mtls-cert", credentials.GenericScrtKey: "generic-mtls-key", credentials.GenericScrtCaCert: "generic-mtls-ca",
 	})
 	genericMtlsCertSplit = makeSecret("generic-mtls-split", map[string]string{
-		kubesecrets.GenericScrtCert: "generic-mtls-split-cert", kubesecrets.GenericScrtKey: "generic-mtls-split-key",
+		credentials.GenericScrtCert: "generic-mtls-split-cert", credentials.GenericScrtKey: "generic-mtls-split-key",
 	})
 	genericMtlsCertSplitCa = makeSecret("generic-mtls-split-cacert", map[string]string{
-		kubesecrets.GenericScrtCaCert: "generic-mtls-split-ca",
+		credentials.GenericScrtCaCert: "generic-mtls-split-ca",
 	})
 )
 
@@ -263,7 +263,7 @@ type Expected struct {
 			if tt.accessReviewResponse != nil {
 				cc.Fake.PrependReactor("create", "subjectaccessreviews", tt.accessReviewResponse)
 			} else {
-				kubesecrets.DisableAuthorizationForTest(cc)
+				credentials.DisableAuthorizationForTest(cc)
 			}
 			cc.Fake.Unlock()
 
@@ -296,7 +296,7 @@ func TestCaching(t *testing.T) {
 		KubernetesObjects: []runtime.Object{genericCert},
 		KubeClientModifier: func(c kube.Client) {
 			cc := c.Kube().(*fake.Clientset)
-			kubesecrets.DisableAuthorizationForTest(cc)
+			credentials.DisableAuthorizationForTest(cc)
 		},
 	})
 	gen := s.Discovery.Generators[v3.SecretType]
diff --git a/pkg/kube/secretcontroller/secretcontroller.go b/pkg/kube/multicluster/secretcontroller.go
similarity index 68%
rename from pkg/kube/secretcontroller/secretcontroller.go
rename to pkg/kube/multicluster/secretcontroller.go
index 721c8500a1..a8bda12674 100644
--- a/pkg/kube/secretcontroller/secretcontroller.go
+++ b/pkg/kube/multicluster/secretcontroller.go
@@ -12,7 +12,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
-package secretcontroller
+package multicluster
 
 import (
 	"bytes"
@@ -24,6 +24,7 @@
 	"sync"
 	"time"
 
+	"github.com/hashicorp/go-multierror"
 	"go.uber.org/atomic"
 	corev1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
@@ -51,30 +52,44 @@
 
 func init() {
 	monitoring.MustRegister(timeouts)
+	monitoring.MustRegister(clustersCount)
 }
 
-var timeouts = monitoring.NewSum(
-	"remote_cluster_sync_timeouts_total",
-	"Number of times remote clusters took too long to sync, causing slow startup that excludes remote clusters.",
-)
+var (
+	timeouts = monitoring.NewSum(
+		"remote_cluster_sync_timeouts_total",
+		"Number of times remote clusters took too long to sync, causing slow startup that excludes remote clusters.",
+	)
+
+	clusterType = monitoring.MustCreateLabel("cluster_type")
+
+	clustersCount = monitoring.NewGauge(
+		"istiod_managed_clusters",
+		"Number of clusters managed by istiod",
+		monitoring.WithLabels(clusterType),
+	)
 
-// newClientCallback prototype for the add secret callback function.
-type newClientCallback func(clusterID cluster.ID, cluster *Cluster) error
+	localClusters  = clustersCount.With(clusterType.Value("local"))
+	remoteClusters = clustersCount.With(clusterType.Value("remote"))
+)
 
-// removeClientCallback prototype for the remove secret callback function.
-type removeClientCallback func(clusterID cluster.ID) error
+type ClusterHandler interface {
+	ClusterAdded(cluster *Cluster, stop <-chan struct{}) error
+	ClusterUpdated(cluster *Cluster, stop <-chan struct{}) error
+	ClusterDeleted(clusterID cluster.ID) error
+}
 
 // Controller is the controller implementation for Secret resources
 type Controller struct {
-	namespace string
-	queue     workqueue.RateLimitingInterface
-	informer  cache.SharedIndexInformer
+	namespace          string
+	localClusterID     cluster.ID
+	localClusterClient kube.Client
+	queue              workqueue.RateLimitingInterface
+	informer           cache.SharedIndexInformer
 
 	cs *ClusterStore
 
-	addCallback    newClientCallback
-	updateCallback newClientCallback
-	removeCallback removeClientCallback
+	handlers []ClusterHandler
 
 	once              sync.Once
 	syncInterval      time.Duration
@@ -84,24 +99,35 @@ type Controller struct {
 
 // Cluster defines cluster struct
 type Cluster struct {
-	clusterID     string
-	kubeConfigSha [sha256.Size]byte
-
+	// ID of the cluster.
+	ID cluster.ID
+	// SyncTimeout is marked after features.RemoteClusterTimeout.
+	SyncTimeout *atomic.Bool
 	// Client for accessing the cluster.
 	Client kube.Client
-	// Stop channel which is closed when the cluster is removed or the secretcontroller that created the client is stopped.
-	// Client.RunAndWait is called using this channel.
-	Stop chan struct{}
+
+	kubeConfigSha [sha256.Size]byte
+
+	stop chan struct{}
 	// initialSync is marked when RunAndWait completes
 	initialSync *atomic.Bool
-	// SyncTimeout is marked after features.RemoteClusterTimeout
-	SyncTimeout *atomic.Bool
+}
+
+// Stop channel which is closed when the cluster is removed or the Controller that created the client is stopped.
+// Client.RunAndWait is called using this channel.
+func (r *Cluster) Stop() <-chan struct{} {
+	return r.stop
+}
+
+func (c *Controller) AddHandler(h ClusterHandler) {
+	log.Infof("handling remote clusters in %T", h)
+	c.handlers = append(c.handlers, h)
 }
 
 // Run starts the cluster's informers and waits for caches to sync. Once caches are synced, we mark the cluster synced.
 // This should be called after each of the handlers have registered informers, and should be run in a goroutine.
 func (r *Cluster) Run() {
-	r.Client.RunAndWait(r.Stop)
+	r.Client.RunAndWait(r.Stop())
 	r.initialSync.Store(true)
 }
 
@@ -145,6 +171,18 @@ func (c *ClusterStore) Get(secretKey string, clusterID cluster.ID) *Cluster {
 	return c.remoteClusters[secretKey][clusterID]
 }
 
+func (c *ClusterStore) GetByID(clusterID cluster.ID) *Cluster {
+	c.RLock()
+	defer c.RUnlock()
+	for _, clusters := range c.remoteClusters {
+		c, ok := clusters[clusterID]
+		if ok {
+			return c
+		}
+	}
+	return nil
+}
+
 // All returns a copy of the current remote clusters.
 func (c *ClusterStore) All() map[string]map[cluster.ID]*Cluster {
 	if c == nil {
@@ -185,12 +223,7 @@ func (c *ClusterStore) Len() int {
 }
 
 // NewController returns a new secret controller
-func NewController(
-	kubeclientset kubernetes.Interface,
-	namespace string,
-	addCallback newClientCallback,
-	updateCallback newClientCallback,
-	removeCallback removeClientCallback) *Controller {
+func NewController(kubeclientset kube.Client, namespace string, localClusterID cluster.ID) *Controller {
 	secretsInformer := cache.NewSharedIndexInformer(
 		&cache.ListWatch{
 			ListFunc: func(opts metav1.ListOptions) (runtime.Object, error) {
@@ -205,16 +238,20 @@ func NewController(
 		&corev1.Secret{}, 0, cache.Indexers{},
 	)
 
+	// init gauges
+	localClusters.Record(1.0)
+	remoteClusters.Record(0.0)
+
 	queue := workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter())
 
 	controller := &Controller{
-		namespace:      namespace,
-		cs:             newClustersStore(),
-		informer:       secretsInformer,
-		queue:          queue,
-		addCallback:    addCallback,
-		updateCallback: updateCallback,
-		removeCallback: removeCallback,
+		namespace:          namespace,
+		localClusterID:     localClusterID,
+		localClusterClient: kubeclientset,
+		cs:                 newClustersStore(),
+		informer:           secretsInformer,
+		queue:              queue,
+		syncInterval:       100 * time.Millisecond,
 	}
 
 	secretsInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
@@ -249,30 +286,39 @@ func NewController(
 }
 
 // Run starts the controller until it receives a message over stopCh
-func (c *Controller) Run(stopCh <-chan struct{}) {
-	defer utilruntime.HandleCrash()
-	defer c.queue.ShutDown()
+func (c *Controller) Run(stopCh <-chan struct{}) error {
+	// run handlers for the local cluster; do not store this *Cluster in the ClusterStore or give it a SyncTimeout
+	// this is done outside the goroutine, we should block other Run/startFuncs until this is registered
+	localCluster := &Cluster{Client: c.localClusterClient, ID: c.localClusterID}
+	if err := c.handleAdd(localCluster, stopCh); err != nil {
+		return fmt.Errorf("failed initializing local cluster %s: %v", c.localClusterID, err)
+	}
+	go func() {
+		defer utilruntime.HandleCrash()
+		defer c.queue.ShutDown()
 
-	t0 := time.Now()
-	log.Info("Starting Secrets controller")
+		t0 := time.Now()
+		log.Info("Starting multicluster remote secrets controller")
 
-	go c.informer.Run(stopCh)
+		go c.informer.Run(stopCh)
 
-	if !kube.WaitForCacheSyncInterval(stopCh, c.syncInterval, c.informer.HasSynced) {
-		log.Error("Failed to sync secret controller cache")
-		return
-	}
-	log.Infof("Secret controller cache synced in %v", time.Since(t0))
-	// all secret events before this signal must be processed before we're marked "ready"
-	c.queue.Add(initialSyncSignal)
-	if features.RemoteClusterTimeout != 0 {
-		time.AfterFunc(features.RemoteClusterTimeout, func() {
-			c.remoteSyncTimeout.Store(true)
-		})
-	}
-	go wait.Until(c.runWorker, 5*time.Second, stopCh)
-	<-stopCh
-	c.close()
+		if !kube.WaitForCacheSyncInterval(stopCh, c.syncInterval, c.informer.HasSynced) {
+			log.Error("Failed to sync multicluster remote secrets controller cache")
+			return
+		}
+		log.Infof("multicluster remote secrets controller cache synced in %v", time.Since(t0))
+		// all secret events before this signal must be processed before we're marked "ready"
+		c.queue.Add(initialSyncSignal)
+		if features.RemoteClusterTimeout != 0 {
+			time.AfterFunc(features.RemoteClusterTimeout, func() {
+				c.remoteSyncTimeout.Store(true)
+			})
+		}
+		go wait.Until(c.runWorker, 5*time.Second, stopCh)
+		<-stopCh
+		c.close()
+	}()
+	return nil
 }
 
 func (c *Controller) close() {
@@ -280,7 +326,7 @@ func (c *Controller) close() {
 	defer c.cs.Unlock()
 	for _, clusterMap := range c.cs.remoteClusters {
 		for _, cluster := range clusterMap {
-			close(cluster.Stop)
+			close(cluster.stop)
 		}
 	}
 }
@@ -296,7 +342,7 @@ func (c *Controller) hasSynced() bool {
 	for _, clusterMap := range c.cs.remoteClusters {
 		for _, cluster := range clusterMap {
 			if !cluster.HasSynced() {
-				log.Debugf("remote cluster %s registered informers have not been synced up yet", cluster.clusterID)
+				log.Debugf("remote cluster %s registered informers have not been synced up yet", cluster.ID)
 				return false
 			}
 		}
@@ -321,23 +367,6 @@ func (c *Controller) HasSynced() bool {
 	return synced
 }
 
-// StartSecretController creates the secret controller.
-func StartSecretController(
-	kubeclientset kubernetes.Interface,
-	addCallback newClientCallback, updateCallback newClientCallback,
-	removeCallback removeClientCallback,
-	namespace string,
-	syncInterval time.Duration,
-	stop <-chan struct{},
-) *Controller {
-	controller := NewController(kubeclientset, namespace, addCallback, updateCallback, removeCallback)
-	controller.syncInterval = syncInterval
-
-	go controller.Run(stop)
-
-	return controller
-}
-
 func (c *Controller) runWorker() {
 	for c.processNextItem() {
 	}
@@ -364,6 +393,7 @@ func (c *Controller) processNextItem() bool {
 		log.Errorf("Error processing %s (giving up): %v", key, err)
 		c.queue.Forget(key)
 	}
+	remoteClusters.Record(float64(c.cs.Len()))
 
 	return true
 }
@@ -420,10 +450,9 @@ func (c *Controller) createRemoteCluster(kubeConfig []byte, clusterID string) (*
 		return nil, err
 	}
 	return &Cluster{
-		clusterID: clusterID,
-		Client:    clients,
-		// access outside this package should only be reading
-		Stop: make(chan struct{}),
+		ID:     cluster.ID(clusterID),
+		Client: clients,
+		stop:   make(chan struct{}),
 		// for use inside the package, to close on cleanup
 		initialSync:   atomic.NewBool(false),
 		SyncTimeout:   &c.remoteSyncTimeout,
@@ -435,15 +464,15 @@ func (c *Controller) addSecret(secretKey string, s *corev1.Secret) {
 	// First delete clusters
 	existingClusters := c.cs.GetExistingClustersFor(secretKey)
 	for _, existingCluster := range existingClusters {
-		if _, ok := s.Data[existingCluster.clusterID]; !ok {
-			c.deleteMemberCluster(secretKey, cluster.ID(existingCluster.clusterID))
+		if _, ok := s.Data[string(existingCluster.ID)]; !ok {
+			c.deleteCluster(secretKey, existingCluster.ID)
 		}
 	}
 
 	for clusterID, kubeConfig := range s.Data {
-		action, callback := "Adding", c.addCallback
+		action, callback := "Adding", c.handleAdd
 		if prev := c.cs.Get(secretKey, cluster.ID(clusterID)); prev != nil {
-			action, callback = "Updating", c.updateCallback
+			action, callback = "Updating", c.handleUpdate
 			// clusterID must be unique even across multiple secrets
 			// TODO： warning
 			kubeConfigSha := sha256.Sum256(kubeConfig)
@@ -452,6 +481,10 @@ func (c *Controller) addSecret(secretKey string, s *corev1.Secret) {
 				continue
 			}
 		}
+		if cluster.ID(clusterID) == c.localClusterID {
+			log.Infof("ignoring %s cluster %v from secret %v as it would overwrite the local cluster", action, clusterID, secretKey)
+			continue
+		}
 		log.Infof("%s cluster %v from secret %v", action, clusterID, secretKey)
 
 		remoteCluster, err := c.createRemoteCluster(kubeConfig, clusterID)
@@ -459,8 +492,8 @@ func (c *Controller) addSecret(secretKey string, s *corev1.Secret) {
 			log.Errorf("%s cluster_id=%v from secret=%v: %v", action, clusterID, secretKey, err)
 			continue
 		}
-		c.cs.Store(secretKey, cluster.ID(clusterID), remoteCluster)
-		if err := callback(cluster.ID(clusterID), remoteCluster); err != nil {
+		c.cs.Store(secretKey, remoteCluster.ID, remoteCluster)
+		if err := callback(remoteCluster, remoteCluster.stop); err != nil {
 			log.Errorf("%s cluster_id from secret=%v: %s %v", action, clusterID, secretKey, err)
 			continue
 		}
@@ -478,34 +511,62 @@ func (c *Controller) deleteSecret(secretKey string) {
 		log.Infof("Number of remote clusters: %d", c.cs.Len())
 	}()
 	for clusterID, cluster := range c.cs.remoteClusters[secretKey] {
+		if clusterID == c.localClusterID {
+			log.Infof("ignoring delete cluster %v from secret %v as it would overwrite the local cluster", clusterID, secretKey)
+			continue
+		}
 		log.Infof("Deleting cluster_id=%v configured by secret=%v", clusterID, secretKey)
-		err := c.removeCallback(clusterID)
+		err := c.handleDelete(clusterID)
 		if err != nil {
 			log.Errorf("Error removing cluster_id=%v configured by secret=%v: %v",
 				clusterID, secretKey, err)
 		}
-		close(cluster.Stop)
+		close(cluster.stop)
 		delete(c.cs.remoteClusters[secretKey], clusterID)
 	}
 	delete(c.cs.remoteClusters, secretKey)
 }
 
-func (c *Controller) deleteMemberCluster(secretKey string, clusterID cluster.ID) {
+func (c *Controller) deleteCluster(secretKey string, clusterID cluster.ID) {
 	c.cs.Lock()
 	defer func() {
 		c.cs.Unlock()
 		log.Infof("Number of remote clusters: %d", c.cs.Len())
 	}()
 	log.Infof("Deleting cluster_id=%v configured by secret=%v", clusterID, secretKey)
-	err := c.removeCallback(clusterID)
+	err := c.handleDelete(clusterID)
 	if err != nil {
 		log.Errorf("Error removing cluster_id=%v configured by secret=%v: %v",
 			clusterID, secretKey, err)
 	}
-	close(c.cs.remoteClusters[secretKey][clusterID].Stop)
+	close(c.cs.remoteClusters[secretKey][clusterID].stop)
 	delete(c.cs.remoteClusters[secretKey], clusterID)
 }
 
+func (c *Controller) handleAdd(cluster *Cluster, stop <-chan struct{}) error {
+	var errs *multierror.Error
+	for _, handler := range c.handlers {
+		errs = multierror.Append(errs, handler.ClusterAdded(cluster, stop))
+	}
+	return errs.ErrorOrNil()
+}
+
+func (c *Controller) handleUpdate(cluster *Cluster, stop <-chan struct{}) error {
+	var errs *multierror.Error
+	for _, handler := range c.handlers {
+		errs = multierror.Append(errs, handler.ClusterUpdated(cluster, stop))
+	}
+	return errs.ErrorOrNil()
+}
+
+func (c *Controller) handleDelete(key cluster.ID) error {
+	var errs *multierror.Error
+	for _, handler := range c.handlers {
+		errs = multierror.Append(errs, handler.ClusterDeleted(key))
+	}
+	return errs.ErrorOrNil()
+}
+
 // ListRemoteClusters provides debug info about connected remote clusters.
 func (c *Controller) ListRemoteClusters() []cluster.DebugInfo {
 	var out []cluster.DebugInfo
@@ -527,3 +588,10 @@ func (c *Controller) ListRemoteClusters() []cluster.DebugInfo {
 	}
 	return out
 }
+
+func (c *Controller) GetRemoteKubeClient(clusterID cluster.ID) kubernetes.Interface {
+	if remoteCluster := c.cs.GetByID(clusterID); remoteCluster != nil {
+		return remoteCluster.Client
+	}
+	return nil
+}
diff --git a/pkg/kube/secretcontroller/secretcontroller_test.go b/pkg/kube/multicluster/secretcontroller_test.go
similarity index 92%
rename from pkg/kube/secretcontroller/secretcontroller_test.go
rename to pkg/kube/multicluster/secretcontroller_test.go
index bcc806899a..176236a9e7 100644
--- a/pkg/kube/secretcontroller/secretcontroller_test.go
+++ b/pkg/kube/multicluster/secretcontroller_test.go
@@ -12,7 +12,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 
-package secretcontroller
+package multicluster
 
 import (
 	"context"
@@ -63,21 +63,25 @@ func makeSecret(secret string, clusterConfigs ...clusterCredential) *v1.Secret {
 	deleted cluster.ID
 )
 
-func addCallback(id cluster.ID, _ *Cluster) error {
+var _ ClusterHandler = &handler{}
+
+type handler struct{}
+
+func (h handler) ClusterAdded(cluster *Cluster, stop <-chan struct{}) error {
 	mu.Lock()
 	defer mu.Unlock()
-	added = id
+	added = cluster.ID
 	return nil
 }
 
-func updateCallback(id cluster.ID, _ *Cluster) error {
+func (h handler) ClusterUpdated(cluster *Cluster, stop <-chan struct{}) error {
 	mu.Lock()
 	defer mu.Unlock()
-	updated = id
+	updated = cluster.ID
 	return nil
 }
 
-func deleteCallback(id cluster.ID) error {
+func (h handler) ClusterDeleted(id cluster.ID) error {
 	mu.Lock()
 	defer mu.Unlock()
 	deleted = id
@@ -133,7 +137,9 @@ func Test_SecretController(t *testing.T) {
 	t.Cleanup(func() {
 		close(stopCh)
 	})
-	c := StartSecretController(clientset, addCallback, updateCallback, deleteCallback, secretNamespace, time.Microsecond, stopCh)
+	c := NewController(clientset, secretNamespace, "")
+	c.AddHandler(&handler{})
+	_ = c.Run(stopCh)
 	t.Run("sync timeout", func(t *testing.T) {
 		retry.UntilOrFail(t, c.HasSynced, retry.Timeout(2*time.Second))
 	})
diff --git a/pkg/test/framework/components/istio/operator.go b/pkg/test/framework/components/istio/operator.go
index 6e7fb73550..c5a15a2ba3 100644
--- a/pkg/test/framework/components/istio/operator.go
+++ b/pkg/test/framework/components/istio/operator.go
@@ -282,6 +282,7 @@ func (i *operatorComponent) Dump(ctx resource.Context) {
 	kube2.DumpWebhooks(ctx, d)
 	for _, c := range ctx.Clusters().Kube() {
 		kube2.DumpDebug(ctx, c, d, "configz")
+		kube2.DumpDebug(ctx, c, d, "clusterz")
 	}
 	// Dump istio-cni.
 	kube2.DumpPods(ctx, d, "kube-system", []string{"k8s-app=istio-cni-node"})
-- 
2.35.3

