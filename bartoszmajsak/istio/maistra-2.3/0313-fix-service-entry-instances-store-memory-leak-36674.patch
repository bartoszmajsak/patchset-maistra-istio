From f2e05be5fcc600e129fcb30cf37f6be0d35ddeaf Mon Sep 17 00:00:00 2001
From: dwq <41563853+dddddai@users.noreply.github.com>
Date: Fri, 31 Dec 2021 15:10:30 +0800
Subject: fix service entry instances store memory leak (#36674)

Signed-off-by: dddddai <dddwq@foxmail.com>
---
 .../serviceentry/servicediscovery.go          |  2 ++
 .../serviceentry/servicediscovery_test.go     | 24 +++++++++++++++++++
 2 files changed, 26 insertions(+)

diff --git a/pilot/pkg/serviceregistry/serviceentry/servicediscovery.go b/pilot/pkg/serviceregistry/serviceentry/servicediscovery.go
index e6fad0e3e9..d473f5b660 100644
--- a/pilot/pkg/serviceregistry/serviceentry/servicediscovery.go
+++ b/pilot/pkg/serviceregistry/serviceentry/servicediscovery.go
@@ -463,6 +463,8 @@ func (s *ServiceEntryStore) WorkloadInstanceHandler(wi *model.WorkloadInstance,
 				instancesDeleted = append(instancesDeleted, di)
 			}
 			s.serviceInstances.deleteServiceEntryInstances(seNamespacedName, key)
+		} else if event == model.EventDelete {
+			s.serviceInstances.deleteServiceEntryInstances(seNamespacedName, key)
 		} else {
 			s.serviceInstances.updateServiceEntryInstancesPerConfig(seNamespacedName, key, instance)
 		}
diff --git a/pilot/pkg/serviceregistry/serviceentry/servicediscovery_test.go b/pilot/pkg/serviceregistry/serviceentry/servicediscovery_test.go
index 2b00fda0e0..5c7bb81697 100644
--- a/pilot/pkg/serviceregistry/serviceentry/servicediscovery_test.go
+++ b/pilot/pkg/serviceregistry/serviceentry/servicediscovery_test.go
@@ -22,6 +22,8 @@
 	"testing"
 	"time"
 
+	"k8s.io/apimachinery/pkg/types"
+
 	"istio.io/api/label"
 	networking "istio.io/api/networking/v1alpha3"
 	"istio.io/istio/pilot/pkg/config/memory"
@@ -972,6 +974,18 @@ func TestServiceDiscoveryWorkloadInstance(t *testing.T) {
 		expectServiceInstances(t, sd, selector, 0, instances)
 		expectEvents(t, events, Event{kind: "eds", host: "selector.com", namespace: selector.Namespace, endpoints: 2})
 
+		key := instancesKey{namespace: selector.Namespace, hostname: "selector.com"}
+		namespacedName := types.NamespacedName{Namespace: selector.Namespace, Name: selector.Name}
+		if len(sd.serviceInstances.ip2instance) != 1 {
+			t.Fatalf("service instances store `ip2instance` memory leak, expect 1, got %d", len(sd.serviceInstances.ip2instance))
+		}
+		if len(sd.serviceInstances.instances[key]) != 1 {
+			t.Fatalf("service instances store `instances` memory leak, expect 1, got %d", len(sd.serviceInstances.instances[key]))
+		}
+		if len(sd.serviceInstances.instancesBySE[namespacedName]) != 1 {
+			t.Fatalf("service instances store `instancesBySE` memory leak, expect 1, got %d", len(sd.serviceInstances.instancesBySE[namespacedName]))
+		}
+
 		// The following sections mimic this scenario:
 		// f1 starts terminating, f3 picks up the IP, f3 delete event (pod
 		// not ready yet) comes before f1
@@ -988,6 +1002,16 @@ func TestServiceDiscoveryWorkloadInstance(t *testing.T) {
 		expectServiceInstances(t, sd, selector, 0, instances)
 		expectEvents(t, events, Event{kind: "eds", host: "selector.com", namespace: selector.Namespace, endpoints: 0})
 
+		if len(sd.serviceInstances.ip2instance) != 0 {
+			t.Fatalf("service instances store `ip2instance` memory leak, expect 0, got %d", len(sd.serviceInstances.ip2instance))
+		}
+		if len(sd.serviceInstances.instances[key]) != 0 {
+			t.Fatalf("service instances store `instances` memory leak, expect 0, got %d", len(sd.serviceInstances.instances[key]))
+		}
+		if len(sd.serviceInstances.instancesBySE[namespacedName]) != 0 {
+			t.Fatalf("service instances store `instancesBySE` memory leak, expect 0, got %d", len(sd.serviceInstances.instancesBySE[namespacedName]))
+		}
+
 		// Add f3 event
 		callInstanceHandlers([]*model.WorkloadInstance{fi3}, sd, model.EventAdd, t)
 		instances = []*model.ServiceInstance{
-- 
2.35.3

