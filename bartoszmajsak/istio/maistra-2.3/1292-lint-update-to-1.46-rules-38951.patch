From 32afc27ea91ef68e4e5f0ee3dd47d0aedfba35f1 Mon Sep 17 00:00:00 2001
From: John Howard <howardjohn@google.com>
Date: Thu, 19 May 2022 16:29:30 -0700
Subject: lint: update to 1.46 rules (#38951)

* Rerun gofumpt

* Update remaining issues

* revert accidental change

* fix copy order
---
 cni/pkg/plugin/iptables_unspecified.go        |  6 ++--
 cni/pkg/repair/repair_test.go                 |  1 +
 cni/pkg/taint/taintcontroller_test.go         |  3 +-
 cni/test/install_cni.go                       |  9 +++--
 istioctl/cmd/add-to-mesh.go                   | 12 ++++---
 istioctl/cmd/dashboard.go                     |  3 +-
 istioctl/cmd/injector-list.go                 |  3 +-
 istioctl/cmd/internal-debug.go                |  6 ++--
 istioctl/cmd/kubeinject.go                    |  3 +-
 istioctl/cmd/metrics_test.go                  |  3 +-
 istioctl/cmd/remove-from-mesh.go              |  6 ++--
 istioctl/cmd/revision.go                      |  3 +-
 istioctl/cmd/wait.go                          |  3 +-
 istioctl/cmd/workload.go                      |  3 +-
 istioctl/pkg/multixds/gather.go               |  9 +++--
 istioctl/pkg/verifier/verifier.go             |  5 +--
 istioctl/pkg/writer/compare/sds/util.go       |  3 +-
 istioctl/pkg/xds/client.go                    |  6 ++--
 operator/cmd/mesh/install.go                  |  9 +++--
 operator/cmd/mesh/manifest-diff.go            |  6 ++--
 operator/cmd/mesh/shared.go                   |  3 +-
 operator/cmd/mesh/uninstall.go                |  3 +-
 operator/pkg/helmreconciler/prune.go          |  9 +++--
 operator/pkg/helmreconciler/wait.go           |  3 +-
 operator/pkg/manifest/shared.go               | 15 ++++++---
 operator/pkg/translate/translate.go           | 32 ++++++++++--------
 operator/pkg/translate/translate_value.go     |  9 +++--
 pilot/cmd/pilot-agent/app/cmd.go              |  3 +-
 pilot/cmd/pilot-agent/options/security.go     |  3 +-
 pilot/pkg/bootstrap/istio_ca.go               | 12 ++++---
 pilot/pkg/config/kube/gateway/conversion.go   |  9 +++--
 pilot/pkg/config/kube/ingress/controller.go   |  3 +-
 pilot/pkg/config/kube/ingress/conversion.go   |  3 +-
 pilot/pkg/config/kube/ingressv1/controller.go |  3 +-
 pilot/pkg/config/kube/ingressv1/conversion.go |  6 ++--
 .../pkg/leaderelection/leaderelection_test.go |  6 ++--
 pilot/pkg/model/authentication.go             |  9 +++--
 pilot/pkg/model/authentication_test.go        |  3 +-
 pilot/pkg/model/extensions.go                 |  3 +-
 pilot/pkg/model/gateway_test.go               |  3 +-
 pilot/pkg/model/push_context.go               |  3 +-
 pilot/pkg/model/service.go                    |  4 +--
 pilot/pkg/model/sidecar.go                    |  3 +-
 pilot/pkg/model/virtualservice.go             |  3 +-
 .../pkg/networking/core/v1alpha3/accesslog.go | 12 ++++---
 pilot/pkg/networking/core/v1alpha3/cluster.go | 27 ++++++++++-----
 .../core/v1alpha3/cluster_builder.go          | 21 ++++++++----
 .../v1alpha3/envoyfilter/cluster_patch.go     |  3 +-
 .../v1alpha3/envoyfilter/listener_patch.go    | 33 ++++++++++++-------
 .../envoyfilter/listener_patch_test.go        |  3 +-
 .../core/v1alpha3/envoyfilter/rc_patch.go     | 18 ++++++----
 .../core/v1alpha3/extension/wasmplugin.go     |  6 ++--
 .../core/v1alpha3/extension_config_builder.go |  3 +-
 pilot/pkg/networking/core/v1alpha3/gateway.go | 19 +++++++----
 .../core/v1alpha3/httproute_test.go           |  7 ----
 .../pkg/networking/core/v1alpha3/listener.go  | 24 +++++++++-----
 .../networking/core/v1alpha3/listener_test.go |  3 +-
 .../v1alpha3/loadbalancer/loadbalancer.go     | 12 ++++---
 .../networking/core/v1alpha3/networkfilter.go |  9 +++--
 pilot/pkg/networking/core/v1alpha3/tls.go     |  9 +++--
 pilot/pkg/networking/core/v1alpha3/tracing.go | 17 ++++++----
 .../networking/core/v1alpha3/tracing_test.go  | 15 ++++++---
 pilot/pkg/networking/grpcgen/lds.go           |  6 ++--
 pilot/pkg/networking/networking.go            |  3 +-
 pilot/pkg/networking/util/util.go             |  3 +-
 pilot/pkg/security/authz/builder/extauthz.go  | 12 ++++---
 pilot/pkg/security/model/authentication.go    |  3 +-
 .../kube/controller/controller.go             |  6 ++--
 .../kube/controller/controller_test.go        | 16 +++++----
 .../kube/controller/endpoint_builder.go       |  3 +-
 .../kube/controller/endpoints.go              |  6 ++--
 .../kube/controller/multicluster.go           |  3 +-
 .../serviceentry/controller.go                |  6 ++--
 .../serviceentry/conversion.go                | 12 ++++---
 .../serviceentry/conversion_test.go           |  9 +++--
 .../serviceregistry/serviceregistry_test.go   |  6 ++--
 pilot/pkg/simulation/traffic.go               |  8 +++--
 pilot/pkg/xds/cds.go                          |  3 +-
 pilot/pkg/xds/debug.go                        |  6 ++--
 pilot/pkg/xds/delta.go                        |  3 +-
 pilot/pkg/xds/deltaadstest.go                 |  3 +-
 pilot/pkg/xds/ecds.go                         |  3 +-
 pilot/pkg/xds/eds.go                          | 21 ++++++++----
 pilot/pkg/xds/eds_test.go                     |  5 +--
 pilot/pkg/xds/sds.go                          |  3 +-
 pilot/test/xdstest/grpc.go                    |  3 +-
 .../analyzers/gateway/conflictinggateway.go   |  3 +-
 .../virtualservice/destinationhosts.go        |  3 +-
 .../virtualservice/destinationrules.go        |  6 ++--
 pkg/config/analysis/incluster/controller.go   |  3 +-
 pkg/config/analysis/local/istiod_analyze.go   |  6 ++--
 pkg/config/validation/validation.go           | 15 ++++++---
 pkg/dns/client/dns.go                         |  3 +-
 pkg/envoy/agent.go                            |  3 +-
 pkg/istio-agent/xds_proxy_delta.go            |  6 ++--
 pkg/kube/adapter.go                           |  2 --
 pkg/kube/client.go                            |  3 +-
 pkg/kube/inject/inject.go                     |  6 ++--
 pkg/kube/mock_client.go                       |  3 +-
 pkg/kube/rpc_creds.go                         |  6 ++--
 pkg/spiffe/spiffe.go                          | 13 +++++---
 pkg/test/csrctrl/controllers/start_csrctrl.go |  3 +-
 .../framework/components/cluster/clusters.go  |  3 +-
 .../framework/components/istio/operator.go    | 12 ++++---
 pkg/test/framework/components/istio/util.go   |  9 +++--
 pkg/test/kube/util.go                         |  6 ++--
 pkg/test/loadbalancersim/lb_test.go           |  8 ++---
 pkg/wasm/cache.go                             |  3 +-
 pkg/wasm/convert_test.go                      |  3 +-
 .../validation/controller/controller.go       |  6 ++--
 pkg/webhooks/webhookpatch.go                  |  6 ++--
 security/pkg/k8s/chiron/controller.go         |  3 +-
 security/pkg/k8s/controller/casecret.go       |  6 ++--
 .../caclient/providers/google/mock/ca_mock.go |  3 +-
 security/pkg/nodeagent/sds/sdsservice.go      |  2 +-
 security/pkg/nodeagent/test/mock/caserver.go  |  3 +-
 security/pkg/pki/ca/ca.go                     | 18 ++++++----
 .../pkg/pki/ca/selfsignedcarootcertrotator.go |  6 ++--
 .../ca/selfsignedcarootcertrotator_test.go    |  3 +-
 security/pkg/pki/util/generate_cert.go        | 14 +++++---
 security/pkg/pki/util/keycertbundle.go        |  9 +++--
 .../ca/authenticate/kubeauth/kube_jwt.go      |  3 +-
 security/pkg/server/ca/server.go              |  6 ++--
 security/pkg/stsservice/server/server_test.go |  3 +-
 .../google/tokenexchangeplugin_test.go        |  3 +-
 tests/fuzz/ca_server_fuzzer.go                |  5 ++-
 tests/fuzz/kube_gateway_controller_fuzzer.go  |  1 +
 tests/fuzz/security_authz_builder_fuzzer.go   |  4 +--
 tests/integration/helm/util.go                |  6 ++--
 tests/integration/operator/switch_cr_test.go  |  9 +++--
 tests/integration/pilot/grpc_probe_test.go    |  3 +-
 .../discoverability/discoverability_test.go   |  3 +-
 tests/integration/pilot/tcp_probe_test.go     |  3 +-
 .../security/authorization_test.go            | 12 ++++---
 .../egress_gateway_origination_test.go        |  3 +-
 .../egress_gateway_origination_test.go        |  3 +-
 .../security/mtls_healthcheck_test.go         |  3 +-
 .../sds_istio_mutual_egress_test.go           |  3 +-
 .../security/sds_ingress/util/util.go         | 18 ++++++----
 tools/bug-report/pkg/bugreport/bugreport.go   | 12 ++++---
 tools/bug-report/pkg/filter/filter.go         |  3 +-
 tools/istio-iptables/pkg/capture/run.go       | 11 ++++---
 .../pkg/capture/run_unspecified.go            |  8 ++---
 143 files changed, 630 insertions(+), 350 deletions(-)

diff --git a/cni/pkg/plugin/iptables_unspecified.go b/cni/pkg/plugin/iptables_unspecified.go
index 4bb5093b66..1f733b6157 100644
--- a/cni/pkg/plugin/iptables_unspecified.go
+++ b/cni/pkg/plugin/iptables_unspecified.go
@@ -21,10 +21,8 @@
 
 import "errors"
 
-var (
-	// ErrNotImplemented is returned when a requested feature is not implemented.
-	ErrNotImplemented = errors.New("not implemented")
-)
+// ErrNotImplemented is returned when a requested feature is not implemented.
+var ErrNotImplemented = errors.New("not implemented")
 
 // Program defines a method which programs iptables based on the parameters
 // provided in Redirect.
diff --git a/cni/pkg/repair/repair_test.go b/cni/pkg/repair/repair_test.go
index d07579c5bf..32b5443743 100644
--- a/cni/pkg/repair/repair_test.go
+++ b/cni/pkg/repair/repair_test.go
@@ -222,6 +222,7 @@ type args struct {
 		},
 	}
 	for _, tt := range tests {
+		tt := tt
 		t.Run(tt.name, func(t *testing.T) {
 			bpr := brokenPodReconciler{
 				client: fake.NewSimpleClientset(),
diff --git a/cni/pkg/taint/taintcontroller_test.go b/cni/pkg/taint/taintcontroller_test.go
index a3de56937b..0ebd13c7e7 100644
--- a/cni/pkg/taint/taintcontroller_test.go
+++ b/cni/pkg/taint/taintcontroller_test.go
@@ -34,7 +34,8 @@
 // simplified taintsetter controller build upon fake sourcer, return controller and namespace, label based sourcer created by configmap
 func newMockTaintSetterController(ts *Setter,
 	nodeSource *fcache.FakeControllerSource) (c *Controller,
-	sourcer map[string]map[string]*fcache.FakeControllerSource) {
+	sourcer map[string]map[string]*fcache.FakeControllerSource,
+) {
 	c = &Controller{
 		clientset:       ts.Client,
 		podWorkQueue:    workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),
diff --git a/cni/test/install_cni.go b/cni/test/install_cni.go
index 601f55d912..5e939d4e24 100644
--- a/cni/test/install_cni.go
+++ b/cni/test/install_cni.go
@@ -171,7 +171,8 @@ func populateTempDirs(wd string, cniDirOrderedFiles []string, tempCNIConfDir, te
 
 // startDocker starts a test Docker container and runs the install-cni script.
 func runInstall(ctx context.Context, tempCNIConfDir, tempCNIBinDir,
-	tempK8sSvcAcctDir, cniConfFileName string, chainedCNIPlugin bool) {
+	tempK8sSvcAcctDir, cniConfFileName string, chainedCNIPlugin bool,
+) {
 	root := cmd.GetCommand()
 	constants.ServiceAccountPath = tempK8sSvcAcctDir
 	constants.HostCNIBinDir = tempCNIBinDir
@@ -259,7 +260,8 @@ func checkTempFilesCleaned(tempCNIConfDir string, t *testing.T) {
 // doTest sets up necessary environment variables, runs the Docker installation
 // container and verifies output file correctness.
 func doTest(t *testing.T, chainedCNIPlugin bool, wd, preConfFile, resultFileName, delayedConfFile, expectedOutputFile,
-	expectedPostCleanFile, tempCNIConfDir, tempCNIBinDir, tempK8sSvcAcctDir string) {
+	expectedPostCleanFile, tempCNIConfDir, tempCNIBinDir, tempK8sSvcAcctDir string,
+) {
 	t.Logf("prior cni-conf='%v', expected result='%v'", preConfFile, resultFileName)
 
 	// Don't set the CNI conf file env var if preConfFile is not set
@@ -345,7 +347,8 @@ func doTest(t *testing.T, chainedCNIPlugin bool, wd, preConfFile, resultFileName
 // prefix. This func is only meant to be invoked programmatically. A separate
 // install_cni_test.go file exists for executing this test.
 func RunInstallCNITest(t *testing.T, chainedCNIPlugin bool, preConfFile, resultFileName, delayedConfFile, expectedOutputFile,
-	expectedPostCleanFile string, cniConfDirOrderedFiles []string) {
+	expectedPostCleanFile string, cniConfDirOrderedFiles []string,
+) {
 	wd := env.IstioSrc + "/cni/test"
 	testWorkRootDir := getEnv("TEST_WORK_ROOTDIR", "/tmp")
 
diff --git a/istioctl/cmd/add-to-mesh.go b/istioctl/cmd/add-to-mesh.go
index e361921abe..6a132ad954 100644
--- a/istioctl/cmd/add-to-mesh.go
+++ b/istioctl/cmd/add-to-mesh.go
@@ -241,7 +241,8 @@ func svcMeshifyCmd() *cobra.Command {
 }
 
 func injectSideCarIntoDeployments(client kubernetes.Interface, deps []appsv1.Deployment, sidecarTemplate inject.RawTemplates, valuesConfig,
-	name, namespace string, revision string, meshConfig *meshconfig.MeshConfig, writer io.Writer, warningHandler func(string)) error {
+	name, namespace string, revision string, meshConfig *meshconfig.MeshConfig, writer io.Writer, warningHandler func(string),
+) error {
 	var errs error
 	for _, dep := range deps {
 		err := injectSideCarIntoDeployment(client, &dep, sidecarTemplate, valuesConfig,
@@ -337,7 +338,8 @@ func setupParameters(sidecarTemplate *inject.RawTemplates, valuesConfig *string,
 }
 
 func injectSideCarIntoDeployment(client kubernetes.Interface, dep *appsv1.Deployment, sidecarTemplate inject.RawTemplates, valuesConfig,
-	svcName, svcNamespace string, revision string, meshConfig *meshconfig.MeshConfig, writer io.Writer, warningHandler func(string)) error {
+	svcName, svcNamespace string, revision string, meshConfig *meshconfig.MeshConfig, writer io.Writer, warningHandler func(string),
+) error {
 	var errs error
 	log.Debugf("updating deployment %s.%s with Istio sidecar injected",
 		dep.Name, dep.Namespace)
@@ -471,7 +473,8 @@ func str2NamedPort(str string) (namedPort, error) {
 
 // addServiceOnVMToMesh adds a service running on VM into Istio service mesh
 func addServiceOnVMToMesh(dynamicClient dynamic.Interface, client kubernetes.Interface, ns string,
-	args, l, a []string, svcAcctAnn string, writer io.Writer) error {
+	args, l, a []string, svcAcctAnn string, writer io.Writer,
+) error {
 	svcName := args[0]
 	ips := strings.Split(args[1], ",")
 	portsListStr := args[2:]
@@ -664,7 +667,8 @@ func createK8sService(client kubernetes.Interface, ns string, svc *corev1.Servic
 
 // createServiceEntry creates an Istio ServiceEntry object in order to register vm service.
 func createServiceEntry(dynamicClient dynamic.Interface, ns string,
-	u *unstructured.Unstructured, name string, writer io.Writer) error {
+	u *unstructured.Unstructured, name string, writer io.Writer,
+) error {
 	if u == nil {
 		return fmt.Errorf("failed to create vm service")
 	}
diff --git a/istioctl/cmd/dashboard.go b/istioctl/cmd/dashboard.go
index 3ae7be9364..5a1c45c802 100644
--- a/istioctl/cmd/dashboard.go
+++ b/istioctl/cmd/dashboard.go
@@ -405,7 +405,8 @@ func skywalkingDashCmd() *cobra.Command {
 
 // portForward first tries to forward localhost:remotePort to podName:remotePort, falls back to dynamic local port
 func portForward(podName, namespace, flavor, urlFormat, localAddress string, remotePort int,
-	client kube.ExtendedClient, writer io.Writer, browser bool) error {
+	client kube.ExtendedClient, writer io.Writer, browser bool,
+) error {
 	// port preference:
 	// - If --listenPort is specified, use it
 	// - without --listenPort, prefer the remotePort but fall back to a random port
diff --git a/istioctl/cmd/injector-list.go b/istioctl/cmd/injector-list.go
index f76fa633d3..19a2e0fbf7 100644
--- a/istioctl/cmd/injector-list.go
+++ b/istioctl/cmd/injector-list.go
@@ -128,7 +128,8 @@ func getNamespaces(ctx context.Context, client kube.ExtendedClient) ([]v1.Namesp
 }
 
 func printNS(writer io.Writer, namespaces []v1.Namespace, hooks []admit_v1.MutatingWebhookConfiguration,
-	allPods map[resource.Namespace][]v1.Pod) error {
+	allPods map[resource.Namespace][]v1.Pod,
+) error {
 	outputCount := 0
 
 	w := new(tabwriter.Writer).Init(writer, 0, 8, 1, ' ', 0)
diff --git a/istioctl/cmd/internal-debug.go b/istioctl/cmd/internal-debug.go
index 58cc6f9680..ff4b22cf44 100644
--- a/istioctl/cmd/internal-debug.go
+++ b/istioctl/cmd/internal-debug.go
@@ -32,7 +32,8 @@
 
 func HandlerForRetrieveDebugList(kubeClient kube.ExtendedClient,
 	centralOpts clioptions.CentralControlPlaneOptions,
-	writer io.Writer) (map[string]*xdsapi.DiscoveryResponse, error) {
+	writer io.Writer,
+) (map[string]*xdsapi.DiscoveryResponse, error) {
 	var namespace, serviceAccount string
 	xdsRequest := xdsapi.DiscoveryRequest{
 		ResourceNames: []string{"list"},
@@ -53,7 +54,8 @@ func HandlerForRetrieveDebugList(kubeClient kube.ExtendedClient,
 func HandlerForDebugErrors(kubeClient kube.ExtendedClient,
 	centralOpts *clioptions.CentralControlPlaneOptions,
 	writer io.Writer,
-	xdsResponses map[string]*xdsapi.DiscoveryResponse) (map[string]*xdsapi.DiscoveryResponse, error) {
+	xdsResponses map[string]*xdsapi.DiscoveryResponse,
+) (map[string]*xdsapi.DiscoveryResponse, error) {
 	for _, response := range xdsResponses {
 		for _, resource := range response.Resources {
 			eString := string(resource.Value)
diff --git a/istioctl/cmd/kubeinject.go b/istioctl/cmd/kubeinject.go
index f920d6f5ef..4332efc340 100644
--- a/istioctl/cmd/kubeinject.go
+++ b/istioctl/cmd/kubeinject.go
@@ -375,7 +375,8 @@ func validateFlags() error {
 }
 
 func setupKubeInjectParameters(sidecarTemplate *inject.RawTemplates, valuesConfig *string,
-	revision, injectorAddress string) (*ExternalInjector, *meshconfig.MeshConfig, error) {
+	revision, injectorAddress string,
+) (*ExternalInjector, *meshconfig.MeshConfig, error) {
 	var err error
 	injector := &ExternalInjector{}
 	if injectConfigFile != "" {
diff --git a/istioctl/cmd/metrics_test.go b/istioctl/cmd/metrics_test.go
index e52b79dfd1..37496ccdc8 100644
--- a/istioctl/cmd/metrics_test.go
+++ b/istioctl/cmd/metrics_test.go
@@ -193,7 +193,8 @@ func (client mockPromAPI) WalReplay(ctx context.Context) (promv1.WalReplayStatus
 }
 
 func (client mockPromAPI) Series(ctx context.Context, matches []string,
-	startTime time.Time, endTime time.Time) ([]prometheus_model.LabelSet, promv1.Warnings, error) {
+	startTime time.Time, endTime time.Time,
+) ([]prometheus_model.LabelSet, promv1.Warnings, error) {
 	return nil, nil, nil
 }
 
diff --git a/istioctl/cmd/remove-from-mesh.go b/istioctl/cmd/remove-from-mesh.go
index d8101256f1..bac5a2faec 100644
--- a/istioctl/cmd/remove-from-mesh.go
+++ b/istioctl/cmd/remove-from-mesh.go
@@ -212,7 +212,8 @@ func externalSvcUnMeshifyCmd() *cobra.Command {
 }
 
 func unInjectSideCarFromDeployment(client kubernetes.Interface, deps []appsv1.Deployment,
-	svcName, svcNamespace string, writer io.Writer) error {
+	svcName, svcNamespace string, writer io.Writer,
+) error {
 	var errs error
 	name := strings.Join([]string{svcName, svcNamespace}, ".")
 	for _, dep := range deps {
@@ -287,7 +288,8 @@ func unInjectSideCarFromDeployment(client kubernetes.Interface, deps []appsv1.De
 
 // removeServiceOnVMFromMesh removes the Service Entry and K8s service for the specified external service
 func removeServiceOnVMFromMesh(dynamicClient dynamic.Interface, client kubernetes.Interface, ns string,
-	svcName string, writer io.Writer) error {
+	svcName string, writer io.Writer,
+) error {
 	// Pre-check Kubernetes service and service entry does not exist.
 	_, err := client.CoreV1().Services(ns).Get(context.TODO(), svcName, metav1.GetOptions{})
 	if err != nil {
diff --git a/istioctl/cmd/revision.go b/istioctl/cmd/revision.go
index fae1225434..c327ac8952 100644
--- a/istioctl/cmd/revision.go
+++ b/istioctl/cmd/revision.go
@@ -690,7 +690,8 @@ func annotateWithIOPCustomization(revDesc *RevisionDescription, manifestsPath st
 }
 
 func getBasicRevisionDescription(iopCRs []*iopv1alpha1.IstioOperator,
-	mutatingWebhooks []admit_v1.MutatingWebhookConfiguration) *RevisionDescription {
+	mutatingWebhooks []admit_v1.MutatingWebhookConfiguration,
+) *RevisionDescription {
 	revDescription := &RevisionDescription{
 		IstioOperatorCRs: []*IstioOperatorCRInfo{},
 		Webhooks:         []*MutatingWebhookConfigInfo{},
diff --git a/istioctl/cmd/wait.go b/istioctl/cmd/wait.go
index fb37462f59..a679a52a2c 100644
--- a/istioctl/cmd/wait.go
+++ b/istioctl/cmd/wait.go
@@ -179,7 +179,8 @@ func countVersions(versionCount map[string]int, configVersion string) {
 func poll(cmd *cobra.Command,
 	acceptedVersions []string,
 	targetResource string,
-	opts clioptions.ControlPlaneOptions) (present, notpresent, sdcnum int, err error) {
+	opts clioptions.ControlPlaneOptions,
+) (present, notpresent, sdcnum int, err error) {
 	kubeClient, err := kubeClientWithRevision(kubeconfig, configContext, opts.Revision)
 	if err != nil {
 		return 0, 0, 0, err
diff --git a/istioctl/cmd/workload.go b/istioctl/cmd/workload.go
index 3425654f40..64ec3245b0 100644
--- a/istioctl/cmd/workload.go
+++ b/istioctl/cmd/workload.go
@@ -291,7 +291,8 @@ func readWorkloadGroup(filename string, wg *clientv1alpha3.WorkloadGroup) error
 
 // Creates all the relevant config for the given workload group and cluster
 func createConfig(kubeClient kube.ExtendedClient, wg *clientv1alpha3.WorkloadGroup, clusterID, ingressIP, internalIP,
-	externalIP string, outputDir string, out io.Writer) error {
+	externalIP string, outputDir string, out io.Writer,
+) error {
 	if err := os.MkdirAll(outputDir, filePerms); err != nil {
 		return err
 	}
diff --git a/istioctl/pkg/multixds/gather.go b/istioctl/pkg/multixds/gather.go
index 5e3b9b1727..6a55eef872 100644
--- a/istioctl/pkg/multixds/gather.go
+++ b/istioctl/pkg/multixds/gather.go
@@ -138,7 +138,8 @@ func makeSan(istioNamespace, revision string) string {
 // AllRequestAndProcessXds returns all XDS responses from 1 central or 1..N K8s cluster-based XDS servers
 // nolint: lll
 func AllRequestAndProcessXds(dr *xdsapi.DiscoveryRequest, centralOpts clioptions.CentralControlPlaneOptions, istioNamespace string,
-	ns string, serviceAccount string, kubeClient kube.ExtendedClient) (map[string]*xdsapi.DiscoveryResponse, error) {
+	ns string, serviceAccount string, kubeClient kube.ExtendedClient,
+) (map[string]*xdsapi.DiscoveryResponse, error) {
 	return MultiRequestAndProcessXds(true, dr, centralOpts, istioNamespace, ns, serviceAccount, kubeClient)
 }
 
@@ -146,7 +147,8 @@ func AllRequestAndProcessXds(dr *xdsapi.DiscoveryRequest, centralOpts clioptions
 // stopping after the first response that returns any resources.
 // nolint: lll
 func FirstRequestAndProcessXds(dr *xdsapi.DiscoveryRequest, centralOpts clioptions.CentralControlPlaneOptions, istioNamespace string,
-	ns string, serviceAccount string, kubeClient kube.ExtendedClient) (map[string]*xdsapi.DiscoveryResponse, error) {
+	ns string, serviceAccount string, kubeClient kube.ExtendedClient,
+) (map[string]*xdsapi.DiscoveryResponse, error) {
 	return MultiRequestAndProcessXds(false, dr, centralOpts, istioNamespace, ns, serviceAccount, kubeClient)
 }
 
@@ -170,7 +172,8 @@ func getXdsAddressFromWebhooks(client kube.ExtendedClient) (string, error) {
 
 // nolint: lll
 func MultiRequestAndProcessXds(all bool, dr *xdsapi.DiscoveryRequest, centralOpts clioptions.CentralControlPlaneOptions, istioNamespace string,
-	ns string, serviceAccount string, kubeClient kube.ExtendedClient) (map[string]*xdsapi.DiscoveryResponse, error) {
+	ns string, serviceAccount string, kubeClient kube.ExtendedClient,
+) (map[string]*xdsapi.DiscoveryResponse, error) {
 	// If Central Istiod case, just call it
 	if ns == "" {
 		ns = istioNamespace
diff --git a/istioctl/pkg/verifier/verifier.go b/istioctl/pkg/verifier/verifier.go
index 69c1835793..71b93e38ad 100644
--- a/istioctl/pkg/verifier/verifier.go
+++ b/istioctl/pkg/verifier/verifier.go
@@ -83,7 +83,8 @@ func WithIOP(iop *v1alpha1.IstioOperator) StatusVerifierOptions {
 // which checks the status of various resources from the manifest.
 func NewStatusVerifier(istioNamespace, manifestsPath, kubeconfig, context string,
 	filenames []string, controlPlaneOpts clioptions.ControlPlaneOptions,
-	options ...StatusVerifierOptions) (*StatusVerifier, error) {
+	options ...StatusVerifierOptions,
+) (*StatusVerifier, error) {
 	client, err := kube.NewExtendedClient(kube.BuildClientCmd(kubeconfig, context), "")
 	if err != nil {
 		return nil, fmt.Errorf("failed to connect Kubernetes API server, error: %v", err)
@@ -223,7 +224,7 @@ func (v *StatusVerifier) verifyPostInstallIstioOperator(iop *v1alpha1.IstioOpera
 	}
 
 	manifests, errs := cp.RenderManifest()
-	if errs != nil && len(errs) > 0 {
+	if len(errs) > 0 {
 		return 0, 0, errs.ToError()
 	}
 
diff --git a/istioctl/pkg/writer/compare/sds/util.go b/istioctl/pkg/writer/compare/sds/util.go
index 75df95ca4c..71a511ad0d 100644
--- a/istioctl/pkg/writer/compare/sds/util.go
+++ b/istioctl/pkg/writer/compare/sds/util.go
@@ -141,7 +141,8 @@ func (s *secretItemBuilder) Build() (SecretItem, error) {
 
 // GetEnvoySecrets parses the secrets section of the config dump into []SecretItem
 func GetEnvoySecrets(
-	wrapper *configdump.Wrapper) ([]SecretItem, error) {
+	wrapper *configdump.Wrapper,
+) ([]SecretItem, error) {
 	secretConfigDump, err := wrapper.GetSecretConfigDump()
 	if err != nil {
 		return nil, err
diff --git a/istioctl/pkg/xds/client.go b/istioctl/pkg/xds/client.go
index e3e6736900..036062e0e9 100644
--- a/istioctl/pkg/xds/client.go
+++ b/istioctl/pkg/xds/client.go
@@ -41,7 +41,8 @@
 
 // GetXdsResponse opens a gRPC connection to opts.xds and waits for a single response
 func GetXdsResponse(dr *xdsapi.DiscoveryRequest, ns string, serviceAccount string, opts clioptions.CentralControlPlaneOptions,
-	grpcOpts []grpc.DialOption) (*xdsapi.DiscoveryResponse, error) {
+	grpcOpts []grpc.DialOption,
+) (*xdsapi.DiscoveryResponse, error) {
 	adscConn, err := adsc.NewWithBackoffPolicy(opts.Xds, &adsc.Config{
 		Meta: model.NodeMetadata{
 			Generator:      "event",
@@ -71,7 +72,8 @@ func GetXdsResponse(dr *xdsapi.DiscoveryRequest, ns string, serviceAccount strin
 
 // DialOptions constructs gRPC dial options from command line configuration
 func DialOptions(opts clioptions.CentralControlPlaneOptions,
-	ns string, serviceAccount string, kubeClient kube.ExtendedClient) ([]grpc.DialOption, error) {
+	ns string, serviceAccount string, kubeClient kube.ExtendedClient,
+) ([]grpc.DialOption, error) {
 	// If we are using the insecure 15010 don't bother getting a token
 	if opts.Plaintext || opts.CertDir != "" {
 		return make([]grpc.DialOption, 0), nil
diff --git a/operator/cmd/mesh/install.go b/operator/cmd/mesh/install.go
index f10f25bdbd..51cfbcdaca 100644
--- a/operator/cmd/mesh/install.go
+++ b/operator/cmd/mesh/install.go
@@ -251,11 +251,14 @@ func Install(rootArgs *RootArgs, iArgs *InstallArgs, logOpts *log.Options, stdOu
 
 // InstallManifests generates manifests from the given istiooperator instance and applies them to the
 // cluster. See GenManifests for more description of the manifest generation process.
-//  force   validation warnings are written to logger but command is not aborted
-//  DryRun  all operations are done but nothing is written
+//
+//	force   validation warnings are written to logger but command is not aborted
+//	DryRun  all operations are done but nothing is written
+//
 // Returns final IstioOperator after installation if successful.
 func InstallManifests(iop *v1alpha12.IstioOperator, force bool, dryRun bool, kubeClient kube.Client, client client.Client,
-	waitTimeout time.Duration, l clog.Logger) (*v1alpha12.IstioOperator, error) {
+	waitTimeout time.Duration, l clog.Logger,
+) (*v1alpha12.IstioOperator, error) {
 	// Needed in case we are running a test through this path that doesn't start a new process.
 	cache.FlushObjectCaches()
 	opts := &helmreconciler.Options{
diff --git a/operator/cmd/mesh/manifest-diff.go b/operator/cmd/mesh/manifest-diff.go
index 8323e4215d..43b58be830 100644
--- a/operator/cmd/mesh/manifest-diff.go
+++ b/operator/cmd/mesh/manifest-diff.go
@@ -111,7 +111,8 @@ func manifestDiffCmd(rootArgs *RootArgs, diffArgs *manifestDiffArgs) *cobra.Comm
 
 // compareManifestsFromFiles compares two manifest files
 func compareManifestsFromFiles(rootArgs *RootArgs, args []string, verbose bool,
-	renameResources, selectResources, ignoreResources string) (bool, error) {
+	renameResources, selectResources, ignoreResources string,
+) (bool, error) {
 	initLogsOrExit(rootArgs)
 
 	a, err := os.ReadFile(args[0])
@@ -143,7 +144,8 @@ func yamlFileFilter(path string) bool {
 
 // compareManifestsFromDirs compares manifests from two directories
 func compareManifestsFromDirs(rootArgs *RootArgs, verbose bool, dirName1, dirName2,
-	renameResources, selectResources, ignoreResources string) (bool, error) {
+	renameResources, selectResources, ignoreResources string,
+) (bool, error) {
 	initLogsOrExit(rootArgs)
 
 	mf1, err := util.ReadFilesWithFilter(dirName1, yamlFileFilter)
diff --git a/operator/cmd/mesh/shared.go b/operator/cmd/mesh/shared.go
index e74c9b562e..c2f5235fb1 100644
--- a/operator/cmd/mesh/shared.go
+++ b/operator/cmd/mesh/shared.go
@@ -153,7 +153,8 @@ type applyOptions struct {
 }
 
 func applyManifest(kubeClient kube.Client, client client.Client, manifestStr string,
-	componentName name.ComponentName, opts *applyOptions, iop *v1alpha1.IstioOperator, l clog.Logger) error {
+	componentName name.ComponentName, opts *applyOptions, iop *v1alpha1.IstioOperator, l clog.Logger,
+) error {
 	// Needed in case we are running a test through this path that doesn't start a new process.
 	cache.FlushObjectCaches()
 	reconciler, err := helmreconciler.NewHelmReconciler(client, kubeClient, iop, &helmreconciler.Options{DryRun: opts.DryRun, Log: l})
diff --git a/operator/cmd/mesh/uninstall.go b/operator/cmd/mesh/uninstall.go
index b841681a5d..56a8614ef1 100644
--- a/operator/cmd/mesh/uninstall.go
+++ b/operator/cmd/mesh/uninstall.go
@@ -190,7 +190,8 @@ func uninstall(cmd *cobra.Command, rootArgs *RootArgs, uiArgs *uninstallArgs, lo
 // 1. checks proxies still pointing to the target control plane revision.
 // 2. lists to be pruned resources if user uninstall by --revision flag.
 func preCheckWarnings(cmd *cobra.Command, uiArgs *uninstallArgs,
-	rev string, resourcesList []*unstructured.UnstructuredList, objectsList object.K8sObjects, l *clog.ConsoleLogger) {
+	rev string, resourcesList []*unstructured.UnstructuredList, objectsList object.K8sObjects, l *clog.ConsoleLogger,
+) {
 	pids, err := proxyinfo.GetIDsFromProxyInfo(uiArgs.kubeConfigPath, uiArgs.context, rev, uiArgs.istioNamespace)
 	if err != nil {
 		l.LogAndError(err.Error())
diff --git a/operator/pkg/helmreconciler/prune.go b/operator/pkg/helmreconciler/prune.go
index dbdcc94fc6..f75734db3a 100644
--- a/operator/pkg/helmreconciler/prune.go
+++ b/operator/pkg/helmreconciler/prune.go
@@ -210,7 +210,8 @@ func (h *HelmReconciler) DeleteObjectsList(objectsList []*unstructured.Unstructu
 // If componentName is not empty, only resources associated with specific components would be returned
 // UnstructuredList of objects and corresponding list of name kind hash of k8sObjects would be returned
 func (h *HelmReconciler) GetPrunedResources(revision string, includeClusterResources bool, componentName string) (
-	[]*unstructured.UnstructuredList, error) {
+	[]*unstructured.UnstructuredList, error,
+) {
 	var usList []*unstructured.UnstructuredList
 	labels := make(map[string]string)
 	if revision != "" {
@@ -304,7 +305,8 @@ func (h *HelmReconciler) getIstioOperatorCR() *unstructured.UnstructuredList {
 // DeleteControlPlaneByManifests removed resources by manifests with matching revision label.
 // If purge option is set to true, all manifests would be removed regardless of labels match.
 func (h *HelmReconciler) DeleteControlPlaneByManifests(manifestMap name.ManifestMap,
-	revision string, includeClusterResources bool) error {
+	revision string, includeClusterResources bool,
+) error {
 	labels := map[string]string{
 		operatorLabelStr: operatorReconcileStr,
 	}
@@ -391,7 +393,8 @@ func (h *HelmReconciler) runForAllTypes(callback func(labels map[string]string,
 // deleteResources delete any resources from the given component that are not in the excluded map. Resource
 // labels are used to identify the resources belonging to the component.
 func (h *HelmReconciler) deleteResources(excluded map[string]bool, coreLabels map[string]string,
-	componentName string, objects *unstructured.UnstructuredList, all bool) error {
+	componentName string, objects *unstructured.UnstructuredList, all bool,
+) error {
 	var errs util.Errors
 	labels := h.addComponentLabels(coreLabels, componentName)
 	selector := klabels.Set(labels).AsSelectorPreValidated()
diff --git a/operator/pkg/helmreconciler/wait.go b/operator/pkg/helmreconciler/wait.go
index 6cb1b251d1..789bad1d14 100644
--- a/operator/pkg/helmreconciler/wait.go
+++ b/operator/pkg/helmreconciler/wait.go
@@ -54,7 +54,8 @@ type deployment struct {
 // WaitForResources polls to get the current status of all pods, PVCs, and Services
 // until all are ready or a timeout is reached
 func WaitForResources(objects object.K8sObjects, client kube.Client,
-	waitTimeout time.Duration, dryRun bool, l *progress.ManifestLog) error {
+	waitTimeout time.Duration, dryRun bool, l *progress.ManifestLog,
+) error {
 	if dryRun || TestMode {
 		return nil
 	}
diff --git a/operator/pkg/manifest/shared.go b/operator/pkg/manifest/shared.go
index 48728a991e..cf3e0ac34c 100644
--- a/operator/pkg/manifest/shared.go
+++ b/operator/pkg/manifest/shared.go
@@ -50,7 +50,8 @@
 // If force is set, validation errors will not cause processing to abort but will result in warnings going to the
 // supplied logger.
 func GenManifests(inFilename []string, setFlags []string, force bool, filter []string,
-	client kube.Client, l clog.Logger) (name.ManifestMap, *iopv1alpha1.IstioOperator, error) {
+	client kube.Client, l clog.Logger,
+) (name.ManifestMap, *iopv1alpha1.IstioOperator, error) {
 	mergedYAML, _, err := GenerateConfig(inFilename, setFlags, force, client, l)
 	if err != nil {
 		return nil, nil, err
@@ -89,7 +90,8 @@ func GenManifests(inFilename []string, setFlags []string, force bool, filter []s
 // In step 3, the remaining fields in the same user overlay are applied on the resulting profile base.
 // The force flag causes validation errors not to abort but only emit log/console warnings.
 func GenerateConfig(inFilenames []string, setFlags []string, force bool, client kube.Client,
-	l clog.Logger) (string, *iopv1alpha1.IstioOperator, error) {
+	l clog.Logger,
+) (string, *iopv1alpha1.IstioOperator, error) {
 	if err := validateSetFlags(setFlags); err != nil {
 		return "", nil, err
 	}
@@ -103,7 +105,8 @@ func GenerateConfig(inFilenames []string, setFlags []string, force bool, client
 }
 
 func OverlayYAMLStrings(profile string, fy string,
-	setFlags []string, force bool, client kube.Client, l clog.Logger) (string, *iopv1alpha1.IstioOperator, error) {
+	setFlags []string, force bool, client kube.Client, l clog.Logger,
+) (string, *iopv1alpha1.IstioOperator, error) {
 	iopsString, iops, err := GenIOPFromProfile(profile, fy, setFlags, force, false, client, l)
 	if err != nil {
 		return "", nil, err
@@ -123,7 +126,8 @@ func OverlayYAMLStrings(profile string, fy string,
 // GenIOPFromProfile generates an IstioOperator from the given profile name or path, and overlay YAMLs from user
 // files and the --set flag. If successful, it returns an IstioOperator string and struct.
 func GenIOPFromProfile(profileOrPath, fileOverlayYAML string, setFlags []string, skipValidation, allowUnknownField bool,
-	client kube.Client, l clog.Logger) (string, *iopv1alpha1.IstioOperator, error) {
+	client kube.Client, l clog.Logger,
+) (string, *iopv1alpha1.IstioOperator, error) {
 	installPackagePath, err := getInstallPackagePath(fileOverlayYAML)
 	if err != nil {
 		return "", nil, err
@@ -325,7 +329,8 @@ func GetProfile(iop *iopv1alpha1.IstioOperator) string {
 }
 
 func GetMergedIOP(userIOPStr, profile, manifestsPath, revision string, client kube.Client,
-	logger clog.Logger) (*iopv1alpha1.IstioOperator, error) {
+	logger clog.Logger,
+) (*iopv1alpha1.IstioOperator, error) {
 	extraFlags := make([]string, 0)
 	if manifestsPath != "" {
 		extraFlags = append(extraFlags, fmt.Sprintf("installPackagePath=%s", manifestsPath))
diff --git a/operator/pkg/translate/translate.go b/operator/pkg/translate/translate.go
index 82aed81adf..cfdcc13caf 100644
--- a/operator/pkg/translate/translate.go
+++ b/operator/pkg/translate/translate.go
@@ -379,7 +379,8 @@ func skipReplicaCountWithAutoscaleEnabled(iop *v1alpha1.IstioOperatorSpec, compo
 }
 
 func (t *Translator) fixMergedObjectWithCustomServicePortOverlay(oo *object.K8sObject,
-	msvc *v1alpha1.ServiceSpec, mergedObj *object.K8sObject) (*object.K8sObject, error) {
+	msvc *v1alpha1.ServiceSpec, mergedObj *object.K8sObject,
+) (*object.K8sObject, error) {
 	var basePorts []*v1.ServicePort
 	bps, _, err := unstructured.NestedSlice(oo.Unstructured(), "spec", "ports")
 	if err != nil {
@@ -870,7 +871,8 @@ type Temp struct {
 // renderResourceComponentPathTemplate renders a template of the form <path>{{.ResourceName}}<path>{{.ContainerName}}<path> with
 // the supplied parameters.
 func (t *Translator) renderResourceComponentPathTemplate(tmpl string, componentName name.ComponentName,
-	resourceName, revision string) (string, error) {
+	resourceName, revision string,
+) (string, error) {
 	cn := string(componentName)
 	cmp := t.ComponentMap(cn)
 	if cmp == nil {
@@ -962,19 +964,21 @@ func MergeK8sObject(base *object.K8sObject, overlayNode interface{}, path util.P
 
 // createPatchObjectFromPath constructs patch object for node with path, returns nil object and error if the path is invalid.
 // eg. node:
-//     - name: NEW_VAR
-//       value: new_value
+//   - name: NEW_VAR
+//     value: new_value
+//
 // and path:
-//       spec.template.spec.containers.[name:discovery].env
-//     will constructs the following patch object:
-//       spec:
-//         template:
-//           spec:
-//             containers:
-//             - name: discovery
-//               env:
-//               - name: NEW_VAR
-//                 value: new_value
+//
+//	  spec.template.spec.containers.[name:discovery].env
+//	will constructs the following patch object:
+//	  spec:
+//	    template:
+//	      spec:
+//	        containers:
+//	        - name: discovery
+//	          env:
+//	          - name: NEW_VAR
+//	            value: new_value
 func createPatchObjectFromPath(node interface{}, path util.Path) (map[string]interface{}, error) {
 	if len(path) == 0 {
 		return nil, fmt.Errorf("empty path %s", path)
diff --git a/operator/pkg/translate/translate_value.go b/operator/pkg/translate/translate_value.go
index 6498918cd2..d410a944ac 100644
--- a/operator/pkg/translate/translate_value.go
+++ b/operator/pkg/translate/translate_value.go
@@ -550,7 +550,8 @@ func translateEnv(outPath string, value interface{}, cpSpecTree map[string]inter
 
 // translateK8sTree is internal method for translating K8s configurations from value.yaml tree.
 func (t *ReverseTranslator) translateK8sTree(valueTree map[string]interface{},
-	cpSpecTree map[string]interface{}, mapping map[string]*Translation) error {
+	cpSpecTree map[string]interface{}, mapping map[string]*Translation,
+) error {
 	for inPath, v := range mapping {
 		scope.Debugf("Checking for k8s path %s in helm Value.yaml tree", inPath)
 		path := util.PathFromString(inPath)
@@ -620,7 +621,8 @@ func (t *ReverseTranslator) translateK8sTree(valueTree map[string]interface{},
 
 // translateRemainingPaths translates remaining paths that are not available in existing mappings.
 func (t *ReverseTranslator) translateRemainingPaths(valueTree map[string]interface{},
-	cpSpecTree map[string]interface{}, path util.Path) error {
+	cpSpecTree map[string]interface{}, path util.Path,
+) error {
 	for key, val := range valueTree {
 		newPath := append(path, key)
 		// value set to nil means no translation needed or being translated already.
@@ -652,7 +654,8 @@ func (t *ReverseTranslator) translateRemainingPaths(valueTree map[string]interfa
 
 // translateAPI is internal method for translating value.yaml tree based on API mapping.
 func (t *ReverseTranslator) translateAPI(valueTree map[string]interface{},
-	cpSpecTree map[string]interface{}) error {
+	cpSpecTree map[string]interface{},
+) error {
 	for inPath, v := range t.APIMapping {
 		scope.Debugf("Checking for path %s in helm Value.yaml tree", inPath)
 		m, found, err := tpath.Find(valueTree, util.ToYAMLPath(inPath))
diff --git a/pilot/cmd/pilot-agent/app/cmd.go b/pilot/cmd/pilot-agent/app/cmd.go
index af793b1f75..37e5fcd469 100644
--- a/pilot/cmd/pilot-agent/app/cmd.go
+++ b/pilot/cmd/pilot-agent/app/cmd.go
@@ -215,7 +215,8 @@ func addFlags(proxyCmd *cobra.Command) {
 }
 
 func initStatusServer(ctx context.Context, proxy *model.Proxy, proxyConfig *meshconfig.ProxyConfig,
-	envoyPrometheusPort int, agent *istio_agent.Agent) error {
+	envoyPrometheusPort int, agent *istio_agent.Agent,
+) error {
 	o := options.NewStatusServerOptions(proxy, proxyConfig, agent)
 	o.EnvoyPrometheusPort = envoyPrometheusPort
 	o.Context = ctx
diff --git a/pilot/cmd/pilot-agent/options/security.go b/pilot/cmd/pilot-agent/options/security.go
index 98eaade0b8..18740ea78b 100644
--- a/pilot/cmd/pilot-agent/options/security.go
+++ b/pilot/cmd/pilot-agent/options/security.go
@@ -76,7 +76,8 @@ func NewSecurityOptions(proxyConfig *meshconfig.ProxyConfig, stsPort int, tokenM
 }
 
 func SetupSecurityOptions(proxyConfig *meshconfig.ProxyConfig, secOpt *security.Options, jwtPolicy,
-	credFetcherTypeEnv, credIdentityProvider string) (*security.Options, error) {
+	credFetcherTypeEnv, credIdentityProvider string,
+) (*security.Options, error) {
 	var jwtPath string
 	switch jwtPolicy {
 	case jwt.PolicyThirdParty:
diff --git a/pilot/pkg/bootstrap/istio_ca.go b/pilot/pkg/bootstrap/istio_ca.go
index 742e64ba03..65cdc532ff 100644
--- a/pilot/pkg/bootstrap/istio_ca.go
+++ b/pilot/pkg/bootstrap/istio_ca.go
@@ -373,8 +373,9 @@ func (s *Server) initCACertsWatcher() {
 
 // createIstioCA initializes the Istio CA signing functionality.
 // - for 'plugged in', uses ./etc/cacert directory, mounted from 'cacerts' secret in k8s.
-//   Inside, the key/cert are 'ca-key.pem' and 'ca-cert.pem'. The root cert signing the intermediate is root-cert.pem,
-//   which may contain multiple roots. A 'cert-chain.pem' file has the full cert chain.
+//
+//	Inside, the key/cert are 'ca-key.pem' and 'ca-cert.pem'. The root cert signing the intermediate is root-cert.pem,
+//	which may contain multiple roots. A 'cert-chain.pem' file has the full cert chain.
 func (s *Server) createIstioCA(client corev1.CoreV1Interface, opts *caOptions) (*ca.IstioCA, error) {
 	var caOpts *ca.IstioCAOptions
 	var err error
@@ -453,10 +454,13 @@ func (s *Server) createIstioCA(client corev1.CoreV1Interface, opts *caOptions) (
 // ca cert can come from three sources, order matters:
 // 1. Define ca cert via kubernetes secret and mount the secret through `external-ca-cert` volume
 // 2. Use kubernetes ca cert `/var/run/secrets/kubernetes.io/serviceaccount/ca.crt` if signer is
-//    kubernetes built-in `kubernetes.io/legacy-unknown" signer
+//
+//	kubernetes built-in `kubernetes.io/legacy-unknown" signer
+//
 // 3. Extract from the cert-chain signed by other CSR signer.
 func (s *Server) createIstioRA(client kubelib.Client,
-	opts *caOptions) (ra.RegistrationAuthority, error) {
+	opts *caOptions,
+) (ra.RegistrationAuthority, error) {
 	caCertFile := path.Join(ra.DefaultExtCACertDir, constants.CACertNamespaceConfigMapDataName)
 	certSignerDomain := opts.CertSignerDomain
 	_, err := os.Stat(caCertFile)
diff --git a/pilot/pkg/config/kube/gateway/conversion.go b/pilot/pkg/config/kube/gateway/conversion.go
index 2367aa26b3..f887ac7e9c 100644
--- a/pilot/pkg/config/kube/gateway/conversion.go
+++ b/pilot/pkg/config/kube/gateway/conversion.go
@@ -192,7 +192,8 @@ func convertVirtualService(r *KubernetesResources, gatewayMap map[parentKey]map[
 }
 
 func buildHTTPVirtualServices(obj config.Config, gateways map[parentKey]map[k8s.SectionName]*parentInfo, domain string,
-	gatewayRoutes map[string]map[string]*config.Config, meshRoutes map[string]map[string]*config.Config) {
+	gatewayRoutes map[string]map[string]*config.Config, meshRoutes map[string]map[string]*config.Config,
+) {
 	route := obj.Spec.(*k8s.HTTPRouteSpec)
 	for _, r := range route.Rules {
 		if len(r.Matches) > 1 {
@@ -513,7 +514,8 @@ func referenceAllowed(p *parentInfo, routeKind config.GroupVersionKind, parentKi
 }
 
 func extractParentReferenceInfo(gateways map[parentKey]map[k8s.SectionName]*parentInfo, routeRefs []k8s.ParentReference,
-	hostnames []k8s.Hostname, kind config.GroupVersionKind, localNamespace string) []routeParentReference {
+	hostnames []k8s.Hostname, kind config.GroupVersionKind, localNamespace string,
+) []routeParentReference {
 	parentRefs := []routeParentReference{}
 	for _, ref := range routeRefs {
 		ir, err := toInternalParentReference(ref, localNamespace)
@@ -1320,7 +1322,8 @@ func convertGateways(r *KubernetesResources) ([]config.Config, map[parentKey]map
 // Multiple hostname/IP - It is feasible but preference is to create multiple Gateways. This would also break the 1:1 mapping of GW:Service
 // Mixed hostname and IP - doesn't make sense; user should define the IP in service
 // NamedAddress - Service has no concept of named address. For cloud's that have named addresses they can be configured by annotations,
-//   which users can add to the Gateway.
+//
+//	which users can add to the Gateway.
 func IsManaged(gw *k8s.GatewaySpec) bool {
 	if len(gw.Addresses) == 0 {
 		return true
diff --git a/pilot/pkg/config/kube/ingress/controller.go b/pilot/pkg/config/kube/ingress/controller.go
index 16ccc5f0ae..1cc0ec0d75 100644
--- a/pilot/pkg/config/kube/ingress/controller.go
+++ b/pilot/pkg/config/kube/ingress/controller.go
@@ -142,7 +142,8 @@ func NetworkingIngressAvailable(client kube.Client) bool {
 
 // NewController creates a new Kubernetes controller
 func NewController(client kube.Client, meshWatcher mesh.Holder,
-	options kubecontroller.Options) model.ConfigStoreController {
+	options kubecontroller.Options,
+) model.ConfigStoreController {
 	if ingressNamespace == "" {
 		ingressNamespace = constants.IstioIngressNamespace
 	}
diff --git a/pilot/pkg/config/kube/ingress/conversion.go b/pilot/pkg/config/kube/ingress/conversion.go
index bbd11fbd93..adaae6a199 100644
--- a/pilot/pkg/config/kube/ingress/conversion.go
+++ b/pilot/pkg/config/kube/ingress/conversion.go
@@ -252,7 +252,8 @@ func getMatchURILength(match *networking.HTTPMatchRequest) (length int, exact bo
 }
 
 func ingressBackendToHTTPRoute(backend *v1beta1.IngressBackend, namespace string, domainSuffix string,
-	serviceLister listerv1.ServiceLister) *networking.HTTPRoute {
+	serviceLister listerv1.ServiceLister,
+) *networking.HTTPRoute {
 	if backend == nil {
 		return nil
 	}
diff --git a/pilot/pkg/config/kube/ingressv1/controller.go b/pilot/pkg/config/kube/ingressv1/controller.go
index d916e83004..62afc24b21 100644
--- a/pilot/pkg/config/kube/ingressv1/controller.go
+++ b/pilot/pkg/config/kube/ingressv1/controller.go
@@ -102,7 +102,8 @@ type controller struct {
 
 // NewController creates a new Kubernetes controller
 func NewController(client kube.Client, meshWatcher mesh.Holder,
-	options kubecontroller.Options) model.ConfigStoreController {
+	options kubecontroller.Options,
+) model.ConfigStoreController {
 	if ingressNamespace == "" {
 		ingressNamespace = constants.IstioIngressNamespace
 	}
diff --git a/pilot/pkg/config/kube/ingressv1/conversion.go b/pilot/pkg/config/kube/ingressv1/conversion.go
index a79983aacd..031f83188d 100644
--- a/pilot/pkg/config/kube/ingressv1/conversion.go
+++ b/pilot/pkg/config/kube/ingressv1/conversion.go
@@ -125,7 +125,8 @@ func ConvertIngressV1alpha3(ingress knetworking.Ingress, mesh *meshconfig.MeshCo
 
 // ConvertIngressVirtualService converts from ingress spec to Istio VirtualServices
 func ConvertIngressVirtualService(ingress knetworking.Ingress, domainSuffix string,
-	ingressByHost map[string]*config.Config, serviceLister listerv1.ServiceLister) {
+	ingressByHost map[string]*config.Config, serviceLister listerv1.ServiceLister,
+) {
 	// Ingress allows a single host - if missing '*' is assumed
 	// We need to merge all rules with a particular host across
 	// all ingresses, and return a separate VirtualService for each
@@ -253,7 +254,8 @@ func getMatchURILength(match *networking.HTTPMatchRequest) (length int, exact bo
 }
 
 func ingressBackendToHTTPRoute(backend *knetworking.IngressBackend, namespace string, domainSuffix string,
-	serviceLister listerv1.ServiceLister) *networking.HTTPRoute {
+	serviceLister listerv1.ServiceLister,
+) *networking.HTTPRoute {
 	if backend == nil {
 		return nil
 	}
diff --git a/pilot/pkg/leaderelection/leaderelection_test.go b/pilot/pkg/leaderelection/leaderelection_test.go
index 2f9ead96c5..bf1c734772 100644
--- a/pilot/pkg/leaderelection/leaderelection_test.go
+++ b/pilot/pkg/leaderelection/leaderelection_test.go
@@ -37,7 +37,8 @@ func createElection(t *testing.T,
 	name string, revision string,
 	watcher revisions.DefaultWatcher,
 	prioritized, expectLeader bool,
-	client kubernetes.Interface, fns ...func(stop <-chan struct{})) (*LeaderElection, chan struct{}) {
+	client kubernetes.Interface, fns ...func(stop <-chan struct{}),
+) (*LeaderElection, chan struct{}) {
 	return createElectionMulticluster(t, name, revision, false, watcher, prioritized, expectLeader, client, fns...)
 }
 
@@ -46,7 +47,8 @@ func createElectionMulticluster(t *testing.T,
 	remote bool,
 	watcher revisions.DefaultWatcher,
 	prioritized, expectLeader bool,
-	client kubernetes.Interface, fns ...func(stop <-chan struct{})) (*LeaderElection, chan struct{}) {
+	client kubernetes.Interface, fns ...func(stop <-chan struct{}),
+) (*LeaderElection, chan struct{}) {
 	t.Helper()
 	l := &LeaderElection{
 		namespace:      "ns",
diff --git a/pilot/pkg/model/authentication.go b/pilot/pkg/model/authentication.go
index 69af95c6f7..d2dd78d699 100644
--- a/pilot/pkg/model/authentication.go
+++ b/pilot/pkg/model/authentication.go
@@ -205,13 +205,15 @@ func (policy *AuthenticationPolicies) GetNamespaceMutualTLSMode(namespace string
 
 // GetJwtPoliciesForWorkload returns a list of JWT policies matching to labels.
 func (policy *AuthenticationPolicies) GetJwtPoliciesForWorkload(namespace string,
-	workloadLabels labels.Instance) []*config.Config {
+	workloadLabels labels.Instance,
+) []*config.Config {
 	return getConfigsForWorkload(policy.requestAuthentications, policy.rootNamespace, namespace, workloadLabels)
 }
 
 // GetPeerAuthenticationsForWorkload returns a list of peer authentication policies matching to labels.
 func (policy *AuthenticationPolicies) GetPeerAuthenticationsForWorkload(namespace string,
-	workloadLabels labels.Instance) []*config.Config {
+	workloadLabels labels.Instance,
+) []*config.Config {
 	return getConfigsForWorkload(policy.peerAuthentications, policy.rootNamespace, namespace, workloadLabels)
 }
 
@@ -228,7 +230,8 @@ func (policy *AuthenticationPolicies) GetVersion() string {
 func getConfigsForWorkload(configsByNamespace map[string][]config.Config,
 	rootNamespace string,
 	namespace string,
-	workloadLabels labels.Instance) []*config.Config {
+	workloadLabels labels.Instance,
+) []*config.Config {
 	configs := make([]*config.Config, 0)
 	lookupInNamespaces := []string{namespace}
 	if namespace != rootNamespace {
diff --git a/pilot/pkg/model/authentication_test.go b/pilot/pkg/model/authentication_test.go
index e60a0a2807..d324fbdd88 100644
--- a/pilot/pkg/model/authentication_test.go
+++ b/pilot/pkg/model/authentication_test.go
@@ -723,7 +723,8 @@ func createTestRequestAuthenticationResource(name string, namespace string, sele
 }
 
 func createTestPeerAuthenticationResource(name string, namespace string, timestamp time.Time,
-	selector *selectorpb.WorkloadSelector, mode securityBeta.PeerAuthentication_MutualTLS_Mode) *config.Config {
+	selector *selectorpb.WorkloadSelector, mode securityBeta.PeerAuthentication_MutualTLS_Mode,
+) *config.Config {
 	return &config.Config{
 		Meta: config.Meta{
 			GroupVersionKind:  collections.IstioSecurityV1Beta1Peerauthentications.Resource().GroupVersionKind(),
diff --git a/pilot/pkg/model/extensions.go b/pilot/pkg/model/extensions.go
index 3a1651fd29..24f9be13b0 100644
--- a/pilot/pkg/model/extensions.go
+++ b/pilot/pkg/model/extensions.go
@@ -166,7 +166,8 @@ func buildDataSource(u *url.URL, wasmPlugin *extensions.WasmPlugin) *envoyCoreV3
 func buildVMConfig(
 	datasource *envoyCoreV3.AsyncDataSource,
 	resourceVersion string,
-	wasmPlugin *extensions.WasmPlugin) *envoyExtensionsWasmV3.PluginConfig_VmConfig {
+	wasmPlugin *extensions.WasmPlugin,
+) *envoyExtensionsWasmV3.PluginConfig_VmConfig {
 	cfg := &envoyExtensionsWasmV3.PluginConfig_VmConfig{
 		VmConfig: &envoyExtensionsWasmV3.VmConfig{
 			Runtime: defaultRuntime,
diff --git a/pilot/pkg/model/gateway_test.go b/pilot/pkg/model/gateway_test.go
index 5ab48854c1..e7ea4f997f 100644
--- a/pilot/pkg/model/gateway_test.go
+++ b/pilot/pkg/model/gateway_test.go
@@ -161,7 +161,8 @@ func TestMergeGateways(t *testing.T) {
 }
 
 func makeConfig(name, namespace, host, portName, portProtocol string, portNumber uint32, gw string, bind string,
-	mode networking.ServerTLSSettings_TLSmode) config.Config {
+	mode networking.ServerTLSSettings_TLSmode,
+) config.Config {
 	c := config.Config{
 		Meta: config.Meta{
 			Name:      name,
diff --git a/pilot/pkg/model/push_context.go b/pilot/pkg/model/push_context.go
index a38b40ebf7..dd60bd94bf 100644
--- a/pilot/pkg/model/push_context.go
+++ b/pilot/pkg/model/push_context.go
@@ -1188,7 +1188,8 @@ func (ps *PushContext) createNewContext(env *Environment) error {
 func (ps *PushContext) updateContext(
 	env *Environment,
 	oldPushContext *PushContext,
-	pushReq *PushRequest) error {
+	pushReq *PushRequest,
+) error {
 	var servicesChanged, virtualServicesChanged, destinationRulesChanged, gatewayChanged,
 		authnChanged, authzChanged, envoyFiltersChanged, sidecarsChanged, telemetryChanged, gatewayAPIChanged,
 		wasmPluginsChanged, proxyConfigsChanged bool
diff --git a/pilot/pkg/model/service.go b/pilot/pkg/model/service.go
index 67bd7b78a3..58ec2dd890 100644
--- a/pilot/pkg/model/service.go
+++ b/pilot/pkg/model/service.go
@@ -852,9 +852,7 @@ func (s *Service) DeepCopy() *Service {
 
 	if s.ServiceAccounts != nil {
 		out.ServiceAccounts = make([]string, len(s.ServiceAccounts))
-		for i, sa := range s.ServiceAccounts {
-			out.ServiceAccounts[i] = sa
-		}
+		copy(out.ServiceAccounts, s.ServiceAccounts)
 	}
 	out.ClusterVIPs = s.ClusterVIPs.DeepCopy()
 	return &out
diff --git a/pilot/pkg/model/sidecar.go b/pilot/pkg/model/sidecar.go
index 811e938e95..fe418a6be1 100644
--- a/pilot/pkg/model/sidecar.go
+++ b/pilot/pkg/model/sidecar.go
@@ -437,7 +437,8 @@ type serviceIndex struct {
 }
 
 func convertIstioListenerToWrapper(ps *PushContext, configNamespace string,
-	istioListener *networking.IstioEgressListener) *IstioEgressListenerWrapper {
+	istioListener *networking.IstioEgressListener,
+) *IstioEgressListenerWrapper {
 	out := &IstioEgressListenerWrapper{
 		IstioListener: istioListener,
 	}
diff --git a/pilot/pkg/model/virtualservice.go b/pilot/pkg/model/virtualservice.go
index c9a8557a3c..adb5a72431 100644
--- a/pilot/pkg/model/virtualservice.go
+++ b/pilot/pkg/model/virtualservice.go
@@ -151,7 +151,8 @@ func resolveVirtualServiceShortnames(rule *networking.VirtualService, meta confi
 // Return merged virtual services and the root->delegate vs map
 func mergeVirtualServicesIfNeeded(
 	vServices []config.Config,
-	defaultExportTo map[visibility.Instance]bool) ([]config.Config, map[ConfigKey][]ConfigKey) {
+	defaultExportTo map[visibility.Instance]bool,
+) ([]config.Config, map[ConfigKey][]ConfigKey) {
 	out := make([]config.Config, 0, len(vServices))
 	delegatesMap := map[string]config.Config{}
 	delegatesExportToMap := map[string]map[visibility.Instance]bool{}
diff --git a/pilot/pkg/networking/core/v1alpha3/accesslog.go b/pilot/pkg/networking/core/v1alpha3/accesslog.go
index 25b6100df0..ab350efdc4 100644
--- a/pilot/pkg/networking/core/v1alpha3/accesslog.go
+++ b/pilot/pkg/networking/core/v1alpha3/accesslog.go
@@ -223,7 +223,8 @@ func buildAccessLogFilterFromTelemetry(spec *model.LoggingConfig) *accesslog.Acc
 }
 
 func (b *AccessLogBuilder) setHTTPAccessLog(push *model.PushContext, proxy *model.Proxy,
-	connectionManager *hcm.HttpConnectionManager, class networking.ListenerClass) {
+	connectionManager *hcm.HttpConnectionManager, class networking.ListenerClass,
+) {
 	mesh := push.Mesh
 	cfg := push.Telemetry.AccessLogging(proxy, class)
 
@@ -245,7 +246,8 @@ func (b *AccessLogBuilder) setHTTPAccessLog(push *model.PushContext, proxy *mode
 }
 
 func (b *AccessLogBuilder) setListenerAccessLog(push *model.PushContext, proxy *model.Proxy,
-	listener *listener.Listener, class networking.ListenerClass) {
+	listener *listener.Listener, class networking.ListenerClass,
+) {
 	mesh := push.Mesh
 	if mesh.DisableEnvoyListenerLog {
 		return
@@ -360,7 +362,8 @@ func buildFileAccessTextLogFormat(text string) (*fileaccesslog.FileAccessLog_Log
 }
 
 func buildFileAccessJSONLogFormat(
-	logFormat *meshconfig.MeshConfig_ExtensionProvider_EnvoyFileAccessLogProvider_LogFormat_Labels) (*fileaccesslog.FileAccessLog_LogFormat, bool) {
+	logFormat *meshconfig.MeshConfig_ExtensionProvider_EnvoyFileAccessLogProvider_LogFormat_Labels,
+) (*fileaccesslog.FileAccessLog_LogFormat, bool) {
 	jsonLogStruct := EnvoyJSONLogFormatIstio
 	if logFormat.Labels != nil {
 		jsonLogStruct = logFormat.Labels
@@ -490,7 +493,8 @@ func buildFileAccessLogHelper(path string, mesh *meshconfig.MeshConfig) *accessl
 }
 
 func buildOpenTelemetryLogHelper(pushCtx *model.PushContext,
-	provider *meshconfig.MeshConfig_ExtensionProvider_EnvoyOpenTelemetryLogProvider) *accesslog.AccessLog {
+	provider *meshconfig.MeshConfig_ExtensionProvider_EnvoyOpenTelemetryLogProvider,
+) *accesslog.AccessLog {
 	_, cluster, err := clusterLookupFn(pushCtx, provider.Service, int(provider.Port))
 	if err != nil {
 		log.Errorf("could not find cluster for open telemetry provider %q: %v", provider, err)
diff --git a/pilot/pkg/networking/core/v1alpha3/cluster.go b/pilot/pkg/networking/core/v1alpha3/cluster.go
index 756990642b..4cce11545e 100644
--- a/pilot/pkg/networking/core/v1alpha3/cluster.go
+++ b/pilot/pkg/networking/core/v1alpha3/cluster.go
@@ -82,7 +82,8 @@ func (configgen *ConfigGeneratorImpl) BuildClusters(proxy *model.Proxy, req *mod
 // BuildDeltaClusters generates the deltas (add and delete) for a given proxy. Currently, only service changes are reflected with deltas.
 // Otherwise, we fall back onto generating everything.
 func (configgen *ConfigGeneratorImpl) BuildDeltaClusters(proxy *model.Proxy, updates *model.PushRequest,
-	watched *model.WatchedResource) ([]*discovery.Resource, []string, model.XdsLogDetails, bool) {
+	watched *model.WatchedResource,
+) ([]*discovery.Resource, []string, model.XdsLogDetails, bool) {
 	// if we can't use delta, fall back to generate all
 	if !shouldUseDelta(updates) {
 		cl, lg := configgen.BuildClusters(proxy, updates)
@@ -140,7 +141,8 @@ func (configgen *ConfigGeneratorImpl) BuildDeltaClusters(proxy *model.Proxy, upd
 
 // buildClusters builds clusters for the proxy with the services passed.
 func (configgen *ConfigGeneratorImpl) buildClusters(proxy *model.Proxy, req *model.PushRequest,
-	services []*model.Service) ([]*discovery.Resource, model.XdsLogDetails) {
+	services []*model.Service,
+) ([]*discovery.Resource, model.XdsLogDetails) {
 	clusters := make([]*cluster.Cluster, 0)
 	resources := model.Resources{}
 	envoyFilterPatches := req.Push.EnvoyFilters(proxy)
@@ -204,7 +206,8 @@ func deltaAwareConfigTypes(cfgs map[model.ConfigKey]struct{}) bool {
 
 // buildOutboundClusters generates all outbound (including subsets) clusters for a given proxy.
 func (configgen *ConfigGeneratorImpl) buildOutboundClusters(cb *ClusterBuilder, proxy *model.Proxy, cp clusterPatcher,
-	services []*model.Service) ([]*discovery.Resource, cacheStats) {
+	services []*model.Service,
+) ([]*discovery.Resource, cacheStats) {
 	resources := make([]*discovery.Resource, 0)
 	efKeys := cp.efw.Keys()
 	hit, miss := 0, 0
@@ -306,7 +309,8 @@ func (p clusterPatcher) hasPatches() bool {
 // All SniDnat clusters are internal services in the mesh.
 // TODO enable cache - there is no blockers here, skipped to simplify the original caching implementation
 func (configgen *ConfigGeneratorImpl) buildOutboundSniDnatClusters(proxy *model.Proxy, req *model.PushRequest,
-	cp clusterPatcher) []*cluster.Cluster {
+	cp clusterPatcher,
+) []*cluster.Cluster {
 	clusters := make([]*cluster.Cluster, 0)
 	cb := NewClusterBuilder(proxy, req, nil)
 
@@ -362,7 +366,8 @@ func buildInboundLocalityLbEndpoints(bind string, port uint32) []*endpoint.Local
 }
 
 func (configgen *ConfigGeneratorImpl) buildInboundClusters(cb *ClusterBuilder, proxy *model.Proxy, instances []*model.ServiceInstance,
-	cp clusterPatcher) []*cluster.Cluster {
+	cp clusterPatcher,
+) []*cluster.Cluster {
 	clusters := make([]*cluster.Cluster, 0)
 
 	// The inbound clusters for a node depends on whether the node has a SidecarScope with inbound listeners
@@ -465,7 +470,8 @@ func (configgen *ConfigGeneratorImpl) buildInboundClusters(cb *ClusterBuilder, p
 }
 
 func findOrCreateServiceInstance(instances []*model.ServiceInstance,
-	ingressListener *networking.IstioIngressListener, sidecar string, sidecarns string) *model.ServiceInstance {
+	ingressListener *networking.IstioIngressListener, sidecar string, sidecarns string,
+) *model.ServiceInstance {
 	for _, realInstance := range instances {
 		if realInstance.Endpoint.EndpointPort == ingressListener.Port.Number {
 			// We need to create a copy of the instance, as it is modified later while building clusters/listeners.
@@ -514,7 +520,8 @@ func convertResolution(proxyType model.NodeType, service *model.Service) cluster
 
 // SelectTrafficPolicyComponents returns the components of TrafficPolicy that should be used for given port.
 func selectTrafficPolicyComponents(policy *networking.TrafficPolicy) (
-	*networking.ConnectionPoolSettings, *networking.OutlierDetection, *networking.LoadBalancerSettings, *networking.ClientTLSSettings) {
+	*networking.ConnectionPoolSettings, *networking.OutlierDetection, *networking.LoadBalancerSettings, *networking.ClientTLSSettings,
+) {
 	if policy == nil {
 		return nil, nil, nil, nil
 	}
@@ -674,7 +681,8 @@ func defaultLBAlgorithm() cluster.Cluster_LbPolicy {
 }
 
 func applyLoadBalancer(c *cluster.Cluster, lb *networking.LoadBalancerSettings, port *model.Port,
-	locality *core.Locality, proxyLabels map[string]string, meshConfig *meshconfig.MeshConfig) {
+	locality *core.Locality, proxyLabels map[string]string, meshConfig *meshconfig.MeshConfig,
+) {
 	localityLbSetting := loadbalancer.GetLocalityLbSetting(meshConfig.GetLocalityLbSetting(), lb.GetLocalityLbSetting())
 	if localityLbSetting != nil {
 		if c.CommonLbConfig == nil {
@@ -779,7 +787,8 @@ func ApplyRingHashLoadBalancer(c *cluster.Cluster, lb *networking.LoadBalancerSe
 }
 
 func applyLocalityLBSetting(locality *core.Locality, proxyLabels map[string]string, cluster *cluster.Cluster,
-	localityLB *networking.LocalityLoadBalancerSetting) {
+	localityLB *networking.LocalityLoadBalancerSetting,
+) {
 	// Failover should only be applied with outlier detection, or traffic will never failover.
 	enabledFailover := cluster.OutlierDetection != nil
 	if cluster.LoadAssignment != nil {
diff --git a/pilot/pkg/networking/core/v1alpha3/cluster_builder.go b/pilot/pkg/networking/core/v1alpha3/cluster_builder.go
index 797f7c47cb..1a7392174a 100644
--- a/pilot/pkg/networking/core/v1alpha3/cluster_builder.go
+++ b/pilot/pkg/networking/core/v1alpha3/cluster_builder.go
@@ -166,7 +166,8 @@ func (cb *ClusterBuilder) sidecarProxy() bool {
 }
 
 func (cb *ClusterBuilder) buildSubsetCluster(opts buildClusterOpts, destRule *config.Config, subset *networking.Subset, service *model.Service,
-	proxyView model.ProxyView) *cluster.Cluster {
+	proxyView model.ProxyView,
+) *cluster.Cluster {
 	opts.serviceMTLSMode = cb.req.Push.BestEffortInferServiceMTLSMode(subset.GetTrafficPolicy(), service, opts.port)
 	var subsetClusterName string
 	var defaultSni string
@@ -236,7 +237,8 @@ func (cb *ClusterBuilder) buildSubsetCluster(opts buildClusterOpts, destRule *co
 // applyDestinationRule applies the destination rule if it exists for the Service. It returns the subset clusters if any created as it
 // applies the destination rule.
 func (cb *ClusterBuilder) applyDestinationRule(mc *MutableCluster, clusterMode ClusterMode, service *model.Service,
-	port *model.Port, proxyView model.ProxyView, destRule *config.Config, serviceAccounts []string) []*cluster.Cluster {
+	port *model.Port, proxyView model.ProxyView, destRule *config.Config, serviceAccounts []string,
+) []*cluster.Cluster {
 	destinationRule := CastDestinationRule(destRule)
 	// merge applicable port level traffic policy settings
 	trafficPolicy := MergeTrafficPolicy(nil, destinationRule.GetTrafficPolicy(), port)
@@ -343,7 +345,8 @@ func MergeTrafficPolicy(original, subsetPolicy *networking.TrafficPolicy, port *
 // buildDefaultCluster builds the default cluster and also applies default traffic policy.
 func (cb *ClusterBuilder) buildDefaultCluster(name string, discoveryType cluster.Cluster_DiscoveryType,
 	localityLbEndpoints []*endpoint.LocalityLbEndpoints, direction model.TrafficDirection,
-	port *model.Port, service *model.Service, allInstances []*model.ServiceInstance) *MutableCluster {
+	port *model.Port, service *model.Service, allInstances []*model.ServiceInstance,
+) *MutableCluster {
 	if allInstances == nil {
 		allInstances = cb.serviceInstances
 	}
@@ -411,7 +414,8 @@ func (cb *ClusterBuilder) buildDefaultCluster(name string, discoveryType cluster
 // Note: clusterPort and instance.Endpoint.EndpointPort are identical for standard Services; however,
 // Sidecar.Ingress allows these to be different.
 func (cb *ClusterBuilder) buildInboundClusterForPortOrUDS(clusterPort int, bind string,
-	proxy *model.Proxy, instance *model.ServiceInstance, allInstance []*model.ServiceInstance) *MutableCluster {
+	proxy *model.Proxy, instance *model.ServiceInstance, allInstance []*model.ServiceInstance,
+) *MutableCluster {
 	clusterName := model.BuildInboundSubsetKey(clusterPort)
 	localityLbEndpoints := buildInboundLocalityLbEndpoints(bind, instance.Endpoint.EndpointPort)
 	clusterType := cluster.Cluster_ORIGINAL_DST
@@ -475,7 +479,8 @@ func (cb *ClusterBuilder) buildInboundClusterForPortOrUDS(clusterPort int, bind
 }
 
 func (cb *ClusterBuilder) buildLocalityLbEndpoints(proxyView model.ProxyView, service *model.Service,
-	port int, labels labels.Instance) []*endpoint.LocalityLbEndpoints {
+	port int, labels labels.Instance,
+) []*endpoint.LocalityLbEndpoints {
 	if !(service.Resolution == model.DNSLB || service.Resolution == model.DNSRoundRobinLB) {
 		return nil
 	}
@@ -653,7 +658,8 @@ func (cb *ClusterBuilder) applyH2Upgrade(opts buildClusterOpts, connectionPool *
 
 // shouldH2Upgrade function returns true if the cluster  should be upgraded to http2.
 func (cb *ClusterBuilder) shouldH2Upgrade(clusterName string, direction model.TrafficDirection, port *model.Port, mesh *meshconfig.MeshConfig,
-	connectionPool *networking.ConnectionPoolSettings) bool {
+	connectionPool *networking.ConnectionPoolSettings,
+) bool {
 	if direction != model.TrafficDirectionOutbound {
 		return false
 	}
@@ -743,7 +749,8 @@ func (cb *ClusterBuilder) buildAutoMtlsSettings(
 	sni string,
 	autoMTLSEnabled bool,
 	meshExternal bool,
-	serviceMTLSMode model.MutualTLSMode) (*networking.ClientTLSSettings, mtlsContextType) {
+	serviceMTLSMode model.MutualTLSMode,
+) (*networking.ClientTLSSettings, mtlsContextType) {
 	if tls != nil {
 		if tls.Mode == networking.ClientTLSSettings_DISABLE || tls.Mode == networking.ClientTLSSettings_SIMPLE {
 			return tls, userSupplied
diff --git a/pilot/pkg/networking/core/v1alpha3/envoyfilter/cluster_patch.go b/pilot/pkg/networking/core/v1alpha3/envoyfilter/cluster_patch.go
index bb3e3d3a79..2bebd04955 100644
--- a/pilot/pkg/networking/core/v1alpha3/envoyfilter/cluster_patch.go
+++ b/pilot/pkg/networking/core/v1alpha3/envoyfilter/cluster_patch.go
@@ -32,7 +32,8 @@
 
 // ApplyClusterMerge processes the MERGE operation and merges the supplied configuration to the matched clusters.
 func ApplyClusterMerge(pctx networking.EnvoyFilter_PatchContext, efw *model.EnvoyFilterWrapper,
-	c *cluster.Cluster, hosts []host.Name) (out *cluster.Cluster) {
+	c *cluster.Cluster, hosts []host.Name,
+) (out *cluster.Cluster) {
 	defer runtime.HandleCrash(runtime.LogPanic, func(interface{}) {
 		log.Errorf("clusters patch caused panic, so the patches did not take effect")
 		IncrementEnvoyFilterErrorMetric(Cluster)
diff --git a/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch.go b/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch.go
index 05d9eaa4f5..8c4c2bc5a0 100644
--- a/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch.go
+++ b/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch.go
@@ -38,7 +38,8 @@ func ApplyListenerPatches(
 	patchContext networking.EnvoyFilter_PatchContext,
 	efw *model.EnvoyFilterWrapper,
 	listeners []*xdslistener.Listener,
-	skipAdds bool) (out []*xdslistener.Listener) {
+	skipAdds bool,
+) (out []*xdslistener.Listener) {
 	defer runtime.HandleCrash(runtime.LogPanic, func(interface{}) {
 		IncrementEnvoyFilterErrorMetric(Listener)
 		log.Errorf("listeners patch caused panic, so the patches did not take effect")
@@ -57,7 +58,8 @@ func patchListeners(
 	patchContext networking.EnvoyFilter_PatchContext,
 	efw *model.EnvoyFilterWrapper,
 	listeners []*xdslistener.Listener,
-	skipAdds bool) []*xdslistener.Listener {
+	skipAdds bool,
+) []*xdslistener.Listener {
 	listenersRemoved := false
 
 	// do all the changes for a single envoy filter crd object. [including adds]
@@ -105,7 +107,8 @@ func patchListeners(
 
 func patchListener(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	listener *xdslistener.Listener, listenersRemoved *bool) {
+	listener *xdslistener.Listener, listenersRemoved *bool,
+) {
 	for _, lp := range patches[networking.EnvoyFilter_LISTENER] {
 		if !commonConditionMatch(patchContext, lp) ||
 			!listenerMatch(listener, lp) {
@@ -127,7 +130,8 @@ func patchListener(patchContext networking.EnvoyFilter_PatchContext,
 
 func patchFilterChains(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	listener *xdslistener.Listener) {
+	listener *xdslistener.Listener,
+) {
 	filterChainsRemoved := false
 	for i, fc := range listener.FilterChains {
 		if fc.Filters == nil {
@@ -167,7 +171,8 @@ func patchFilterChains(patchContext networking.EnvoyFilter_PatchContext,
 func patchFilterChain(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
 	listener *xdslistener.Listener,
-	fc *xdslistener.FilterChain, filterChainRemoved *bool) {
+	fc *xdslistener.FilterChain, filterChainRemoved *bool,
+) {
 	for _, lp := range patches[networking.EnvoyFilter_FILTER_CHAIN] {
 		if !commonConditionMatch(patchContext, lp) ||
 			!listenerMatch(listener, lp) ||
@@ -239,7 +244,8 @@ func mergeTransportSocketListener(fc *xdslistener.FilterChain, lp *model.EnvoyFi
 
 func patchNetworkFilters(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	listener *xdslistener.Listener, fc *xdslistener.FilterChain) {
+	listener *xdslistener.Listener, fc *xdslistener.FilterChain,
+) {
 	for _, lp := range patches[networking.EnvoyFilter_NETWORK_FILTER] {
 		if !commonConditionMatch(patchContext, lp) ||
 			!listenerMatch(listener, lp) ||
@@ -346,7 +352,8 @@ func patchNetworkFilters(patchContext networking.EnvoyFilter_PatchContext,
 func patchNetworkFilter(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
 	listener *xdslistener.Listener, fc *xdslistener.FilterChain,
-	filter *xdslistener.Filter) bool {
+	filter *xdslistener.Filter,
+) bool {
 	for _, lp := range patches[networking.EnvoyFilter_NETWORK_FILTER] {
 		if !commonConditionMatch(patchContext, lp) ||
 			!listenerMatch(listener, lp) ||
@@ -405,7 +412,8 @@ func patchNetworkFilter(patchContext networking.EnvoyFilter_PatchContext,
 
 func patchHTTPFilters(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	listener *xdslistener.Listener, fc *xdslistener.FilterChain, filter *xdslistener.Filter) {
+	listener *xdslistener.Listener, fc *xdslistener.FilterChain, filter *xdslistener.Filter,
+) {
 	httpconn := &hcm.HttpConnectionManager{}
 	if filter.GetTypedConfig() != nil {
 		if err := filter.GetTypedConfig().UnmarshalTo(httpconn); err != nil {
@@ -528,7 +536,8 @@ func patchHTTPFilters(patchContext networking.EnvoyFilter_PatchContext,
 func patchHTTPFilter(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
 	listener *xdslistener.Listener, fc *xdslistener.FilterChain, filter *xdslistener.Filter,
-	httpFilter *hcm.HttpFilter) bool {
+	httpFilter *hcm.HttpFilter,
+) bool {
 	for _, lp := range patches[networking.EnvoyFilter_HTTP_FILTER] {
 		applied := false
 		if !commonConditionMatch(patchContext, lp) ||
@@ -714,12 +723,14 @@ func httpFilterMatch(filter *hcm.HttpFilter, lp *model.EnvoyFilterConfigPatchWra
 }
 
 func patchContextMatch(patchContext networking.EnvoyFilter_PatchContext,
-	lp *model.EnvoyFilterConfigPatchWrapper) bool {
+	lp *model.EnvoyFilterConfigPatchWrapper,
+) bool {
 	return lp.Match.Context == patchContext || lp.Match.Context == networking.EnvoyFilter_ANY
 }
 
 func commonConditionMatch(patchContext networking.EnvoyFilter_PatchContext,
-	lp *model.EnvoyFilterConfigPatchWrapper) bool {
+	lp *model.EnvoyFilterConfigPatchWrapper,
+) bool {
 	return patchContextMatch(patchContext, lp)
 }
 
diff --git a/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch_test.go b/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch_test.go
index 7bdc8ece2a..b4e76c08e5 100644
--- a/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch_test.go
+++ b/pilot/pkg/networking/core/v1alpha3/envoyfilter/listener_patch_test.go
@@ -96,7 +96,8 @@ func buildGolangPatchStruct(config string) *structpb.Struct {
 }
 
 func newTestEnvironment(serviceDiscovery model.ServiceDiscovery, meshConfig *meshconfig.MeshConfig,
-	configStore model.ConfigStore) *model.Environment {
+	configStore model.ConfigStore,
+) *model.Environment {
 	e := &model.Environment{
 		ServiceDiscovery: serviceDiscovery,
 		ConfigStore:      configStore,
diff --git a/pilot/pkg/networking/core/v1alpha3/envoyfilter/rc_patch.go b/pilot/pkg/networking/core/v1alpha3/envoyfilter/rc_patch.go
index d1bc4922f7..8517d6e1d4 100644
--- a/pilot/pkg/networking/core/v1alpha3/envoyfilter/rc_patch.go
+++ b/pilot/pkg/networking/core/v1alpha3/envoyfilter/rc_patch.go
@@ -33,7 +33,8 @@ func ApplyRouteConfigurationPatches(
 	patchContext networking.EnvoyFilter_PatchContext,
 	proxy *model.Proxy,
 	efw *model.EnvoyFilterWrapper,
-	routeConfiguration *route.RouteConfiguration) (out *route.RouteConfiguration) {
+	routeConfiguration *route.RouteConfiguration,
+) (out *route.RouteConfiguration) {
 	defer runtime.HandleCrash(runtime.LogPanic, func(interface{}) {
 		IncrementEnvoyFilterErrorMetric(Route)
 		log.Errorf("route patch caused panic, so the patches did not take effect")
@@ -69,7 +70,8 @@ func ApplyRouteConfigurationPatches(
 
 func patchVirtualHosts(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	routeConfiguration *route.RouteConfiguration, portMap model.GatewayPortMap) {
+	routeConfiguration *route.RouteConfiguration, portMap model.GatewayPortMap,
+) {
 	removedVirtualHosts := sets.New()
 	// first do removes/merges/replaces
 	for i := range routeConfiguration.VirtualHosts {
@@ -108,7 +110,8 @@ func patchVirtualHosts(patchContext networking.EnvoyFilter_PatchContext,
 func patchVirtualHost(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
 	routeConfiguration *route.RouteConfiguration, virtualHosts []*route.VirtualHost,
-	idx int, portMap model.GatewayPortMap) bool {
+	idx int, portMap model.GatewayPortMap,
+) bool {
 	for _, rp := range patches[networking.EnvoyFilter_VIRTUAL_HOST] {
 		applied := false
 		if commonConditionMatch(patchContext, rp) &&
@@ -145,7 +148,8 @@ func hasRouteMatch(rp *model.EnvoyFilterConfigPatchWrapper) bool {
 
 func patchHTTPRoutes(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	routeConfiguration *route.RouteConfiguration, virtualHost *route.VirtualHost, portMap model.GatewayPortMap) {
+	routeConfiguration *route.RouteConfiguration, virtualHost *route.VirtualHost, portMap model.GatewayPortMap,
+) {
 	routesRemoved := false
 	// Apply the route level removes/merges if any.
 	for index := range virtualHost.Routes {
@@ -237,7 +241,8 @@ func patchHTTPRoutes(patchContext networking.EnvoyFilter_PatchContext,
 
 func patchHTTPRoute(patchContext networking.EnvoyFilter_PatchContext,
 	patches map[networking.EnvoyFilter_ApplyTo][]*model.EnvoyFilterConfigPatchWrapper,
-	routeConfiguration *route.RouteConfiguration, virtualHost *route.VirtualHost, routeIndex int, routesRemoved *bool, portMap model.GatewayPortMap) {
+	routeConfiguration *route.RouteConfiguration, virtualHost *route.VirtualHost, routeIndex int, routesRemoved *bool, portMap model.GatewayPortMap,
+) {
 	for _, rp := range patches[networking.EnvoyFilter_HTTP_ROUTE] {
 		applied := false
 		if commonConditionMatch(patchContext, rp) &&
@@ -261,7 +266,8 @@ func patchHTTPRoute(patchContext networking.EnvoyFilter_PatchContext,
 }
 
 func routeConfigurationMatch(patchContext networking.EnvoyFilter_PatchContext, rc *route.RouteConfiguration,
-	rp *model.EnvoyFilterConfigPatchWrapper, portMap model.GatewayPortMap) bool {
+	rp *model.EnvoyFilterConfigPatchWrapper, portMap model.GatewayPortMap,
+) bool {
 	rMatch := rp.Match.GetRouteConfiguration()
 	if rMatch == nil {
 		return true
diff --git a/pilot/pkg/networking/core/v1alpha3/extension/wasmplugin.go b/pilot/pkg/networking/core/v1alpha3/extension/wasmplugin.go
index 400847898f..154b873a79 100644
--- a/pilot/pkg/networking/core/v1alpha3/extension/wasmplugin.go
+++ b/pilot/pkg/networking/core/v1alpha3/extension/wasmplugin.go
@@ -46,7 +46,8 @@
 // plugins of a provided phase from the WASM plugin set and append them to the list of filters
 func PopAppend(list []*hcm_filter.HttpFilter,
 	filterMap map[extensions.PluginPhase][]*model.WasmPluginWrapper,
-	phase extensions.PluginPhase) []*hcm_filter.HttpFilter {
+	phase extensions.PluginPhase,
+) []*hcm_filter.HttpFilter {
 	for _, ext := range filterMap[phase] {
 		list = append(list, toEnvoyHTTPFilter(ext))
 	}
@@ -69,7 +70,8 @@ func toEnvoyHTTPFilter(wasmPlugin *model.WasmPluginWrapper) *hcm_filter.HttpFilt
 // InsertedExtensionConfigurations returns pre-generated extension configurations added via WasmPlugin.
 func InsertedExtensionConfigurations(
 	wasmPlugins map[extensions.PluginPhase][]*model.WasmPluginWrapper,
-	names []string, pullSecrets map[string][]byte) []*envoy_config_core_v3.TypedExtensionConfig {
+	names []string, pullSecrets map[string][]byte,
+) []*envoy_config_core_v3.TypedExtensionConfig {
 	result := make([]*envoy_config_core_v3.TypedExtensionConfig, 0)
 	if len(wasmPlugins) == 0 {
 		return result
diff --git a/pilot/pkg/networking/core/v1alpha3/extension_config_builder.go b/pilot/pkg/networking/core/v1alpha3/extension_config_builder.go
index e82b052a29..1ab64c23db 100644
--- a/pilot/pkg/networking/core/v1alpha3/extension_config_builder.go
+++ b/pilot/pkg/networking/core/v1alpha3/extension_config_builder.go
@@ -25,7 +25,8 @@
 // BuildExtensionConfiguration returns the list of extension configuration for the given proxy and list of names.
 // This is the ECDS output.
 func (configgen *ConfigGeneratorImpl) BuildExtensionConfiguration(
-	proxy *model.Proxy, push *model.PushContext, extensionConfigNames []string, pullSecrets map[string][]byte) []*core.TypedExtensionConfig {
+	proxy *model.Proxy, push *model.PushContext, extensionConfigNames []string, pullSecrets map[string][]byte,
+) []*core.TypedExtensionConfig {
 	envoyFilterPatches := push.EnvoyFilters(proxy)
 	extensions := envoyfilter.InsertedExtensionConfigurations(envoyFilterPatches, extensionConfigNames)
 	wasmPlugins := push.WasmPlugins(proxy)
diff --git a/pilot/pkg/networking/core/v1alpha3/gateway.go b/pilot/pkg/networking/core/v1alpha3/gateway.go
index 7fc8baff67..e4df9fd96c 100644
--- a/pilot/pkg/networking/core/v1alpha3/gateway.go
+++ b/pilot/pkg/networking/core/v1alpha3/gateway.go
@@ -276,7 +276,8 @@ func getListenerName(bind string, port int, transport istionetworking.TransportP
 }
 
 func buildNameToServiceMapForHTTPRoutes(node *model.Proxy, push *model.PushContext,
-	virtualService config.Config) map[host.Name]*model.Service {
+	virtualService config.Config,
+) map[host.Name]*model.Service {
 	vs := virtualService.Spec.(*networking.VirtualService)
 	nameToServiceMap := map[host.Name]*model.Service{}
 
@@ -318,7 +319,8 @@ func buildNameToServiceMapForHTTPRoutes(node *model.Proxy, push *model.PushConte
 }
 
 func (configgen *ConfigGeneratorImpl) buildGatewayHTTPRouteConfig(node *model.Proxy, push *model.PushContext,
-	routeName string) *route.RouteConfiguration {
+	routeName string,
+) *route.RouteConfiguration {
 	if node.MergedGateway == nil {
 		log.Warnf("buildGatewayRoutes: no gateways for router %v", node.ID)
 		return &route.RouteConfiguration{
@@ -660,12 +662,15 @@ func buildGatewayConnectionManager(proxyConfig *meshconfig.ProxyConfig, node *mo
 // TLS mode      | Mesh-wide SDS | Ingress SDS | Resulting Configuration
 // SIMPLE/MUTUAL |    ENABLED    |   ENABLED   | support SDS at ingress gateway to terminate SSL communication outside the mesh
 // ISTIO_MUTUAL  |    ENABLED    |   DISABLED  | support SDS at gateway to terminate workload mTLS, with internal workloads
-// 											   | for egress or with another trusted cluster for ingress)
+//
+//	| for egress or with another trusted cluster for ingress)
+//
 // ISTIO_MUTUAL  |    DISABLED   |   DISABLED  | use file-mounted secret paths to terminate workload mTLS from gateway
 //
 // Note that ISTIO_MUTUAL TLS mode and ingressSds should not be used simultaneously on the same ingress gateway.
 func buildGatewayListenerTLSContext(
-	server *networking.Server, proxy *model.Proxy, transportProtocol istionetworking.TransportProtocol) *tls.DownstreamTlsContext {
+	server *networking.Server, proxy *model.Proxy, transportProtocol istionetworking.TransportProtocol,
+) *tls.DownstreamTlsContext {
 	// Server.TLS cannot be nil or passthrough. But as a safety guard, return nil
 	if server.Tls == nil || gateway.IsPassThroughServer(server) {
 		return nil // We don't need to setup TLS context for passthrough mode
@@ -686,7 +691,8 @@ func convertTLSProtocol(in networking.ServerTLSSettings_TLSProtocol) tls.TlsPara
 
 func (configgen *ConfigGeneratorImpl) createGatewayTCPFilterChainOpts(
 	node *model.Proxy, push *model.PushContext, server *networking.Server,
-	gatewayName string) []*filterChainOpts {
+	gatewayName string,
+) []*filterChainOpts {
 	// We have a TCP/TLS server. This could be TLS termination (user specifies server.TLS with simple/mutual)
 	// or opaque TCP (server.TLS is nil). or it could be a TLS passthrough with SNI based routing.
 
@@ -772,7 +778,8 @@ func buildGatewayNetworkFiltersFromTCPRoutes(node *model.Proxy, push *model.Push
 // It first obtains all virtual services bound to the set of Gateways for this workload, filters them by this
 // server's port and hostnames, and produces network filters for each destination from the filtered services
 func buildGatewayNetworkFiltersFromTLSRoutes(node *model.Proxy, push *model.PushContext, server *networking.Server,
-	gatewayName string) []*filterChainOpts {
+	gatewayName string,
+) []*filterChainOpts {
 	port := &model.Port{
 		Name:     server.Port.Name,
 		Port:     int(server.Port.Number),
diff --git a/pilot/pkg/networking/core/v1alpha3/httproute_test.go b/pilot/pkg/networking/core/v1alpha3/httproute_test.go
index b6d5f65b73..7bf07b5f74 100644
--- a/pilot/pkg/networking/core/v1alpha3/httproute_test.go
+++ b/pilot/pkg/networking/core/v1alpha3/httproute_test.go
@@ -433,15 +433,11 @@ func TestSidecarOutboundHTTPRouteConfigWithDuplicateHosts(t *testing.T) {
 			})
 
 			vHostCache := make(map[int][]*route.VirtualHost)
-			routeName := "80"
 			resource, _ := cg.ConfigGen.buildSidecarOutboundHTTPRouteConfig(
 				cg.SetupProxy(nil), &model.PushRequest{Push: cg.PushContext()}, "80", vHostCache, nil, nil)
 			routeCfg := &route.RouteConfiguration{}
 			resource.Resource.UnmarshalTo(routeCfg)
 			xdstest.ValidateRouteConfiguration(t, routeCfg)
-			if routeCfg == nil {
-				t.Fatalf("got nil route for %s", routeName)
-			}
 
 			got := map[string][]string{}
 			clusters := map[string]string{}
@@ -1528,9 +1524,6 @@ func testSidecarRDSVHosts(t *testing.T, services []*model.Service,
 	routeCfg := &route.RouteConfiguration{}
 	resource.Resource.UnmarshalTo(routeCfg)
 	xdstest.ValidateRouteConfiguration(t, routeCfg)
-	if routeCfg == nil {
-		t.Fatalf("got nil route for %s", routeName)
-	}
 
 	if expectedRoutes == 0 {
 		expectedRoutes = len(expectedHosts)
diff --git a/pilot/pkg/networking/core/v1alpha3/listener.go b/pilot/pkg/networking/core/v1alpha3/listener.go
index 9ec2386537..44d00fda4d 100644
--- a/pilot/pkg/networking/core/v1alpha3/listener.go
+++ b/pilot/pkg/networking/core/v1alpha3/listener.go
@@ -101,7 +101,8 @@ type MutableListener struct {
 
 // BuildListeners produces a list of listeners and referenced clusters for all proxies
 func (configgen *ConfigGeneratorImpl) BuildListeners(node *model.Proxy,
-	push *model.PushContext) []*listener.Listener {
+	push *model.PushContext,
+) []*listener.Listener {
 	builder := NewListenerBuilder(node, push)
 
 	switch node.Type {
@@ -116,7 +117,8 @@ func (configgen *ConfigGeneratorImpl) BuildListeners(node *model.Proxy,
 }
 
 func BuildListenerTLSContext(serverTLSSettings *networking.ServerTLSSettings,
-	proxy *model.Proxy, transportProtocol istionetworking.TransportProtocol, gatewayTCPServerWithTerminatingTLS bool) *auth.DownstreamTlsContext {
+	proxy *model.Proxy, transportProtocol istionetworking.TransportProtocol, gatewayTCPServerWithTerminatingTLS bool,
+) *auth.DownstreamTlsContext {
 	alpnByTransport := util.ALPNHttp
 	if transportProtocol == istionetworking.TransportProtocolQUIC {
 		alpnByTransport = util.ALPNHttp3OverQUIC
@@ -281,7 +283,8 @@ func (c outboundListenerConflict) addMetric(metrics model.Metrics) {
 // buildSidecarOutboundListeners generates http and tcp listeners for
 // outbound connections from the proxy based on the sidecar scope associated with the proxy.
 func (lb *ListenerBuilder) buildSidecarOutboundListeners(node *model.Proxy,
-	push *model.PushContext) []*listener.Listener {
+	push *model.PushContext,
+) []*listener.Listener {
 	noneMode := node.GetInterceptionMode() == model.InterceptionNone
 
 	actualWildcard, actualLocalHostAddress := getActualWildcardAndLocalHost(node)
@@ -511,7 +514,8 @@ func (lb *ListenerBuilder) buildSidecarOutboundListeners(node *model.Proxy,
 }
 
 func (lb *ListenerBuilder) buildHTTPProxy(node *model.Proxy,
-	push *model.PushContext) *listener.Listener {
+	push *model.PushContext,
+) *listener.Listener {
 	httpProxyPort := push.Mesh.ProxyHttpPort // global
 	if node.Metadata.HTTPProxyPort != "" {
 		port, err := strconv.Atoi(node.Metadata.HTTPProxyPort)
@@ -576,7 +580,8 @@ func (lb *ListenerBuilder) buildHTTPProxy(node *model.Proxy,
 
 func buildSidecarOutboundHTTPListenerOptsForPortOrUDS(listenerMapKey *string,
 	currentListenerEntry **outboundListenerEntry, listenerOpts *buildListenerOpts,
-	listenerMap map[string]*outboundListenerEntry, actualWildcard string) (bool, []*filterChainOpts) {
+	listenerMap map[string]*outboundListenerEntry, actualWildcard string,
+) (bool, []*filterChainOpts) {
 	// first identify the bind if its not set. Then construct the key
 	// used to lookup the listener in the conflict map.
 	if len(listenerOpts.bind) == 0 { // no user specified bind. Use 0.0.0.0:Port
@@ -675,7 +680,8 @@ func buildSidecarOutboundHTTPListenerOptsForPortOrUDS(listenerMapKey *string,
 
 func buildSidecarOutboundTCPListenerOptsForPortOrUDS(listenerMapKey *string,
 	currentListenerEntry **outboundListenerEntry, listenerOpts *buildListenerOpts, listenerMap map[string]*outboundListenerEntry,
-	virtualServices []config.Config, actualWildcard string) (bool, []*filterChainOpts) {
+	virtualServices []config.Config, actualWildcard string,
+) (bool, []*filterChainOpts) {
 	// first identify the bind if its not set. Then construct the key
 	// used to lookup the listener in the conflict map.
 
@@ -784,7 +790,8 @@ func buildSidecarOutboundTCPListenerOptsForPortOrUDS(listenerMapKey *string,
 // (as vhosts are shipped through RDS).  TCP listeners on same port are
 // allowed only if they have different CIDR matches.
 func (lb *ListenerBuilder) buildSidecarOutboundListenerForPortOrUDS(listenerOpts buildListenerOpts,
-	listenerMap map[string]*outboundListenerEntry, virtualServices []config.Config, actualWildcard string) {
+	listenerMap map[string]*outboundListenerEntry, virtualServices []config.Config, actualWildcard string,
+) {
 	var listenerMapKey string
 	var currentListenerEntry *outboundListenerEntry
 	var ret bool
@@ -1366,7 +1373,8 @@ func (ml *MutableListener) build(builder *ListenerBuilder, opts buildListenerOpt
 }
 
 func mergeTCPFilterChains(incoming []*listener.FilterChain, listenerOpts buildListenerOpts, listenerMapKey string,
-	listenerMap map[string]*outboundListenerEntry) []*listener.FilterChain {
+	listenerMap map[string]*outboundListenerEntry,
+) []*listener.FilterChain {
 	// TODO(rshriram) merge multiple identical filter chains with just a single destination CIDR based
 	// filter chain match, into a single filter chain and array of destinationcidr matches
 
diff --git a/pilot/pkg/networking/core/v1alpha3/listener_test.go b/pilot/pkg/networking/core/v1alpha3/listener_test.go
index 09dc74ac47..396ec93b77 100644
--- a/pilot/pkg/networking/core/v1alpha3/listener_test.go
+++ b/pilot/pkg/networking/core/v1alpha3/listener_test.go
@@ -2402,7 +2402,8 @@ func getFilterConfig(filter *listener.Filter, out proto.Message) error {
 }
 
 func buildOutboundListeners(t *testing.T, proxy *model.Proxy, sidecarConfig *config.Config,
-	virtualService *config.Config, services ...*model.Service) []*listener.Listener {
+	virtualService *config.Config, services ...*model.Service,
+) []*listener.Listener {
 	t.Helper()
 	cg := NewConfigGenTest(t, TestOptions{
 		Services:       services,
diff --git a/pilot/pkg/networking/core/v1alpha3/loadbalancer/loadbalancer.go b/pilot/pkg/networking/core/v1alpha3/loadbalancer/loadbalancer.go
index 9ed7827983..1671aa40dd 100644
--- a/pilot/pkg/networking/core/v1alpha3/loadbalancer/loadbalancer.go
+++ b/pilot/pkg/networking/core/v1alpha3/loadbalancer/loadbalancer.go
@@ -87,7 +87,8 @@ func ApplyLocalityLBSetting(
 func applyLocalityWeight(
 	locality *core.Locality,
 	loadAssignment *endpoint.ClusterLoadAssignment,
-	distribute []*v1alpha3.LocalityLoadBalancerSetting_Distribute) {
+	distribute []*v1alpha3.LocalityLoadBalancerSetting_Distribute,
+) {
 	if distribute == nil {
 		return
 	}
@@ -146,7 +147,8 @@ func applyLocalityWeight(
 func applyLocalityFailover(
 	locality *core.Locality,
 	loadAssignment *endpoint.ClusterLoadAssignment,
-	failover []*v1alpha3.LocalityLoadBalancerSetting_Failover) {
+	failover []*v1alpha3.LocalityLoadBalancerSetting_Failover,
+) {
 	// key is priority, value is the index of the LocalityLbEndpoints in ClusterLoadAssignment
 	priorityMap := map[int][]int{}
 
@@ -206,7 +208,8 @@ func applyPriorityFailover(
 	loadAssignment *endpoint.ClusterLoadAssignment,
 	wrappedLocalityLbEndpoints []*WrappedLocalityLbEndpoints,
 	proxyLabels map[string]string,
-	failoverPriorities []string) {
+	failoverPriorities []string,
+) {
 	if len(proxyLabels) == 0 || len(wrappedLocalityLbEndpoints) == 0 {
 		return
 	}
@@ -245,7 +248,8 @@ func applyPriorityFailover(
 func applyPriorityFailoverPerLocality(
 	proxyLabels map[string]string,
 	ep *WrappedLocalityLbEndpoints,
-	failoverPriorities []string) []*endpoint.LocalityLbEndpoints {
+	failoverPriorities []string,
+) []*endpoint.LocalityLbEndpoints {
 	lowestPriority := len(failoverPriorities)
 	// key is priority, value is the index of LocalityLbEndpoints.LbEndpoints
 	priorityMap := map[int][]int{}
diff --git a/pilot/pkg/networking/core/v1alpha3/networkfilter.go b/pilot/pkg/networking/core/v1alpha3/networkfilter.go
index 07dde16add..b1f6c5da37 100644
--- a/pilot/pkg/networking/core/v1alpha3/networkfilter.go
+++ b/pilot/pkg/networking/core/v1alpha3/networkfilter.go
@@ -71,7 +71,8 @@ func setAccessLogAndBuildTCPFilter(push *model.PushContext, node *model.Proxy, c
 // buildOutboundNetworkFiltersWithSingleDestination takes a single cluster name
 // and builds a stack of network filters.
 func buildOutboundNetworkFiltersWithSingleDestination(push *model.PushContext, node *model.Proxy,
-	statPrefix, clusterName, subsetName string, port *model.Port, destinationRule *networking.DestinationRule) []*listener.Filter {
+	statPrefix, clusterName, subsetName string, port *model.Port, destinationRule *networking.DestinationRule,
+) []*listener.Filter {
 	tcpProxy := &tcp.TcpProxy{
 		StatPrefix:       statPrefix,
 		ClusterSpecifier: &tcp.TcpProxy_Cluster{Cluster: clusterName},
@@ -95,7 +96,8 @@ func buildOutboundNetworkFiltersWithSingleDestination(push *model.PushContext, n
 // buildOutboundNetworkFiltersWithWeightedClusters takes a set of weighted
 // destination routes and builds a stack of network filters.
 func buildOutboundNetworkFiltersWithWeightedClusters(node *model.Proxy, routes []*networking.RouteDestination,
-	push *model.PushContext, port *model.Port, configMeta config.Meta, destinationRule *networking.DestinationRule) []*listener.Filter {
+	push *model.PushContext, port *model.Port, configMeta config.Meta, destinationRule *networking.DestinationRule,
+) []*listener.Filter {
 	statPrefix := configMeta.Name + "." + configMeta.Namespace
 	clusterSpecifier := &tcp.TcpProxy_WeightedClusters{
 		WeightedClusters: &tcp.TcpProxy_WeightedCluster{},
@@ -199,7 +201,8 @@ func buildNetworkFiltersStack(p protocol.Instance, tcpFilter *listener.Filter, s
 // filter).
 func buildOutboundNetworkFilters(node *model.Proxy,
 	routes []*networking.RouteDestination, push *model.PushContext,
-	port *model.Port, configMeta config.Meta) []*listener.Filter {
+	port *model.Port, configMeta config.Meta,
+) []*listener.Filter {
 	service := push.ServiceForHostname(node, host.Name(routes[0].Destination.Host))
 	var destinationRule *networking.DestinationRule
 	if service != nil {
diff --git a/pilot/pkg/networking/core/v1alpha3/tls.go b/pilot/pkg/networking/core/v1alpha3/tls.go
index 179f3945b4..dc61451770 100644
--- a/pilot/pkg/networking/core/v1alpha3/tls.go
+++ b/pilot/pkg/networking/core/v1alpha3/tls.go
@@ -94,7 +94,8 @@ func hashRuntimeTLSMatchPredicates(match *v1alpha3.TLSMatchAttributes) string {
 
 func buildSidecarOutboundTLSFilterChainOpts(node *model.Proxy, push *model.PushContext, destinationCIDR string,
 	service *model.Service, bind string, listenPort *model.Port,
-	gateways map[string]bool, configs []config.Config) []*filterChainOpts {
+	gateways map[string]bool, configs []config.Config,
+) []*filterChainOpts {
 	if !listenPort.Protocol.IsTLS() {
 		return nil
 	}
@@ -212,7 +213,8 @@ func buildSidecarOutboundTLSFilterChainOpts(node *model.Proxy, push *model.PushC
 
 func buildSidecarOutboundTCPFilterChainOpts(node *model.Proxy, push *model.PushContext, destinationCIDR string,
 	service *model.Service, listenPort *model.Port,
-	gateways map[string]bool, configs []config.Config) []*filterChainOpts {
+	gateways map[string]bool, configs []config.Config,
+) []*filterChainOpts {
 	if listenPort.Protocol.IsTLS() {
 		return nil
 	}
@@ -322,7 +324,8 @@ func buildSidecarOutboundTCPFilterChainOpts(node *model.Proxy, push *model.PushC
 // missing service throughout this file
 func buildSidecarOutboundTCPTLSFilterChainOpts(node *model.Proxy, push *model.PushContext,
 	configs []config.Config, destinationCIDR string, service *model.Service, bind string, listenPort *model.Port,
-	gateways map[string]bool) []*filterChainOpts {
+	gateways map[string]bool,
+) []*filterChainOpts {
 	out := make([]*filterChainOpts, 0)
 	var svcConfigs []config.Config
 	if service != nil {
diff --git a/pilot/pkg/networking/core/v1alpha3/tracing.go b/pilot/pkg/networking/core/v1alpha3/tracing.go
index 1ab44fadbf..6080d6cd1e 100644
--- a/pilot/pkg/networking/core/v1alpha3/tracing.go
+++ b/pilot/pkg/networking/core/v1alpha3/tracing.go
@@ -127,7 +127,8 @@ func configureTracingFromSpec(
 // TODO: follow-on work to enable bootstrapping of clusters for $(HOST_IP):PORT addresses.
 
 func configureFromProviderConfig(pushCtx *model.PushContext, meta *model.NodeMetadata,
-	providerCfg *meshconfig.MeshConfig_ExtensionProvider) (*hpb.HttpConnectionManager_Tracing, *xdsfilters.RouterFilterContext, error) {
+	providerCfg *meshconfig.MeshConfig_ExtensionProvider,
+) (*hpb.HttpConnectionManager_Tracing, *xdsfilters.RouterFilterContext, error) {
 	tracing := &hpb.HttpConnectionManager_Tracing{}
 	var rfCtx *xdsfilters.RouterFilterContext
 	var err error
@@ -281,7 +282,8 @@ func datadogConfigGen(cluster string) (*anypb.Any, error) {
 type typedConfigGenFn func() (*anypb.Any, error)
 
 func buildHCMTracing(pushCtx *model.PushContext, provider, svc string, port, maxTagLen uint32,
-	anyFn typedConfigGenFromClusterFn) (*hpb.HttpConnectionManager_Tracing, error) {
+	anyFn typedConfigGenFromClusterFn,
+) (*hpb.HttpConnectionManager_Tracing, error) {
 	config := &hpb.HttpConnectionManager_Tracing{}
 
 	_, cluster, err := clusterLookupFn(pushCtx, svc, int(port))
@@ -289,14 +291,14 @@ func buildHCMTracing(pushCtx *model.PushContext, provider, svc string, port, max
 		return config, fmt.Errorf("could not find cluster for tracing provider %q: %v", provider, err)
 	}
 
-	any, err := anyFn(cluster)
+	cfg, err := anyFn(cluster)
 	if err != nil {
 		return config, fmt.Errorf("could not configure tracing provider %q: %v", provider, err)
 	}
 
 	config.Provider = &tracingcfg.Tracing_Http{
 		Name:       provider,
-		ConfigType: &tracingcfg.Tracing_Http_TypedConfig{TypedConfig: any},
+		ConfigType: &tracingcfg.Tracing_Http_TypedConfig{TypedConfig: cfg},
 	}
 
 	if maxTagLen != 0 {
@@ -307,14 +309,14 @@ func buildHCMTracing(pushCtx *model.PushContext, provider, svc string, port, max
 
 func buildHCMTracingOpenCensus(provider string, maxTagLen uint32, anyFn typedConfigGenFn) (*hpb.HttpConnectionManager_Tracing, error) {
 	config := &hpb.HttpConnectionManager_Tracing{}
-	any, err := anyFn()
+	cfg, err := anyFn()
 	if err != nil {
 		return config, fmt.Errorf("could not configure tracing provider %q: %v", provider, err)
 	}
 
 	config.Provider = &tracingcfg.Tracing_Http{
 		Name:       provider,
-		ConfigType: &tracingcfg.Tracing_Http_TypedConfig{TypedConfig: any},
+		ConfigType: &tracingcfg.Tracing_Http_TypedConfig{TypedConfig: cfg},
 	}
 
 	if maxTagLen != 0 {
@@ -469,7 +471,8 @@ func proxyConfigSamplingValue(config *meshconfig.ProxyConfig) float64 {
 }
 
 func configureCustomTags(hcmTracing *hpb.HttpConnectionManager_Tracing,
-	providerTags map[string]*telemetrypb.Tracing_CustomTag, proxyCfg *meshconfig.ProxyConfig, metadata *model.NodeMetadata) {
+	providerTags map[string]*telemetrypb.Tracing_CustomTag, proxyCfg *meshconfig.ProxyConfig, metadata *model.NodeMetadata,
+) {
 	var tags []*tracing.CustomTag
 
 	// TODO(dougreid): remove support for this feature. We don't want this to be
diff --git a/pilot/pkg/networking/core/v1alpha3/tracing_test.go b/pilot/pkg/networking/core/v1alpha3/tracing_test.go
index 49eeb9820b..ea8edd42cd 100644
--- a/pilot/pkg/networking/core/v1alpha3/tracing_test.go
+++ b/pilot/pkg/networking/core/v1alpha3/tracing_test.go
@@ -397,7 +397,8 @@ func fakeTracingSpecNoProviderWithNilCustomTag(sampling float64, disableReportin
 }
 
 func fakeTracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider, sampling float64, disableReporting bool,
-	useRequestIDForTraceSampling bool) *model.TracingConfig {
+	useRequestIDForTraceSampling bool,
+) *model.TracingConfig {
 	t := &model.TracingConfig{
 		ClientSpec: tracingSpec(provider, sampling, disableReporting, useRequestIDForTraceSampling),
 		ServerSpec: tracingSpec(provider, sampling, disableReporting, useRequestIDForTraceSampling),
@@ -406,7 +407,8 @@ func fakeTracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider, sampling
 }
 
 func fakeClientOnlyTracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider, sampling float64, disableReporting bool,
-	useRequestIDForTraceSampling bool) *model.TracingConfig {
+	useRequestIDForTraceSampling bool,
+) *model.TracingConfig {
 	t := &model.TracingConfig{
 		ClientSpec: tracingSpec(provider, sampling, disableReporting, useRequestIDForTraceSampling),
 		ServerSpec: model.TracingSpec{
@@ -417,7 +419,8 @@ func fakeClientOnlyTracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider
 }
 
 func fakeServerOnlyTracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider, sampling float64, disableReporting bool,
-	useRequestIDForTraceSampling bool) *model.TracingConfig {
+	useRequestIDForTraceSampling bool,
+) *model.TracingConfig {
 	t := &model.TracingConfig{
 		ClientSpec: model.TracingSpec{
 			Disabled: true,
@@ -428,7 +431,8 @@ func fakeServerOnlyTracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider
 }
 
 func tracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider, sampling float64, disableReporting bool,
-	useRequestIDForTraceSampling bool) model.TracingSpec {
+	useRequestIDForTraceSampling bool,
+) model.TracingSpec {
 	return model.TracingSpec{
 		Provider:                 provider,
 		Disabled:                 disableReporting,
@@ -447,7 +451,8 @@ func tracingSpec(provider *meshconfig.MeshConfig_ExtensionProvider, sampling flo
 }
 
 func fakeTracingSpecWithNilCustomTag(provider *meshconfig.MeshConfig_ExtensionProvider, sampling float64, disableReporting bool,
-	useRequestIDForTraceSampling bool) *model.TracingConfig {
+	useRequestIDForTraceSampling bool,
+) *model.TracingConfig {
 	t := &model.TracingConfig{
 		ClientSpec: model.TracingSpec{
 			Provider:                 provider,
diff --git a/pilot/pkg/networking/grpcgen/lds.go b/pilot/pkg/networking/grpcgen/lds.go
index 9080e21a72..7f9918ec42 100644
--- a/pilot/pkg/networking/grpcgen/lds.go
+++ b/pilot/pkg/networking/grpcgen/lds.go
@@ -236,18 +236,16 @@ func buildInboundFilterChain(node *model.Proxy, push *model.PushContext, nameSuf
 //
 // See: xds/interal/httpfilter/rbac
 //
-//
 // TODO: gRPC also supports 'per route override' - not yet clear how to use it, Istio uses path expressions instead and we don't generate
 // vhosts or routes for the inbound listener.
 //
 // For gateways it would make a lot of sense to use this concept, same for moving path prefix at top level ( more scalable, easier for users)
 // This should probably be done for the v2 API.
 //
-//
-//
 // nolint: unparam
 func buildRBAC(node *model.Proxy, push *model.PushContext, suffix string, context *tls.DownstreamTlsContext,
-	a rbacpb.RBAC_Action, policies []model.AuthorizationPolicy) *rbacpb.RBAC {
+	a rbacpb.RBAC_Action, policies []model.AuthorizationPolicy,
+) *rbacpb.RBAC {
 	rules := &rbacpb.RBAC{
 		Action:   a,
 		Policies: map[string]*rbacpb.Policy{},
diff --git a/pilot/pkg/networking/networking.go b/pilot/pkg/networking/networking.go
index c7eef41a07..574ef4e4c2 100644
--- a/pilot/pkg/networking/networking.go
+++ b/pilot/pkg/networking/networking.go
@@ -46,7 +46,8 @@
 
 // ModelProtocolToListenerProtocol converts from a config.Protocol to its corresponding plugin.ListenerProtocol
 func ModelProtocolToListenerProtocol(p protocol.Instance,
-	trafficDirection core.TrafficDirection) ListenerProtocol {
+	trafficDirection core.TrafficDirection,
+) ListenerProtocol {
 	switch p {
 	case protocol.HTTP, protocol.HTTP2, protocol.HTTP_PROXY, protocol.GRPC, protocol.GRPCWeb:
 		return ListenerProtocolHTTP
diff --git a/pilot/pkg/networking/util/util.go b/pilot/pkg/networking/util/util.go
index 8fbdd09134..a112a45b0e 100644
--- a/pilot/pkg/networking/util/util.go
+++ b/pilot/pkg/networking/util/util.go
@@ -431,7 +431,8 @@ func MergeAnyWithAny(dst *anypb.Any, src *anypb.Any) (*anypb.Any, error) {
 
 // BuildLbEndpointMetadata adds metadata values to a lb endpoint
 func BuildLbEndpointMetadata(networkID network.ID, tlsMode, workloadname, namespace string,
-	clusterID cluster.ID, labels labels.Instance) *core.Metadata {
+	clusterID cluster.ID, labels labels.Instance,
+) *core.Metadata {
 	if networkID == "" && (tlsMode == "" || tlsMode == model.DisabledTLSModeLabel) &&
 		(!features.EndpointTelemetryLabel || !features.EnableTelemetryLabel) {
 		return nil
diff --git a/pilot/pkg/security/authz/builder/extauthz.go b/pilot/pkg/security/authz/builder/extauthz.go
index 058293af8b..9e13a2e699 100644
--- a/pilot/pkg/security/authz/builder/extauthz.go
+++ b/pilot/pkg/security/authz/builder/extauthz.go
@@ -146,7 +146,8 @@ func getExtAuthz(resolved map[string]*builtExtAuthz, providers []string) (*built
 }
 
 func buildExtAuthzHTTP(push *model.PushContext,
-	config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationHttpProvider) (*builtExtAuthz, error) {
+	config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationHttpProvider,
+) (*builtExtAuthz, error) {
 	var errs error
 	port, err := parsePort(config.Port)
 	if err != nil {
@@ -189,7 +190,8 @@ func buildExtAuthzHTTP(push *model.PushContext,
 }
 
 func buildExtAuthzGRPC(push *model.PushContext,
-	config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationGrpcProvider) (*builtExtAuthz, error) {
+	config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationGrpcProvider,
+) (*builtExtAuthz, error) {
 	var errs error
 	port, err := parsePort(config.Port)
 	if err != nil {
@@ -232,7 +234,8 @@ func parseStatusOnError(status string) (*envoytypev3.HttpStatus, error) {
 }
 
 func generateHTTPConfig(hostname, cluster string, status *envoytypev3.HttpStatus,
-	config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationHttpProvider) *builtExtAuthz {
+	config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationHttpProvider,
+) *builtExtAuthz {
 	service := &extauthzhttp.HttpService{
 		PathPrefix: config.PathPrefix,
 		ServerUri: &envoy_config_core_v3.HttpUri{
@@ -292,7 +295,8 @@ func generateHTTPConfig(hostname, cluster string, status *envoytypev3.HttpStatus
 }
 
 func generateGRPCConfig(cluster string, config *meshconfig.MeshConfig_ExtensionProvider_EnvoyExternalAuthorizationGrpcProvider,
-	status *envoytypev3.HttpStatus) *builtExtAuthz {
+	status *envoytypev3.HttpStatus,
+) *builtExtAuthz {
 	// The cluster includes the character `|` that is invalid in gRPC authority header and will cause the connection
 	// rejected in the server side, replace it with a valid character and set in authority otherwise ext_authz will
 	// use the cluster name as default authority.
diff --git a/pilot/pkg/security/model/authentication.go b/pilot/pkg/security/model/authentication.go
index 7301b0fb45..9d76a38505 100644
--- a/pilot/pkg/security/model/authentication.go
+++ b/pilot/pkg/security/model/authentication.go
@@ -186,7 +186,8 @@ func appendURIPrefixToTrustDomain(trustDomainAliases []string) []string {
 
 // ApplyToCommonTLSContext completes the commonTlsContext
 func ApplyToCommonTLSContext(tlsContext *tls.CommonTlsContext, proxy *model.Proxy,
-	subjectAltNames []string, trustDomainAliases []string, validateClient bool) {
+	subjectAltNames []string, trustDomainAliases []string, validateClient bool,
+) {
 	// These are certs being mounted from within the pod. Rather than reading directly in Envoy,
 	// which does not support rotation, we will serve them over SDS by reading the files.
 	// We should check if these certs have values, if yes we should use them or otherwise fall back to defaults.
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller.go b/pilot/pkg/serviceregistry/kube/controller/controller.go
index 09da809ab4..ca2ca19000 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller.go
@@ -997,7 +997,8 @@ func (c *Controller) serviceInstancesFromWorkloadInstances(svc *model.Service, r
 }
 
 func serviceInstanceFromWorkloadInstance(svc *model.Service, servicePort *model.Port,
-	targetPort serviceTargetPort, wi *model.WorkloadInstance) *model.ServiceInstance {
+	targetPort serviceTargetPort, wi *model.WorkloadInstance,
+) *model.ServiceInstance {
 	// create an instance with endpoint whose service port name matches
 	istioEndpoint := *wi.Endpoint
 
@@ -1313,7 +1314,8 @@ func (c *Controller) getProxyServiceInstancesFromMetadata(proxy *model.Proxy) ([
 }
 
 func (c *Controller) getProxyServiceInstancesByPod(pod *v1.Pod,
-	service *v1.Service, proxy *model.Proxy) []*model.ServiceInstance {
+	service *v1.Service, proxy *model.Proxy,
+) []*model.ServiceInstance {
 	var out []*model.ServiceInstance
 
 	for _, svc := range c.servicesForNamespacedName(kube.NamespacedNameForK8sObject(service)) {
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller_test.go b/pilot/pkg/serviceregistry/kube/controller/controller_test.go
index 92122b5825..8d0a6906ea 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller_test.go
@@ -1642,7 +1642,6 @@ func TestInstancesByPort_WorkloadInstances(t *testing.T) {
 	}
 }
 
-//
 func TestExternalNameServiceInstances(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
@@ -1789,7 +1788,8 @@ func TestController_ExternalNameService(t *testing.T) {
 }
 
 func createEndpoints(t *testing.T, controller *FakeController, name, namespace string,
-	portNames, ips []string, refs []*coreV1.ObjectReference, labels map[string]string) {
+	portNames, ips []string, refs []*coreV1.ObjectReference, labels map[string]string,
+) {
 	if labels == nil {
 		labels = make(map[string]string)
 	}
@@ -1915,7 +1915,8 @@ func updateEndpoints(controller *FakeController, name, namespace string, portNam
 }
 
 func createServiceWithTargetPorts(controller *FakeController, name, namespace string, annotations map[string]string,
-	svcPorts []coreV1.ServicePort, selector map[string]string, t *testing.T) {
+	svcPorts []coreV1.ServicePort, selector map[string]string, t *testing.T,
+) {
 	service := &coreV1.Service{
 		ObjectMeta: metaV1.ObjectMeta{
 			Name:        name,
@@ -1937,7 +1938,8 @@ func createServiceWithTargetPorts(controller *FakeController, name, namespace st
 }
 
 func createService(controller *FakeController, name, namespace string, annotations map[string]string,
-	ports []int32, selector map[string]string, t *testing.T) {
+	ports []int32, selector map[string]string, t *testing.T,
+) {
 	svcPorts := make([]coreV1.ServicePort, 0)
 	for _, p := range ports {
 		svcPorts = append(svcPorts, coreV1.ServicePort{
@@ -1983,7 +1985,8 @@ func updateService(controller *FakeController, svc *coreV1.Service, t *testing.T
 }
 
 func createServiceWithoutClusterIP(controller *FakeController, name, namespace string, annotations map[string]string,
-	ports []int32, selector map[string]string, t *testing.T) {
+	ports []int32, selector map[string]string, t *testing.T,
+) {
 	svcPorts := make([]coreV1.ServicePort, 0)
 	for _, p := range ports {
 		svcPorts = append(svcPorts, coreV1.ServicePort{
@@ -2014,7 +2017,8 @@ func createServiceWithoutClusterIP(controller *FakeController, name, namespace s
 
 // nolint: unparam
 func createExternalNameService(controller *FakeController, name, namespace string,
-	ports []int32, externalName string, t *testing.T, xdsEvents <-chan FakeXdsEvent) *coreV1.Service {
+	ports []int32, externalName string, t *testing.T, xdsEvents <-chan FakeXdsEvent,
+) *coreV1.Service {
 	defer func() {
 		<-xdsEvents
 	}()
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpoint_builder.go b/pilot/pkg/serviceregistry/kube/controller/endpoint_builder.go
index f0f203cc27..8613d142a8 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpoint_builder.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpoint_builder.go
@@ -106,7 +106,8 @@ func (b *EndpointBuilder) buildIstioEndpoint(
 	endpointAddress string,
 	endpointPort int32,
 	svcPortName string,
-	discoverabilityPolicy model.EndpointDiscoverabilityPolicy) *model.IstioEndpoint {
+	discoverabilityPolicy model.EndpointDiscoverabilityPolicy,
+) *model.IstioEndpoint {
 	if b == nil {
 		return nil
 	}
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpoints.go b/pilot/pkg/serviceregistry/kube/controller/endpoints.go
index 9a56d436b8..0a707f0141 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpoints.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpoints.go
@@ -184,7 +184,8 @@ func (e *endpointsController) buildIstioEndpoints(endpoint interface{}, host hos
 
 func (e *endpointsController) buildServiceInstances(ep *v1.Endpoints, ss v1.EndpointSubset, endpoints []v1.EndpointAddress,
 	svc *model.Service, discoverabilityPolicy model.EndpointDiscoverabilityPolicy, lbls labels.Instance,
-	svcPort *model.Port, health model.HealthStatus) []*model.ServiceInstance {
+	svcPort *model.Port, health model.HealthStatus,
+) []*model.ServiceInstance {
 	var out []*model.ServiceInstance
 	for _, ea := range endpoints {
 		var podLabels labels.Instance
@@ -220,7 +221,8 @@ func (e *endpointsController) buildServiceInstances(ep *v1.Endpoints, ss v1.Endp
 }
 
 func (e *endpointsController) buildIstioEndpointFromAddress(ep *v1.Endpoints, ss v1.EndpointSubset, endpoints []v1.EndpointAddress,
-	host host.Name, discoverabilityPolicy model.EndpointDiscoverabilityPolicy, health model.HealthStatus) []*model.IstioEndpoint {
+	host host.Name, discoverabilityPolicy model.EndpointDiscoverabilityPolicy, health model.HealthStatus,
+) []*model.IstioEndpoint {
 	var istioEndpoints []*model.IstioEndpoint
 	for _, ea := range endpoints {
 		pod, expectedPod := getPod(e.c, ea.IP, &metav1.ObjectMeta{Name: ep.Name, Namespace: ep.Namespace}, ea.TargetRef, host)
diff --git a/pilot/pkg/serviceregistry/kube/controller/multicluster.go b/pilot/pkg/serviceregistry/kube/controller/multicluster.go
index af8d2c9f35..5e74f0db5a 100644
--- a/pilot/pkg/serviceregistry/kube/controller/multicluster.go
+++ b/pilot/pkg/serviceregistry/kube/controller/multicluster.go
@@ -89,7 +89,8 @@ func NewMulticluster(
 	revision string,
 	startNsController bool,
 	clusterLocal model.ClusterLocalProvider,
-	s server.Instance) *Multicluster {
+	s server.Instance,
+) *Multicluster {
 	remoteKubeController := make(map[cluster.ID]*kubeController)
 	mc := &Multicluster{
 		serverID:               serverID,
diff --git a/pilot/pkg/serviceregistry/serviceentry/controller.go b/pilot/pkg/serviceregistry/serviceentry/controller.go
index 1c899093c7..e13e655d7f 100644
--- a/pilot/pkg/serviceregistry/serviceentry/controller.go
+++ b/pilot/pkg/serviceregistry/serviceentry/controller.go
@@ -123,7 +123,8 @@ func WithNetworkIDCb(cb func(endpointIP string, labels labels.Instance) network.
 
 // NewController creates a new ServiceEntry discovery service.
 func NewController(configController model.ConfigStoreController, store model.ConfigStore, xdsUpdater model.XDSUpdater,
-	options ...Option) *Controller {
+	options ...Option,
+) *Controller {
 	s := newController(store, xdsUpdater, options...)
 	if configController != nil {
 		configController.RegisterEventHandler(gvk.ServiceEntry, s.serviceEntryHandler)
@@ -135,7 +136,8 @@ func NewController(configController model.ConfigStoreController, store model.Con
 
 // NewWorkloadEntryController creates a new WorkloadEntry discovery service.
 func NewWorkloadEntryController(configController model.ConfigStoreController, store model.ConfigStore, xdsUpdater model.XDSUpdater,
-	options ...Option) *Controller {
+	options ...Option,
+) *Controller {
 	s := newController(store, xdsUpdater, options...)
 	// Disable service entry processing for workload entry controller.
 	s.workloadEntryController = true
diff --git a/pilot/pkg/serviceregistry/serviceentry/conversion.go b/pilot/pkg/serviceregistry/serviceentry/conversion.go
index 1bdbc9aeb8..e4b94a99d1 100644
--- a/pilot/pkg/serviceregistry/serviceentry/conversion.go
+++ b/pilot/pkg/serviceregistry/serviceentry/conversion.go
@@ -207,7 +207,8 @@ func convertServices(cfg config.Config) []*model.Service {
 
 func buildServices(hostAddresses []*HostAddress, name, namespace string, ports model.PortList, location networking.ServiceEntry_Location,
 	resolution model.Resolution, exportTo map[visibility.Instance]bool, selectors map[string]string, saccounts []string,
-	ctime time.Time, labels map[string]string) []*model.Service {
+	ctime time.Time, labels map[string]string,
+) []*model.Service {
 	out := make([]*model.Service, 0, len(hostAddresses))
 	lbls := labels
 	if features.CanonicalServiceForMeshExternalServiceEntry && location == networking.ServiceEntry_MESH_EXTERNAL {
@@ -250,7 +251,8 @@ func ensureCanonicalServiceLabels(name string, srcLabels map[string]string) map[
 }
 
 func (s *Controller) convertEndpoint(service *model.Service, servicePort *networking.Port,
-	wle *networking.WorkloadEntry, configKey *configKey, clusterID cluster.ID) *model.ServiceInstance {
+	wle *networking.WorkloadEntry, configKey *configKey, clusterID cluster.ID,
+) *model.ServiceInstance {
 	var instancePort uint32
 	addr := wle.GetAddress()
 	// priority level: unixAddress > we.ports > se.port.targetPort > se.port.number
@@ -300,7 +302,8 @@ func (s *Controller) convertEndpoint(service *model.Service, servicePort *networ
 // convertWorkloadEntryToServiceInstances translates a WorkloadEntry into ServiceInstances. This logic is largely the
 // same as the ServiceEntry convertServiceEntryToInstances.
 func (s *Controller) convertWorkloadEntryToServiceInstances(wle *networking.WorkloadEntry, services []*model.Service,
-	se *networking.ServiceEntry, configKey *configKey, clusterID cluster.ID) []*model.ServiceInstance {
+	se *networking.ServiceEntry, configKey *configKey, clusterID cluster.ID,
+) []*model.ServiceInstance {
 	out := make([]*model.ServiceInstance, 0)
 	for _, service := range services {
 		for _, port := range se.Ports {
@@ -369,7 +372,8 @@ func getTLSModeFromWorkloadEntry(wle *networking.WorkloadEntry) string {
 // The workload instance has pointer to the service and its service port.
 // We need to create our own but we can retain the endpoint already created.
 func convertWorkloadInstanceToServiceInstance(workloadInstance *model.WorkloadInstance, serviceEntryServices []*model.Service,
-	serviceEntry *networking.ServiceEntry) []*model.ServiceInstance {
+	serviceEntry *networking.ServiceEntry,
+) []*model.ServiceInstance {
 	out := make([]*model.ServiceInstance, 0)
 	for _, service := range serviceEntryServices {
 		for _, serviceEntryPort := range serviceEntry.Ports {
diff --git a/pilot/pkg/serviceregistry/serviceentry/conversion_test.go b/pilot/pkg/serviceregistry/serviceentry/conversion_test.go
index d12a4faef6..4c79539747 100644
--- a/pilot/pkg/serviceregistry/serviceentry/conversion_test.go
+++ b/pilot/pkg/serviceregistry/serviceentry/conversion_test.go
@@ -476,7 +476,8 @@ func convertPortNameToProtocol(name string) protocol.Instance {
 }
 
 func makeService(hostname host.Name, configNamespace, address string, ports map[string]int,
-	external bool, resolution model.Resolution, serviceAccounts ...string) *model.Service {
+	external bool, resolution model.Resolution, serviceAccounts ...string,
+) *model.Service {
 	svc := &model.Service{
 		CreationTime:    GlobalTime,
 		Hostname:        hostname,
@@ -526,7 +527,8 @@ func makeService(hostname host.Name, configNamespace, address string, ports map[
 
 // nolint: unparam
 func makeInstanceWithServiceAccount(cfg *config.Config, address string, port int,
-	svcPort *networking.Port, svcLabels map[string]string, serviceAccount string) *model.ServiceInstance {
+	svcPort *networking.Port, svcLabels map[string]string, serviceAccount string,
+) *model.ServiceInstance {
 	i := makeInstance(cfg, address, port, svcPort, svcLabels, MTLSUnlabelled)
 	i.Endpoint.ServiceAccount = spiffe.MustGenSpiffeURI(i.Service.Attributes.Namespace, serviceAccount)
 	return i
@@ -534,7 +536,8 @@ func makeInstanceWithServiceAccount(cfg *config.Config, address string, port int
 
 // nolint: unparam
 func makeInstance(cfg *config.Config, address string, port int,
-	svcPort *networking.Port, svcLabels map[string]string, mtlsMode MTLSMode) *model.ServiceInstance {
+	svcPort *networking.Port, svcLabels map[string]string, mtlsMode MTLSMode,
+) *model.ServiceInstance {
 	services := convertServices(*cfg)
 	svc := services[0] // default
 	for _, s := range services {
diff --git a/pilot/pkg/serviceregistry/serviceregistry_test.go b/pilot/pkg/serviceregistry/serviceregistry_test.go
index f246bea094..7233733911 100644
--- a/pilot/pkg/serviceregistry/serviceregistry_test.go
+++ b/pilot/pkg/serviceregistry/serviceregistry_test.go
@@ -59,7 +59,8 @@ func setupTest(t *testing.T) (
 	*serviceentry.Controller,
 	model.ConfigStoreController,
 	kubernetes.Interface,
-	*xds.FakeXdsUpdater) {
+	*xds.FakeXdsUpdater,
+) {
 	t.Helper()
 	client := kubeclient.NewFakeClient()
 
@@ -1330,7 +1331,8 @@ func createEndpointSlice(t *testing.T, c kubernetes.Interface, name, serviceName
 
 // nolint: unparam
 func createEndpointSliceWithType(t *testing.T, c kubernetes.Interface, name, serviceName, namespace string,
-	ports []v1.EndpointPort, ips []string, addrType discovery.AddressType) {
+	ports []v1.EndpointPort, ips []string, addrType discovery.AddressType,
+) {
 	esps := make([]discovery.EndpointPort, 0)
 	for _, name := range ports {
 		n := name // Create a stable reference to take the pointer from
diff --git a/pilot/pkg/simulation/traffic.go b/pilot/pkg/simulation/traffic.go
index e62a290db6..f8ac8703cc 100644
--- a/pilot/pkg/simulation/traffic.go
+++ b/pilot/pkg/simulation/traffic.go
@@ -222,7 +222,7 @@ func (r Result) Matches(t *testing.T, want Result) {
 		t.Logf("Diff: %+v", diff)
 		t.Logf("Full Diff: %+v", cmp.Diff(want, r, cmpopts.IgnoreUnexported(Result{}), cmpopts.EquateErrors()))
 	} else if want.Skip != "" {
-		t.Skip(fmt.Sprintf("Known bug: %v", r.Skip))
+		t.Skipf("Known bug: %v", r.Skip)
 	}
 }
 
@@ -500,7 +500,8 @@ func (sim *Simulation) matchVirtualHost(rc *route.RouteConfiguration, host strin
 // not match. This means an empty match (`{}`) may not match if another chain
 // matches one criteria but not another.
 func (sim *Simulation) matchFilterChain(chains []*listener.FilterChain, defaultChain *listener.FilterChain,
-	input Call, hasTLSInspector bool) (*listener.FilterChain, error) {
+	input Call, hasTLSInspector bool,
+) (*listener.FilterChain, error) {
 	chains = filter("DestinationPort", chains, func(fc *listener.FilterChainMatch) bool {
 		return fc.GetDestinationPort() == nil
 	}, func(fc *listener.FilterChainMatch) bool {
@@ -575,7 +576,8 @@ func (sim *Simulation) matchFilterChain(chains []*listener.FilterChain, defaultC
 
 func filter(desc string, chains []*listener.FilterChain,
 	empty func(fc *listener.FilterChainMatch) bool,
-	match func(fc *listener.FilterChainMatch) bool) []*listener.FilterChain {
+	match func(fc *listener.FilterChainMatch) bool,
+) []*listener.FilterChain {
 	res := []*listener.FilterChain{}
 	anySet := false
 	for _, c := range chains {
diff --git a/pilot/pkg/xds/cds.go b/pilot/pkg/xds/cds.go
index 8d7f875f6a..796f35c8b6 100644
--- a/pilot/pkg/xds/cds.go
+++ b/pilot/pkg/xds/cds.go
@@ -81,7 +81,8 @@ func (c CdsGenerator) Generate(proxy *model.Proxy, w *model.WatchedResource, req
 
 // GenerateDeltas for CDS currently only builds deltas when services change. todo implement changes for DestinationRule, etc
 func (c CdsGenerator) GenerateDeltas(proxy *model.Proxy, req *model.PushRequest,
-	w *model.WatchedResource) (model.Resources, model.DeletedResources, model.XdsLogDetails, bool, error) {
+	w *model.WatchedResource,
+) (model.Resources, model.DeletedResources, model.XdsLogDetails, bool, error) {
 	if !cdsNeedsPush(req, proxy) {
 		return nil, nil, model.DefaultXdsLogDetails, false, nil
 	}
diff --git a/pilot/pkg/xds/debug.go b/pilot/pkg/xds/debug.go
index acf5f0d47d..09a098d554 100644
--- a/pilot/pkg/xds/debug.go
+++ b/pilot/pkg/xds/debug.go
@@ -135,7 +135,8 @@ type SyncedVersions struct {
 
 // InitDebug initializes the debug handlers and adds a debug in-memory registry.
 func (s *DiscoveryServer) InitDebug(mux *http.ServeMux, sctl *aggregate.Controller, enableProfiling bool,
-	fetchWebhook func() map[string]string) {
+	fetchWebhook func() map[string]string,
+) {
 	// For debugging and load testing v2 we add an memory registry.
 	s.MemRegistry = memory.NewServiceDiscovery()
 	s.MemRegistry.EDSUpdater = s
@@ -213,7 +214,8 @@ func (s *DiscoveryServer) AddDebugHandlers(mux, internalMux *http.ServeMux, enab
 }
 
 func (s *DiscoveryServer) addDebugHandler(mux *http.ServeMux, internalMux *http.ServeMux,
-	path string, help string, handler func(http.ResponseWriter, *http.Request)) {
+	path string, help string, handler func(http.ResponseWriter, *http.Request),
+) {
 	s.debugHandlers[path] = help
 	// Add handler without auth. This mux is never exposed on an HTTP server and only used internally
 	if internalMux != nil {
diff --git a/pilot/pkg/xds/delta.go b/pilot/pkg/xds/delta.go
index 9ee65f0e33..93ebe165ef 100644
--- a/pilot/pkg/xds/delta.go
+++ b/pilot/pkg/xds/delta.go
@@ -383,7 +383,8 @@ func (s *DiscoveryServer) shouldRespondDelta(con *Connection, request *discovery
 // Push an Delta XDS resource for the given connection. Configuration will be generated
 // based on the passed in generator.
 func (s *DiscoveryServer) pushDeltaXds(con *Connection,
-	w *model.WatchedResource, req *model.PushRequest) error {
+	w *model.WatchedResource, req *model.PushRequest,
+) error {
 	if w == nil {
 		return nil
 	}
diff --git a/pilot/pkg/xds/deltaadstest.go b/pilot/pkg/xds/deltaadstest.go
index 9703f159ea..1949b6f3b4 100644
--- a/pilot/pkg/xds/deltaadstest.go
+++ b/pilot/pkg/xds/deltaadstest.go
@@ -39,7 +39,8 @@ func NewDeltaAdsTest(t test.Failer, conn *grpc.ClientConn) *DeltaAdsTest {
 }
 
 func NewDeltaXdsTest(t test.Failer, conn *grpc.ClientConn,
-	getClient func(conn *grpc.ClientConn) (DeltaDiscoveryClient, error)) *DeltaAdsTest {
+	getClient func(conn *grpc.ClientConn) (DeltaDiscoveryClient, error),
+) *DeltaAdsTest {
 	ctx, cancel := context.WithCancel(context.Background())
 
 	cl, err := getClient(conn)
diff --git a/pilot/pkg/xds/ecds.go b/pilot/pkg/xds/ecds.go
index 4a2dd5a622..423a689f11 100644
--- a/pilot/pkg/xds/ecds.go
+++ b/pilot/pkg/xds/ecds.go
@@ -119,7 +119,8 @@ func (e *EcdsGenerator) Generate(proxy *model.Proxy, w *model.WatchedResource, r
 }
 
 func (e *EcdsGenerator) GeneratePullSecrets(proxy *model.Proxy, updatedSecrets map[model.ConfigKey]struct{}, secretResources []SecretResource,
-	secretController credscontroller.Controller, req *model.PushRequest) map[string][]byte {
+	secretController credscontroller.Controller, req *model.PushRequest,
+) map[string][]byte {
 	if proxy.VerifiedIdentity == nil {
 		log.Warnf("proxy %s is not authorized to receive secret. Ensure you are connecting over TLS port and are authenticated.", proxy.ID)
 		return nil
diff --git a/pilot/pkg/xds/eds.go b/pilot/pkg/xds/eds.go
index 7bd07de98a..acd25ba856 100644
--- a/pilot/pkg/xds/eds.go
+++ b/pilot/pkg/xds/eds.go
@@ -102,7 +102,8 @@ func (s *DiscoveryServer) SvcUpdate(shard model.ShardKey, hostname string, names
 // the hostname-keyed map. And it avoids the conversion from Endpoint to ServiceEntry to envoy
 // on each step: instead the conversion happens once, when an endpoint is first discovered.
 func (s *DiscoveryServer) EDSUpdate(shard model.ShardKey, serviceName string, namespace string,
-	istioEndpoints []*model.IstioEndpoint) {
+	istioEndpoints []*model.IstioEndpoint,
+) {
 	inboundEDSUpdates.Increment()
 	// Update the endpoint shards
 	pushType := s.edsCacheUpdate(shard, serviceName, namespace, istioEndpoints)
@@ -128,7 +129,8 @@ func (s *DiscoveryServer) EDSUpdate(shard model.ShardKey, serviceName string, na
 //
 // Note: the difference with `EDSUpdate` is that it only update the cache rather than requesting a push
 func (s *DiscoveryServer) EDSCacheUpdate(shard model.ShardKey, serviceName string, namespace string,
-	istioEndpoints []*model.IstioEndpoint) {
+	istioEndpoints []*model.IstioEndpoint,
+) {
 	inboundEDSUpdates.Increment()
 	// Update the endpoint shards
 	s.edsCacheUpdate(shard, serviceName, namespace, istioEndpoints)
@@ -138,7 +140,8 @@ func (s *DiscoveryServer) EDSCacheUpdate(shard model.ShardKey, serviceName strin
 // It also tracks the changes to ServiceAccounts. It returns whether endpoints need to be pushed and
 // it also returns if they need to be pushed whether a full push is needed or incremental push is sufficient.
 func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string, namespace string,
-	istioEndpoints []*model.IstioEndpoint) PushType {
+	istioEndpoints []*model.IstioEndpoint,
+) PushType {
 	if len(istioEndpoints) == 0 {
 		// Should delete the service EndpointShards when endpoints become zero to prevent memory leak,
 		// but we should not delete the keys from EndpointIndex map - that will trigger
@@ -390,7 +393,8 @@ func (eds *EdsGenerator) Generate(proxy *model.Proxy, w *model.WatchedResource,
 func getOutlierDetectionAndLoadBalancerSettings(
 	destinationRule *networkingapi.DestinationRule,
 	portNumber int,
-	subsetName string) (bool, *networkingapi.LoadBalancerSettings) {
+	subsetName string,
+) (bool, *networkingapi.LoadBalancerSettings) {
 	if destinationRule == nil {
 		return false, nil
 	}
@@ -440,7 +444,8 @@ func buildEmptyClusterLoadAssignment(clusterName string) *endpoint.ClusterLoadAs
 }
 
 func (eds *EdsGenerator) GenerateDeltas(proxy *model.Proxy, req *model.PushRequest,
-	w *model.WatchedResource) (model.Resources, model.DeletedResources, model.XdsLogDetails, bool, error) {
+	w *model.WatchedResource,
+) (model.Resources, model.DeletedResources, model.XdsLogDetails, bool, error) {
 	if !edsNeedsPush(req.ConfigsUpdated) {
 		return nil, nil, model.DefaultXdsLogDetails, false, nil
 	}
@@ -476,7 +481,8 @@ func onlyEndpointsChanged(req *model.PushRequest) bool {
 
 func (eds *EdsGenerator) buildEndpoints(proxy *model.Proxy,
 	req *model.PushRequest,
-	w *model.WatchedResource) (model.Resources, model.XdsLogDetails) {
+	w *model.WatchedResource,
+) (model.Resources, model.XdsLogDetails) {
 	var edsUpdatedServices map[string]struct{}
 	// canSendPartialFullPushes determines if we can send a partial push (ie a subset of known CLAs).
 	// This is safe when only Services has changed, as this implies that only the CLAs for the
@@ -532,7 +538,8 @@ func (eds *EdsGenerator) buildEndpoints(proxy *model.Proxy,
 // TODO(@hzxuzhonghu): merge with buildEndpoints
 func (eds *EdsGenerator) buildDeltaEndpoints(proxy *model.Proxy,
 	req *model.PushRequest,
-	w *model.WatchedResource) (model.Resources, []string, model.XdsLogDetails) {
+	w *model.WatchedResource,
+) (model.Resources, []string, model.XdsLogDetails) {
 	edsUpdatedServices := model.ConfigNamesOfKind(req.ConfigsUpdated, gvk.ServiceEntry)
 	var resources model.Resources
 	var removed []string
diff --git a/pilot/pkg/xds/eds_test.go b/pilot/pkg/xds/eds_test.go
index 4d6af0641b..daace6248c 100644
--- a/pilot/pkg/xds/eds_test.go
+++ b/pilot/pkg/xds/eds_test.go
@@ -4,7 +4,7 @@
 // you may not use this file except in compliance with the License.
 // You may obtain a copy of the License at
 //
-//     http://www.apache.org/licenses/LICENSE-2.0
+//	http://www.apache.org/licenses/LICENSE-2.0
 //
 // Unless required by applicable law or agreed to in writing, software
 // distributed under the License is distributed on an "AS IS" BASIS,
@@ -1017,7 +1017,8 @@ func edsUpdateInc(s *xds.FakeDiscoveryServer, adsc *adsc.ADSC, t *testing.T) {
 // This test includes a 'bad client' regression test, which fails to read on the
 // stream.
 func multipleRequest(s *xds.FakeDiscoveryServer, inc bool, nclients,
-	nPushes int, to time.Duration, _ map[string]string, t *testing.T) {
+	nPushes int, to time.Duration, _ map[string]string, t *testing.T,
+) {
 	wgConnect := &sync.WaitGroup{}
 	wg := &sync.WaitGroup{}
 	errChan := make(chan error, nclients)
diff --git a/pilot/pkg/xds/sds.go b/pilot/pkg/xds/sds.go
index 9aa461436d..58f3c4a16f 100644
--- a/pilot/pkg/xds/sds.go
+++ b/pilot/pkg/xds/sds.go
@@ -401,7 +401,8 @@ type SecretGen struct {
 var _ model.XdsResourceGenerator = &SecretGen{}
 
 func NewSecretGen(sc credscontroller.MulticlusterController, cache model.XdsCache, configCluster cluster.ID,
-	meshConfig *mesh.MeshConfig) *SecretGen {
+	meshConfig *mesh.MeshConfig,
+) *SecretGen {
 	// TODO: Currently we only have a single credentials controller (Kubernetes). In the future, we will need a mapping
 	// of resource type to secret controller (ie kubernetes:// -> KubernetesController, vault:// -> VaultController)
 	return &SecretGen{
diff --git a/pilot/test/xdstest/grpc.go b/pilot/test/xdstest/grpc.go
index 2f87b0cf57..195e17fd56 100644
--- a/pilot/test/xdstest/grpc.go
+++ b/pilot/test/xdstest/grpc.go
@@ -54,7 +54,8 @@ func (w *slowClientStream) SendMsg(m interface{}) error {
 // SlowClientInterceptor is an interceptor that allows injecting delays on Send and Recv
 func SlowClientInterceptor(recv, send time.Duration) grpc.StreamClientInterceptor {
 	return func(ctx context.Context, desc *grpc.StreamDesc, cc *grpc.ClientConn,
-		method string, streamer grpc.Streamer, opts ...grpc.CallOption) (grpc.ClientStream, error) {
+		method string, streamer grpc.Streamer, opts ...grpc.CallOption,
+	) (grpc.ClientStream, error) {
 		clientStream, err := streamer(ctx, desc, cc, method, opts...)
 		return &slowClientStream{clientStream, recv, send}, err
 	}
diff --git a/pkg/config/analysis/analyzers/gateway/conflictinggateway.go b/pkg/config/analysis/analyzers/gateway/conflictinggateway.go
index 9a3a4e02da..78f70fa13a 100644
--- a/pkg/config/analysis/analyzers/gateway/conflictinggateway.go
+++ b/pkg/config/analysis/analyzers/gateway/conflictinggateway.go
@@ -58,7 +58,8 @@ func (s *ConflictingGatewayAnalyzer) Analyze(c analysis.Context) {
 }
 
 func (*ConflictingGatewayAnalyzer) analyzeGateway(r *resource.Instance, c analysis.Context,
-	gwCMap map[string]map[string][]string) {
+	gwCMap map[string]map[string][]string,
+) {
 	gw := r.Message.(*v1alpha3.Gateway)
 	gwName := r.Metadata.FullName.String()
 	// For pods selected by gw.Selector, find Services that select them and remember those ports
diff --git a/pkg/config/analysis/analyzers/virtualservice/destinationhosts.go b/pkg/config/analysis/analyzers/virtualservice/destinationhosts.go
index 7e0f0bd538..62237a3949 100644
--- a/pkg/config/analysis/analyzers/virtualservice/destinationhosts.go
+++ b/pkg/config/analysis/analyzers/virtualservice/destinationhosts.go
@@ -118,7 +118,8 @@ func initVirtualServiceDestinations(ctx analysis.Context) map[resource.FullName]
 }
 
 func (a *DestinationHostAnalyzer) analyzeVirtualService(r *resource.Instance, ctx analysis.Context,
-	serviceEntryHosts map[util.ScopedFqdn]*v1alpha3.ServiceEntry) {
+	serviceEntryHosts map[util.ScopedFqdn]*v1alpha3.ServiceEntry,
+) {
 	vs := r.Message.(*v1alpha3.VirtualService)
 
 	for _, d := range getRouteDestinations(vs) {
diff --git a/pkg/config/analysis/analyzers/virtualservice/destinationrules.go b/pkg/config/analysis/analyzers/virtualservice/destinationrules.go
index ede519f100..d5facf0b7b 100644
--- a/pkg/config/analysis/analyzers/virtualservice/destinationrules.go
+++ b/pkg/config/analysis/analyzers/virtualservice/destinationrules.go
@@ -55,7 +55,8 @@ func (d *DestinationRuleAnalyzer) Analyze(ctx analysis.Context) {
 }
 
 func (d *DestinationRuleAnalyzer) analyzeVirtualService(r *resource.Instance, ctx analysis.Context,
-	destHostsAndSubsets map[hostAndSubset]bool) {
+	destHostsAndSubsets map[hostAndSubset]bool,
+) {
 	vs := r.Message.(*v1alpha3.VirtualService)
 	ns := r.Metadata.FullName.Namespace
 
@@ -91,7 +92,8 @@ func (d *DestinationRuleAnalyzer) analyzeVirtualService(r *resource.Instance, ct
 }
 
 func (d *DestinationRuleAnalyzer) checkDestinationSubset(vsNamespace resource.Namespace, destination *v1alpha3.Destination,
-	destHostsAndSubsets map[hostAndSubset]bool) bool {
+	destHostsAndSubsets map[hostAndSubset]bool,
+) bool {
 	name := util.GetResourceNameFromHost(vsNamespace, destination.GetHost())
 	subset := destination.GetSubset()
 
diff --git a/pkg/config/analysis/incluster/controller.go b/pkg/config/analysis/incluster/controller.go
index ff8fabfd46..bbf944c155 100644
--- a/pkg/config/analysis/incluster/controller.go
+++ b/pkg/config/analysis/incluster/controller.go
@@ -45,7 +45,8 @@ type Controller struct {
 }
 
 func NewController(stop <-chan struct{}, rwConfigStore model.ConfigStoreController,
-	kubeClient kube.Client, namespace string, statusManager *status.Manager, domainSuffix string) (*Controller, error) {
+	kubeClient kube.Client, namespace string, statusManager *status.Manager, domainSuffix string,
+) (*Controller, error) {
 	ia := local.NewIstiodAnalyzer(analyzers.AllCombined(),
 		"", resource.Namespace(namespace), func(name collection.Name) {}, true)
 	ia.AddSource(rwConfigStore)
diff --git a/pkg/config/analysis/local/istiod_analyze.go b/pkg/config/analysis/local/istiod_analyze.go
index 92bd4dcb75..b4e02aaefd 100644
--- a/pkg/config/analysis/local/istiod_analyze.go
+++ b/pkg/config/analysis/local/istiod_analyze.go
@@ -85,7 +85,8 @@ type IstiodAnalyzer struct {
 
 // NewSourceAnalyzer is a drop-in replacement for the galley function, adapting to istiod analyzer.
 func NewSourceAnalyzer(analyzer *analysis.CombinedAnalyzer, namespace, istioNamespace resource.Namespace,
-	cr CollectionReporterFn, serviceDiscovery bool, _ time.Duration) *IstiodAnalyzer {
+	cr CollectionReporterFn, serviceDiscovery bool, _ time.Duration,
+) *IstiodAnalyzer {
 	return NewIstiodAnalyzer(analyzer, namespace, istioNamespace, cr, serviceDiscovery)
 }
 
@@ -93,7 +94,8 @@ func NewSourceAnalyzer(analyzer *analysis.CombinedAnalyzer, namespace, istioName
 // methods to add sources in ascending precedence order,
 // then execute Analyze to perform the analysis
 func NewIstiodAnalyzer(analyzer *analysis.CombinedAnalyzer, namespace,
-	istioNamespace resource.Namespace, cr CollectionReporterFn, serviceDiscovery bool) *IstiodAnalyzer {
+	istioNamespace resource.Namespace, cr CollectionReporterFn, serviceDiscovery bool,
+) *IstiodAnalyzer {
 	// collectionReporter hook function defaults to no-op
 	if cr == nil {
 		cr = func(collection.Name) {}
diff --git a/pkg/config/validation/validation.go b/pkg/config/validation/validation.go
index 8ca128240b..e1456349a9 100644
--- a/pkg/config/validation/validation.go
+++ b/pkg/config/validation/validation.go
@@ -1250,7 +1250,8 @@ func validateSidecarOutboundTrafficPolicy(tp *networking.OutboundTrafficPolicy)
 }
 
 func validateSidecarEgressPortBindAndCaptureMode(port *networking.Port, bind string,
-	captureMode networking.CaptureMode) (errs error) {
+	captureMode networking.CaptureMode,
+) (errs error) {
 	// Port name is optional. Validate if exists.
 	if len(port.Name) > 0 {
 		errs = appendErrors(errs, ValidatePortName(port.Name))
@@ -2231,7 +2232,8 @@ func assignExactOrPrefix(exact, prefix string) string {
 // based on particular HTTPMatchRequest, according to comments on https://github.com/istio/istio/pull/32701
 // only support Match's port, method, authority, headers, query params and nonheaders for now.
 func genMatchHTTPRoutes(route *networking.HTTPRoute, match *networking.HTTPMatchRequest,
-	rulen, matchn int) (matchHTTPRoutes *OverlappingMatchValidationForHTTPRoute) {
+	rulen, matchn int,
+) (matchHTTPRoutes *OverlappingMatchValidationForHTTPRoute) {
 	// skip current match if no match field for current route
 	if match == nil {
 		return nil
@@ -2381,7 +2383,8 @@ func coveredValidation(vA, vB *OverlappingMatchValidationForHTTPRoute) bool {
 }
 
 func analyzeUnreachableHTTPRules(routes []*networking.HTTPRoute,
-	reportUnreachable func(ruleno, reason string), reportIneffective func(ruleno, matchno, dupno string)) {
+	reportUnreachable func(ruleno, reason string), reportIneffective func(ruleno, matchno, dupno string),
+) {
 	matchesEncountered := make(map[string]int)
 	emptyMatchEncountered := -1
 	var matchHTTPRoutes []*OverlappingMatchValidationForHTTPRoute
@@ -2440,7 +2443,8 @@ func analyzeUnreachableHTTPRules(routes []*networking.HTTPRoute,
 
 // NOTE: This method identical to analyzeUnreachableHTTPRules.
 func analyzeUnreachableTCPRules(routes []*networking.TCPRoute,
-	reportUnreachable func(ruleno, reason string), reportIneffective func(ruleno, matchno, dupno string)) {
+	reportUnreachable func(ruleno, reason string), reportIneffective func(ruleno, matchno, dupno string),
+) {
 	matchesEncountered := make(map[string]int)
 	emptyMatchEncountered := -1
 	for rulen, route := range routes {
@@ -2473,7 +2477,8 @@ func analyzeUnreachableTCPRules(routes []*networking.TCPRoute,
 
 // NOTE: This method identical to analyzeUnreachableHTTPRules.
 func analyzeUnreachableTLSRules(routes []*networking.TLSRoute,
-	reportUnreachable func(ruleno, reason string), reportIneffective func(ruleno, matchno, dupno string)) {
+	reportUnreachable func(ruleno, reason string), reportIneffective func(ruleno, matchno, dupno string),
+) {
 	matchesEncountered := make(map[string]int)
 	emptyMatchEncountered := -1
 	for rulen, route := range routes {
diff --git a/pkg/dns/client/dns.go b/pkg/dns/client/dns.go
index 700a2b47e9..eaf7e0902f 100644
--- a/pkg/dns/client/dns.go
+++ b/pkg/dns/client/dns.go
@@ -197,7 +197,8 @@ func (h *LocalDNSServer) UpdateLookupTable(nt *dnsProto.NameTable) {
 // BuildAlternateHosts builds alternate hosts for Kubernetes services in the name table and
 // calls the passed in function with the built alternate hosts.
 func (h *LocalDNSServer) BuildAlternateHosts(nt *dnsProto.NameTable,
-	apply func(map[string]struct{}, []net.IP, []net.IP, []string)) {
+	apply func(map[string]struct{}, []net.IP, []net.IP, []string),
+) {
 	for hostname, ni := range nt.Table {
 		// Given a host
 		// if its a non-k8s host, store the host+. as the key with the pre-computed DNS RR records
diff --git a/pkg/envoy/agent.go b/pkg/envoy/agent.go
index 7847807715..675cc9f380 100644
--- a/pkg/envoy/agent.go
+++ b/pkg/envoy/agent.go
@@ -35,7 +35,8 @@
 
 // NewAgent creates a new proxy agent for the proxy start-up and clean-up functions.
 func NewAgent(proxy Proxy, terminationDrainDuration, minDrainDuration time.Duration, localhost string,
-	adminPort, statusPort, prometheusPort int, exitOnZeroActiveConnections bool) *Agent {
+	adminPort, statusPort, prometheusPort int, exitOnZeroActiveConnections bool,
+) *Agent {
 	knownIstioListeners := sets.New(
 		fmt.Sprintf("listener.0.0.0.0_%d.downstream_cx_active", statusPort),
 		fmt.Sprintf("listener.0.0.0.0_%d.downstream_cx_active", prometheusPort),
diff --git a/pkg/istio-agent/xds_proxy_delta.go b/pkg/istio-agent/xds_proxy_delta.go
index 16b83bee57..1e1ef22b88 100644
--- a/pkg/istio-agent/xds_proxy_delta.go
+++ b/pkg/istio-agent/xds_proxy_delta.go
@@ -293,12 +293,14 @@ func forwardDeltaToEnvoy(con *ProxyConnection, resp *discovery.DeltaDiscoveryRes
 }
 
 func sendUpstreamDelta(deltaUpstream discovery.AggregatedDiscoveryService_DeltaAggregatedResourcesClient,
-	req *discovery.DeltaDiscoveryRequest) error {
+	req *discovery.DeltaDiscoveryRequest,
+) error {
 	return istiogrpc.Send(deltaUpstream.Context(), func() error { return deltaUpstream.Send(req) })
 }
 
 func sendDownstreamDelta(deltaDownstream discovery.AggregatedDiscoveryService_DeltaAggregatedResourcesServer,
-	res *discovery.DeltaDiscoveryResponse) error {
+	res *discovery.DeltaDiscoveryResponse,
+) error {
 	return istiogrpc.Send(deltaDownstream.Context(), func() error { return deltaDownstream.Send(res) })
 }
 
diff --git a/pkg/kube/adapter.go b/pkg/kube/adapter.go
index f4ca7e2fed..62a9298be3 100644
--- a/pkg/kube/adapter.go
+++ b/pkg/kube/adapter.go
@@ -53,7 +53,6 @@ type AdmissionReview struct {
 
 // AdmissionRequest describes the admission.Attributes for the admission request.
 type AdmissionRequest struct {
-
 	// UID is an identifier for the individual request/response. It allows us to distinguish instances of requests which are
 	// otherwise identical (parallel requests, requests when earlier requests did not modify etc)
 	// The UID is meant to track the round trip (request/response) between the KAS and the WebHook, not the user request.
@@ -128,7 +127,6 @@ type AdmissionRequest struct {
 
 // AdmissionResponse describes an admission response.
 type AdmissionResponse struct {
-
 	// UID is an identifier for the individual request/response.
 	// This should be copied over from the corresponding AdmissionRequest.
 	UID types.UID `json:"uid"`
diff --git a/pkg/kube/client.go b/pkg/kube/client.go
index 51c1c48408..dabd18a893 100644
--- a/pkg/kube/client.go
+++ b/pkg/kube/client.go
@@ -937,7 +937,8 @@ func (c *client) ApplyYAMLFilesDryRun(namespace string, yamlFiles ...string) err
 }
 
 func (c *client) CreatePerRPCCredentials(_ context.Context, tokenNamespace, tokenServiceAccount string, audiences []string,
-	expirationSeconds int64) (credentials.PerRPCCredentials, error) {
+	expirationSeconds int64,
+) (credentials.PerRPCCredentials, error) {
 	return NewRPCCredentials(c, tokenNamespace, tokenServiceAccount, audiences, expirationSeconds, 60)
 }
 
diff --git a/pkg/kube/inject/inject.go b/pkg/kube/inject/inject.go
index e8605772fe..0a370ced98 100644
--- a/pkg/kube/inject/inject.go
+++ b/pkg/kube/inject/inject.go
@@ -539,7 +539,8 @@ func runTemplate(tmpl *template.Template, data SidecarTemplateData) (bytes.Buffe
 // kubernetes YAML file.
 // nolint: lll
 func IntoResourceFile(injector Injector, sidecarTemplate Templates,
-	valuesConfig ValuesConfig, revision string, meshconfig *meshconfig.MeshConfig, in io.Reader, out io.Writer, warningHandler func(string)) error {
+	valuesConfig ValuesConfig, revision string, meshconfig *meshconfig.MeshConfig, in io.Reader, out io.Writer, warningHandler func(string),
+) error {
 	reader := yamlDecoder.NewYAMLReader(bufio.NewReaderSize(in, 4096))
 	for {
 		raw, err := reader.Read()
@@ -600,7 +601,8 @@ func FromRawToObject(raw []byte) (runtime.Object, error) {
 // IntoObject convert the incoming resources into Injected resources
 // nolint: lll
 func IntoObject(injector Injector, sidecarTemplate Templates, valuesConfig ValuesConfig,
-	revision string, meshconfig *meshconfig.MeshConfig, in runtime.Object, warningHandler func(string)) (interface{}, error) {
+	revision string, meshconfig *meshconfig.MeshConfig, in runtime.Object, warningHandler func(string),
+) (interface{}, error) {
 	out := in.DeepCopyObject()
 
 	var deploymentMetadata metav1.ObjectMeta
diff --git a/pkg/kube/mock_client.go b/pkg/kube/mock_client.go
index e289e58021..b6e4571bd6 100644
--- a/pkg/kube/mock_client.go
+++ b/pkg/kube/mock_client.go
@@ -204,7 +204,8 @@ func (c MockClient) ApplyYAMLFilesDryRun(string, ...string) error {
 
 // CreatePerRPCCredentials -- when implemented -- mocks per-RPC credentials (bearer token)
 func (c MockClient) CreatePerRPCCredentials(ctx context.Context, tokenNamespace, tokenServiceAccount string, audiences []string,
-	expirationSeconds int64) (credentials.PerRPCCredentials, error) {
+	expirationSeconds int64,
+) (credentials.PerRPCCredentials, error) {
 	panic("not implemented by mock")
 }
 
diff --git a/pkg/kube/rpc_creds.go b/pkg/kube/rpc_creds.go
index 654f26ff39..3cbf1ce832 100644
--- a/pkg/kube/rpc_creds.go
+++ b/pkg/kube/rpc_creds.go
@@ -46,7 +46,8 @@ type tokenSupplier struct {
 
 // NewRPCCredentials creates a PerRPCCredentials capable of getting tokens from Istio and tracking their expiration
 func NewRPCCredentials(kubeClient Client, tokenNamespace, tokenSA string,
-	tokenAudiences []string, expirationSeconds, sunsetPeriodSeconds int64) (credentials.PerRPCCredentials, error) {
+	tokenAudiences []string, expirationSeconds, sunsetPeriodSeconds int64,
+) (credentials.PerRPCCredentials, error) {
 	tokenRequest, err := createServiceAccountToken(context.TODO(), kubeClient, tokenNamespace, tokenSA, tokenAudiences, expirationSeconds)
 	if err != nil {
 		return nil, err
@@ -102,7 +103,8 @@ func (its *tokenSupplier) RequireTransportSecurity() bool {
 }
 
 func createServiceAccountToken(ctx context.Context, client Client,
-	tokenNamespace, tokenServiceAccount string, audiences []string, expirationSeconds int64) (*authenticationv1.TokenRequest, error) {
+	tokenNamespace, tokenServiceAccount string, audiences []string, expirationSeconds int64,
+) (*authenticationv1.TokenRequest, error) {
 	return client.Kube().CoreV1().ServiceAccounts(tokenNamespace).CreateToken(ctx, tokenServiceAccount,
 		&authenticationv1.TokenRequest{
 			Spec: authenticationv1.TokenRequestSpec{
diff --git a/pkg/spiffe/spiffe.go b/pkg/spiffe/spiffe.go
index 13ba0a2a1d..de80e054de 100644
--- a/pkg/spiffe/spiffe.go
+++ b/pkg/spiffe/spiffe.go
@@ -128,9 +128,12 @@ func MustGenSpiffeURI(ns, serviceAccount string) string {
 // We ensure the returned list does not contain duplicates; the original input is always retained.
 // For example,
 // ExpandWithTrustDomains({"spiffe://td1/ns/def/sa/def"}, {"td1", "td2"}) returns
-//   {"spiffe://td1/ns/def/sa/def", "spiffe://td2/ns/def/sa/def"}.
+//
+//	{"spiffe://td1/ns/def/sa/def", "spiffe://td2/ns/def/sa/def"}.
+//
 // ExpandWithTrustDomains({"spiffe://td1/ns/def/sa/a", "spiffe://td1/ns/def/sa/b"}, {"td2"}) returns
-//   {"spiffe://td1/ns/def/sa/a", "spiffe://td2/ns/def/sa/a", "spiffe://td1/ns/def/sa/b", "spiffe://td2/ns/def/sa/b"}.
+//
+//	{"spiffe://td1/ns/def/sa/a", "spiffe://td2/ns/def/sa/a", "spiffe://td1/ns/def/sa/b", "spiffe://td2/ns/def/sa/b"}.
 func ExpandWithTrustDomains(spiffeIdentities, trustDomainAliases []string) map[string]struct{} {
 	out := map[string]struct{}{}
 	for _, id := range spiffeIdentities {
@@ -163,7 +166,8 @@ func GetTrustDomainFromURISAN(uriSan string) (string, error) {
 // The input endpointTuples should be in the format of:
 // "foo|URL1||bar|URL2||baz|URL3..."
 func RetrieveSpiffeBundleRootCertsFromStringInput(inputString string, extraTrustedCerts []*x509.Certificate) (
-	map[string][]*x509.Certificate, error) {
+	map[string][]*x509.Certificate, error,
+) {
 	spiffeLog.Infof("Processing SPIFFE bundle configuration: %v", inputString)
 	config := make(map[string]string)
 	tuples := strings.Split(inputString, "||")
@@ -190,7 +194,8 @@ func RetrieveSpiffeBundleRootCertsFromStringInput(inputString string, extraTrust
 // RetrieveSpiffeBundleRootCerts retrieves the trusted CA certificates from a list of SPIFFE bundle endpoints.
 // It can use the system cert pool and the supplied certificates to validate the endpoints.
 func RetrieveSpiffeBundleRootCerts(config map[string]string, caCertPool *x509.CertPool, retryTimeout time.Duration) (
-	map[string][]*x509.Certificate, error) {
+	map[string][]*x509.Certificate, error,
+) {
 	httpClient := &http.Client{
 		Timeout: time.Second * 10,
 	}
diff --git a/pkg/test/csrctrl/controllers/start_csrctrl.go b/pkg/test/csrctrl/controllers/start_csrctrl.go
index 62c46b2dc0..e9c92defcf 100644
--- a/pkg/test/csrctrl/controllers/start_csrctrl.go
+++ b/pkg/test/csrctrl/controllers/start_csrctrl.go
@@ -52,7 +52,8 @@ type SignerRootCert struct {
 }
 
 func RunCSRController(signerNames string, appendRootCert bool, config *rest.Config, c <-chan struct{},
-	certChan chan *SignerRootCert) {
+	certChan chan *SignerRootCert,
+) {
 	// Config Istio log
 	if err := log.Configure(loggingOptions); err != nil {
 		log.Infof("Unable to configure Istio log error: %v", err)
diff --git a/pkg/test/framework/components/cluster/clusters.go b/pkg/test/framework/components/cluster/clusters.go
index 74f52e0739..ced791ef51 100644
--- a/pkg/test/framework/components/cluster/clusters.go
+++ b/pkg/test/framework/components/cluster/clusters.go
@@ -156,7 +156,8 @@ func exclude(exclude ...Cluster) func(Cluster) bool {
 }
 
 func (c Clusters) filterClusters(included func(Cluster) bool,
-	excluded func(Cluster) bool) Clusters {
+	excluded func(Cluster) bool,
+) Clusters {
 	var out Clusters
 	for _, cc := range c {
 		if !excluded(cc) && included(cc) {
diff --git a/pkg/test/framework/components/istio/operator.go b/pkg/test/framework/components/istio/operator.go
index 4c67be4c34..5e79e63ea9 100644
--- a/pkg/test/framework/components/istio/operator.go
+++ b/pkg/test/framework/components/istio/operator.go
@@ -459,7 +459,8 @@ func initIOPFile(s *resource.Settings, cfg Config, iopFile string, valuesYaml st
 // The cluster is considered a "primary" cluster if it is also a "config cluster", in which case components
 // like ingress will be installed.
 func installControlPlaneCluster(s *resource.Settings, i *operatorComponent, cfg Config, c cluster.Cluster, iopFile string,
-	spec *opAPI.IstioOperatorSpec) error {
+	spec *opAPI.IstioOperatorSpec,
+) error {
 	scopes.Framework.Infof("setting up %s as control-plane cluster", c.Name())
 
 	if !c.IsConfig() {
@@ -616,7 +617,8 @@ type Filenamer interface {
 }
 
 func (i *operatorComponent) generateCommonInstallArgs(s *resource.Settings, cfg Config, c cluster.Cluster, defaultsIOPFile,
-	iopFile string) (*mesh.InstallArgs, error) {
+	iopFile string,
+) (*mesh.InstallArgs, error) {
 	kubeConfigFile, err := kubeConfigFileForCluster(c)
 	if err != nil {
 		return nil, err
@@ -738,7 +740,8 @@ func (i *operatorComponent) configureDirectAPIServerAccess(ctx resource.Context,
 }
 
 func (i *operatorComponent) configureDirectAPIServiceAccessForCluster(ctx resource.Context, cfg Config,
-	c cluster.Cluster) error {
+	c cluster.Cluster,
+) error {
 	clusters := ctx.Clusters().Configs(c.Config())
 	if len(clusters) == 0 {
 		// giving 0 clusters to ctx.ConfigKube() means using all clusters
@@ -748,7 +751,8 @@ func (i *operatorComponent) configureDirectAPIServiceAccessForCluster(ctx resour
 }
 
 func (i *operatorComponent) configureDirectAPIServiceAccessBetweenClusters(ctx resource.Context, cfg Config,
-	c cluster.Cluster, from ...cluster.Cluster) error {
+	c cluster.Cluster, from ...cluster.Cluster,
+) error {
 	// Create a secret.
 	secret, err := CreateRemoteSecret(ctx, c, cfg)
 	if err != nil {
diff --git a/pkg/test/framework/components/istio/util.go b/pkg/test/framework/components/istio/util.go
index 57ba464ab7..b796aaea25 100644
--- a/pkg/test/framework/components/istio/util.go
+++ b/pkg/test/framework/components/istio/util.go
@@ -105,7 +105,8 @@ func (i *operatorComponent) RemoteDiscoveryAddressFor(cluster cluster.Cluster) (
 }
 
 func getRemoteServiceAddress(s *kube.Settings, cluster cluster.Cluster, ns, label, svcName string,
-	port int) (interface{}, bool, error) {
+	port int,
+) (interface{}, bool, error) {
 	if !s.LoadBalancerSupported {
 		pods, err := cluster.PodsForSelector(context.TODO(), ns, fmt.Sprintf("istio=%s", label))
 		if err != nil {
@@ -179,7 +180,8 @@ func (i *operatorComponent) isExternalControlPlane() bool {
 }
 
 func UpdateMeshConfig(t resource.Context, ns string, clusters cluster.Clusters,
-	update func(*meshconfig.MeshConfig) error, cleanupStrategy cleanup.Strategy) error {
+	update func(*meshconfig.MeshConfig) error, cleanupStrategy cleanup.Strategy,
+) error {
 	errG := multierror.Group{}
 	origCfg := map[string]string{}
 	mu := sync.RWMutex{}
@@ -259,7 +261,8 @@ func UpdateMeshConfig(t resource.Context, ns string, clusters cluster.Clusters,
 }
 
 func UpdateMeshConfigOrFail(t framework.TestContext, ns string, clusters cluster.Clusters,
-	update func(*meshconfig.MeshConfig) error, cleanupStrategy cleanup.Strategy) {
+	update func(*meshconfig.MeshConfig) error, cleanupStrategy cleanup.Strategy,
+) {
 	t.Helper()
 	if err := UpdateMeshConfig(t, ns, clusters, update, cleanupStrategy); err != nil {
 		t.Fatal(err)
diff --git a/pkg/test/kube/util.go b/pkg/test/kube/util.go
index 8a0695cb6c..b5a9a106e1 100644
--- a/pkg/test/kube/util.go
+++ b/pkg/test/kube/util.go
@@ -147,7 +147,8 @@ func WaitUntilPodsAreReady(fetchFunc PodFetchFunc, opts ...retry.Option) ([]kube
 // WaitUntilServiceEndpointsAreReady will wait until the service with the given name/namespace is present, and have at least
 // one usable endpoint.
 func WaitUntilServiceEndpointsAreReady(a kubernetes.Interface, ns string, name string,
-	opts ...retry.Option) (*kubeApiCore.Service, *kubeApiCore.Endpoints, error) {
+	opts ...retry.Option,
+) (*kubeApiCore.Service, *kubeApiCore.Endpoints, error) {
 	var service *kubeApiCore.Service
 	var endpoints *kubeApiCore.Endpoints
 	err := retry.UntilSuccess(func() error {
@@ -207,7 +208,8 @@ func WaitForSecretToExist(a kubernetes.Interface, namespace, name string, waitTi
 
 // WaitForSecretToExistOrFail calls WaitForSecretToExist and fails the given test.Failer if an error occurs.
 func WaitForSecretToExistOrFail(t test.Failer, a kubernetes.Interface, namespace, name string,
-	waitTime time.Duration) *kubeApiCore.Secret {
+	waitTime time.Duration,
+) *kubeApiCore.Secret {
 	t.Helper()
 	s, err := WaitForSecretToExist(a, namespace, name, waitTime)
 	if err != nil {
diff --git a/pkg/test/loadbalancersim/lb_test.go b/pkg/test/loadbalancersim/lb_test.go
index d84b18f8c2..06d94f91e6 100644
--- a/pkg/test/loadbalancersim/lb_test.go
+++ b/pkg/test/loadbalancersim/lb_test.go
@@ -48,15 +48,15 @@ func TestLoadBalancing(t *testing.T) {
 		2: 1,
 	}
 	networkLatencies := map[mesh.RouteKey]time.Duration{
-		mesh.RouteKey{
+		{
 			Src:  sameZone,
 			Dest: sameZone,
 		}: 1 * time.Millisecond,
-		mesh.RouteKey{
+		{
 			Src:  sameZone,
 			Dest: sameRegion,
 		}: 10 * time.Millisecond,
-		mesh.RouteKey{
+		{
 			Src:  sameZone,
 			Dest: otherRegion,
 		}: 100 * time.Millisecond,
@@ -210,7 +210,7 @@ func TestLoadBalancing(t *testing.T) {
 		outputFile = fmt.Sprintf("%s/lb_output.csv", homeDir)
 	}
 
-	err := os.WriteFile(outputFile, []byte(sm.toCSV()), 0644)
+	err := os.WriteFile(outputFile, []byte(sm.toCSV()), 0o644)
 	if err != nil {
 		t.Fatal(err)
 	}
diff --git a/pkg/wasm/cache.go b/pkg/wasm/cache.go
index 36f1b549c8..5794767137 100644
--- a/pkg/wasm/cache.go
+++ b/pkg/wasm/cache.go
@@ -176,7 +176,8 @@ func pullIfNotPresent(pullPolicy extensions.PullPolicy, u *url.URL) bool {
 // Get returns path the local Wasm module file.
 func (c *LocalFileCache) Get(
 	downloadURL, checksum, resourceName, resourceVersion string,
-	timeout time.Duration, pullSecret []byte, pullPolicy extensions.PullPolicy) (string, error) {
+	timeout time.Duration, pullSecret []byte, pullPolicy extensions.PullPolicy,
+) (string, error) {
 	// Construct Wasm cache key with downloading URL and provided checksum of the module.
 	key := cacheKey{
 		downloadURL: downloadURL,
diff --git a/pkg/wasm/convert_test.go b/pkg/wasm/convert_test.go
index 4fbf986234..f382b96b98 100644
--- a/pkg/wasm/convert_test.go
+++ b/pkg/wasm/convert_test.go
@@ -45,7 +45,8 @@ type mockCache struct {
 
 func (c *mockCache) Get(
 	downloadURL, checksum, resourceName, resourceVersion string,
-	timeout time.Duration, pullSecret []byte, pullPolicy extensions.PullPolicy) (string, error) {
+	timeout time.Duration, pullSecret []byte, pullPolicy extensions.PullPolicy,
+) (string, error) {
 	url, _ := url.Parse(downloadURL)
 	query := url.Query()
 
diff --git a/pkg/webhooks/validation/controller/controller.go b/pkg/webhooks/validation/controller/controller.go
index d32da321d6..d9b7e86fd8 100644
--- a/pkg/webhooks/validation/controller/controller.go
+++ b/pkg/webhooks/validation/controller/controller.go
@@ -100,7 +100,8 @@ type Controller struct {
 
 // NewValidatingWebhookController creates a new Controller.
 func NewValidatingWebhookController(client kube.Client,
-	revision, ns string, caBundleWatcher *keycertbundle.Watcher) *Controller {
+	revision, ns string, caBundleWatcher *keycertbundle.Watcher,
+) *Controller {
 	o := Options{
 		WatchedNamespace: ns,
 		CABundleWatcher:  caBundleWatcher,
@@ -406,7 +407,8 @@ func (c *Controller) isDryRunOfInvalidConfigRejected() (rejected bool, reason st
 }
 
 func (c *Controller) updateValidatingWebhookConfiguration(current *kubeApiAdmission.ValidatingWebhookConfiguration,
-	caBundle []byte, failurePolicy kubeApiAdmission.FailurePolicyType) error {
+	caBundle []byte, failurePolicy kubeApiAdmission.FailurePolicyType,
+) error {
 	dirty := false
 	for i := range current.Webhooks {
 		if !bytes.Equal(current.Webhooks[i].ClientConfig.CABundle, caBundle) ||
diff --git a/pkg/webhooks/webhookpatch.go b/pkg/webhooks/webhookpatch.go
index b70930b543..06da12e3b1 100644
--- a/pkg/webhooks/webhookpatch.go
+++ b/pkg/webhooks/webhookpatch.go
@@ -64,7 +64,8 @@ type WebhookCertPatcher struct {
 // NewWebhookCertPatcher creates a WebhookCertPatcher
 func NewWebhookCertPatcher(
 	client kubelib.Client,
-	revision, webhookName string, caBundleWatcher *keycertbundle.Watcher) (*WebhookCertPatcher, error) {
+	revision, webhookName string, caBundleWatcher *keycertbundle.Watcher,
+) (*WebhookCertPatcher, error) {
 	p := &WebhookCertPatcher{
 		client:          client.Kube(),
 		revision:        revision,
@@ -118,7 +119,8 @@ func (w *WebhookCertPatcher) webhookPatchTask(o types.NamespacedName) error {
 // patchMutatingWebhookConfig takes a webhookConfigName and patches the CA bundle for that webhook configuration
 func (w *WebhookCertPatcher) patchMutatingWebhookConfig(
 	client admissionregistrationv1client.MutatingWebhookConfigurationInterface,
-	webhookConfigName string) error {
+	webhookConfigName string,
+) error {
 	raw, _, err := w.informer.GetIndexer().GetByKey(webhookConfigName)
 	if raw == nil || err != nil {
 		reportWebhookPatchFailure(webhookConfigName, reasonWebhookConfigNotFound)
diff --git a/security/pkg/k8s/chiron/controller.go b/security/pkg/k8s/chiron/controller.go
index 344f820168..8fbe630655 100644
--- a/security/pkg/k8s/chiron/controller.go
+++ b/security/pkg/k8s/chiron/controller.go
@@ -94,7 +94,8 @@ func NewWebhookController(gracePeriodRatio float32, minGracePeriod time.Duration
 	client clientset.Interface,
 	k8sCaCertFile string,
 	secretNames, dnsNames []string,
-	secretNamespace string, certIssuer string) (*WebhookController, error) {
+	secretNamespace string, certIssuer string,
+) (*WebhookController, error) {
 	if gracePeriodRatio < 0 || gracePeriodRatio > 1 {
 		return nil, fmt.Errorf("grace period ratio %f should be within [0, 1]", gracePeriodRatio)
 	}
diff --git a/security/pkg/k8s/controller/casecret.go b/security/pkg/k8s/controller/casecret.go
index 1c890623da..861c38fc40 100644
--- a/security/pkg/k8s/controller/casecret.go
+++ b/security/pkg/k8s/controller/casecret.go
@@ -42,7 +42,8 @@ func NewCaSecretController(core corev1.CoreV1Interface) *CaSecretController {
 
 // LoadCASecretWithRetry reads CA secret with retries until timeout.
 func (csc *CaSecretController) LoadCASecretWithRetry(secretName, namespace string,
-	retryInterval, timeout time.Duration) (*v1.Secret, error) {
+	retryInterval, timeout time.Duration,
+) (*v1.Secret, error) {
 	start := time.Now()
 	var caSecret *v1.Secret
 	var scrtErr error
@@ -65,7 +66,8 @@ func (csc *CaSecretController) LoadCASecretWithRetry(secretName, namespace strin
 
 // UpdateCASecretWithRetry updates CA secret with retries until timeout.
 func (csc *CaSecretController) UpdateCASecretWithRetry(caSecret *v1.Secret,
-	retryInterval, timeout time.Duration) error {
+	retryInterval, timeout time.Duration,
+) error {
 	start := time.Now()
 	for {
 		_, scrtErr := csc.client.Secrets(caSecret.Namespace).Update(context.TODO(), caSecret, metav1.UpdateOptions{})
diff --git a/security/pkg/nodeagent/caclient/providers/google/mock/ca_mock.go b/security/pkg/nodeagent/caclient/providers/google/mock/ca_mock.go
index 94e67b6ccf..2167c2d5a9 100644
--- a/security/pkg/nodeagent/caclient/providers/google/mock/ca_mock.go
+++ b/security/pkg/nodeagent/caclient/providers/google/mock/ca_mock.go
@@ -33,7 +33,8 @@ type CAService struct {
 
 // CreateCertificate is a mocked function for the Google Mesh CA API.
 func (ca *CAService) CreateCertificate(ctx context.Context, in *gcapb.MeshCertificateRequest) (
-	*gcapb.MeshCertificateResponse, error) {
+	*gcapb.MeshCertificateResponse, error,
+) {
 	if ca.Err == nil {
 		return &gcapb.MeshCertificateResponse{CertChain: ca.Certs}, nil
 	}
diff --git a/security/pkg/nodeagent/sds/sdsservice.go b/security/pkg/nodeagent/sds/sdsservice.go
index 3a6ab33067..2904a4a131 100644
--- a/security/pkg/nodeagent/sds/sdsservice.go
+++ b/security/pkg/nodeagent/sds/sdsservice.go
@@ -214,7 +214,7 @@ func toEnvoySecret(s *security.SecretItem, caRootPath string, pkpConf *mesh.Priv
 	secret := &tls.Secret{
 		Name: s.ResourceName,
 	}
-	cfg := security.SdsCertificateConfig{}
+	var cfg security.SdsCertificateConfig
 	ok := false
 	if s.ResourceName == security.FileRootSystemCACert {
 		cfg, ok = security.SdsCertificateConfigFromResourceNameForOSCACert(caRootPath)
diff --git a/security/pkg/nodeagent/test/mock/caserver.go b/security/pkg/nodeagent/test/mock/caserver.go
index 10bf8c9132..2e437eb366 100644
--- a/security/pkg/nodeagent/test/mock/caserver.go
+++ b/security/pkg/nodeagent/test/mock/caserver.go
@@ -150,7 +150,8 @@ func (s *CAServer) sendEmpty() bool {
 
 // CreateCertificate handles CSR.
 func (s *CAServer) CreateCertificate(ctx context.Context, request *pb.IstioCertificateRequest) (
-	*pb.IstioCertificateResponse, error) {
+	*pb.IstioCertificateResponse, error,
+) {
 	caServerLog.Infof("received CSR request")
 	if s.shouldReject() {
 		caServerLog.Info("force rejecting CSR request")
diff --git a/security/pkg/pki/ca/ca.go b/security/pkg/pki/ca/ca.go
index 7a312c1db3..53fb7a5998 100644
--- a/security/pkg/pki/ca/ca.go
+++ b/security/pkg/pki/ca/ca.go
@@ -101,7 +101,8 @@ func NewSelfSignedIstioCAOptions(ctx context.Context,
 	rootCertGracePeriodPercentile int, caCertTTL, rootCertCheckInverval, defaultCertTTL,
 	maxCertTTL time.Duration, org string, dualUse bool, namespace string,
 	readCertRetryInterval time.Duration, client corev1.CoreV1Interface,
-	rootCertFile string, enableJitter bool, caRSAKeySize int) (caOpts *IstioCAOptions, err error) {
+	rootCertFile string, enableJitter bool, caRSAKeySize int,
+) (caOpts *IstioCAOptions, err error) {
 	// For the first time the CA is up, if readSigningCertOnly is unset,
 	// it generates a self-signed key/cert pair and write it to CASecret.
 	// For subsequent restart, CA will reads key/cert from CASecret.
@@ -191,7 +192,8 @@ func NewSelfSignedIstioCAOptions(ctx context.Context,
 // NewSelfSignedDebugIstioCAOptions returns a new IstioCAOptions instance using self-signed certificate produced by in-memory CA,
 // which runs without K8s, and no local ca key file presented.
 func NewSelfSignedDebugIstioCAOptions(rootCertFile string, caCertTTL, defaultCertTTL, maxCertTTL time.Duration,
-	org string, caRSAKeySize int) (caOpts *IstioCAOptions, err error) {
+	org string, caRSAKeySize int,
+) (caOpts *IstioCAOptions, err error) {
 	caOpts = &IstioCAOptions{
 		CAType:         selfSignedCA,
 		DefaultCertTTL: defaultCertTTL,
@@ -226,7 +228,8 @@ func NewSelfSignedDebugIstioCAOptions(rootCertFile string, caCertTTL, defaultCer
 
 // NewPluggedCertIstioCAOptions returns a new IstioCAOptions instance using given certificate.
 func NewPluggedCertIstioCAOptions(certChainFile, signingCertFile, signingKeyFile, rootCertFile string,
-	defaultCertTTL, maxCertTTL time.Duration, caRSAKeySize int) (caOpts *IstioCAOptions, err error) {
+	defaultCertTTL, maxCertTTL time.Duration, caRSAKeySize int,
+) (caOpts *IstioCAOptions, err error) {
 	caOpts = &IstioCAOptions{
 		CAType:         pluggedCertCA,
 		DefaultCertTTL: defaultCertTTL,
@@ -307,13 +310,15 @@ func (ca *IstioCA) Run(stopChan chan struct{}) {
 
 // Sign takes a PEM-encoded CSR and cert opts, and returns a signed certificate.
 func (ca *IstioCA) Sign(csrPEM []byte, certOpts CertOpts) (
-	[]byte, error) {
+	[]byte, error,
+) {
 	return ca.sign(csrPEM, certOpts.SubjectIDs, certOpts.TTL, true, certOpts.ForCA)
 }
 
 // SignWithCertChain is similar to Sign but returns the leaf cert and the entire cert chain.
 func (ca *IstioCA) SignWithCertChain(csrPEM []byte, certOpts CertOpts) (
-	[]string, error) {
+	[]string, error,
+) {
 	cert, err := ca.signWithCertChain(csrPEM, certOpts.SubjectIDs, certOpts.TTL, true, certOpts.ForCA)
 	if err != nil {
 		return nil, err
@@ -412,7 +417,8 @@ func (ca *IstioCA) sign(csrPEM []byte, subjectIDs []string, requestedLifetime ti
 }
 
 func (ca *IstioCA) signWithCertChain(csrPEM []byte, subjectIDs []string, requestedLifetime time.Duration, lifetimeCheck,
-	forCA bool) ([]byte, error) {
+	forCA bool,
+) ([]byte, error) {
 	cert, err := ca.sign(csrPEM, subjectIDs, requestedLifetime, lifetimeCheck, forCA)
 	if err != nil {
 		return nil, err
diff --git a/security/pkg/pki/ca/selfsignedcarootcertrotator.go b/security/pkg/pki/ca/selfsignedcarootcertrotator.go
index 461cfbdf88..28177ee21d 100644
--- a/security/pkg/pki/ca/selfsignedcarootcertrotator.go
+++ b/security/pkg/pki/ca/selfsignedcarootcertrotator.go
@@ -57,7 +57,8 @@ type SelfSignedCARootCertRotator struct {
 // NewSelfSignedCARootCertRotator returns a new root cert rotator instance that
 // rotates self-signed root cert periodically.
 func NewSelfSignedCARootCertRotator(config *SelfSignedCARootCertRotatorConfig,
-	ca *IstioCA) *SelfSignedCARootCertRotator {
+	ca *IstioCA,
+) *SelfSignedCARootCertRotator {
 	rotator := &SelfSignedCARootCertRotator{
 		caSecretController: controller.NewCaSecretController(config.client),
 		config:             config,
@@ -125,7 +126,8 @@ func (rotator *SelfSignedCARootCertRotator) checkAndRotateRootCert() {
 // root cert if the current one is about to expire. The rotation uses existing
 // root private key to generate a new root cert, and updates root cert secret.
 func (rotator *SelfSignedCARootCertRotator) checkAndRotateRootCertForSigningCertCitadel(
-	caSecret *v1.Secret) {
+	caSecret *v1.Secret,
+) {
 	if caSecret == nil {
 		rootCertRotatorLog.Errorf("root cert secret %s is nil, skip cert rotation job",
 			CASecret)
diff --git a/security/pkg/pki/ca/selfsignedcarootcertrotator_test.go b/security/pkg/pki/ca/selfsignedcarootcertrotator_test.go
index b09a80b5e9..647d3038db 100644
--- a/security/pkg/pki/ca/selfsignedcarootcertrotator_test.go
+++ b/security/pkg/pki/ca/selfsignedcarootcertrotator_test.go
@@ -164,7 +164,8 @@ func TestRootCertRotatorKeepCertFieldsUnchanged(t *testing.T) {
 // updateRootCertWithCustomCertOptions generate root cert and private key with
 // custom cert options, and replaces root cert and key in CA secret.
 func updateRootCertWithCustomCertOptions(t *testing.T,
-	rotator *SelfSignedCARootCertRotator, options util.CertOptions) {
+	rotator *SelfSignedCARootCertRotator, options util.CertOptions,
+) {
 	certItem := loadCert(rotator)
 
 	pemCert, pemKey, err := util.GenCertKeyFromOptions(options)
diff --git a/security/pkg/pki/util/generate_cert.go b/security/pkg/pki/util/generate_cert.go
index 3763ad06ab..4c5c6e3642 100644
--- a/security/pkg/pki/util/generate_cert.go
+++ b/security/pkg/pki/util/generate_cert.go
@@ -221,7 +221,8 @@ func MergeCertOptions(defaultOpts, deltaOpts CertOptions) CertOptions {
 
 // GenCertFromCSR generates a X.509 certificate with the given CSR.
 func GenCertFromCSR(csr *x509.CertificateRequest, signingCert *x509.Certificate, publicKey interface{},
-	signingKey crypto.PrivateKey, subjectIDs []string, ttl time.Duration, isCA bool) (cert []byte, err error) {
+	signingKey crypto.PrivateKey, subjectIDs []string, ttl time.Duration, isCA bool,
+) (cert []byte, err error) {
 	tmpl, err := genCertTemplateFromCSR(csr, subjectIDs, ttl, isCA)
 	if err != nil {
 		return nil, err
@@ -230,8 +231,9 @@ func GenCertFromCSR(csr *x509.CertificateRequest, signingCert *x509.Certificate,
 }
 
 // LoadSignerCredsFromFiles loads the signer cert&key from the given files.
-//   signerCertFile: cert file name
-//   signerPrivFile: private key file name
+//
+//	signerCertFile: cert file name
+//	signerPrivFile: private key file name
 func LoadSignerCredsFromFiles(signerCertFile string, signerPrivFile string) (*x509.Certificate, crypto.PrivateKey, error) {
 	signerCertBytes, err := os.ReadFile(signerCertFile)
 	if err != nil {
@@ -263,7 +265,8 @@ func LoadSignerCredsFromFiles(signerCertFile string, signerPrivFile string) (*x5
 // genCertTemplateFromCSR generates a certificate template with the given CSR.
 // The NotBefore value of the cert is set to current time.
 func genCertTemplateFromCSR(csr *x509.CertificateRequest, subjectIDs []string, ttl time.Duration, isCA bool) (
-	*x509.Certificate, error) {
+	*x509.Certificate, error,
+) {
 	subjectIDsInString := strings.Join(subjectIDs, ",")
 	var keyUsage x509.KeyUsage
 	extKeyUsages := []x509.ExtKeyUsage{}
@@ -397,7 +400,8 @@ func genSerialNum() (*big.Int, error) {
 }
 
 func encodePem(isCSR bool, csrOrCert []byte, priv interface{}, pkcs8 bool) (
-	csrOrCertPem []byte, privPem []byte, err error) {
+	csrOrCertPem []byte, privPem []byte, err error,
+) {
 	encodeMsg := "CERTIFICATE"
 	if isCSR {
 		encodeMsg = "CERTIFICATE REQUEST"
diff --git a/security/pkg/pki/util/keycertbundle.go b/security/pkg/pki/util/keycertbundle.go
index 811d3416a4..5272661a0f 100644
--- a/security/pkg/pki/util/keycertbundle.go
+++ b/security/pkg/pki/util/keycertbundle.go
@@ -56,7 +56,8 @@ func NewKeyCertBundleFromPem(certBytes, privKeyBytes, certChainBytes, rootCertBy
 // NewVerifiedKeyCertBundleFromPem returns a new KeyCertBundle, or error if the provided certs failed the
 // verification.
 func NewVerifiedKeyCertBundleFromPem(certBytes, privKeyBytes, certChainBytes, rootCertBytes []byte) (
-	*KeyCertBundle, error) {
+	*KeyCertBundle, error,
+) {
 	bundle := &KeyCertBundle{}
 	if err := bundle.VerifyAndSetAll(certBytes, privKeyBytes, certChainBytes, rootCertBytes); err != nil {
 		return nil, err
@@ -67,7 +68,8 @@ func NewVerifiedKeyCertBundleFromPem(certBytes, privKeyBytes, certChainBytes, ro
 // NewVerifiedKeyCertBundleFromFile returns a new KeyCertBundle, or error if the provided certs failed the
 // verification.
 func NewVerifiedKeyCertBundleFromFile(certFile, privKeyFile, certChainFile, rootCertFile string) (
-	*KeyCertBundle, error) {
+	*KeyCertBundle, error,
+) {
 	certBytes, err := os.ReadFile(certFile)
 	if err != nil {
 		return nil, err
@@ -125,7 +127,8 @@ func (b *KeyCertBundle) GetAllPem() (certBytes, privKeyBytes, certChainBytes, ro
 // GetAll returns all key/cert in KeyCertBundle together. Getting all values together avoids inconsistency.
 // NOTE: Callers should not modify the content of cert and privKey.
 func (b *KeyCertBundle) GetAll() (cert *x509.Certificate, privKey *crypto.PrivateKey, certChainBytes,
-	rootCertBytes []byte) {
+	rootCertBytes []byte,
+) {
 	b.mutex.RLock()
 	cert = b.cert
 	privKey = b.privKey
diff --git a/security/pkg/server/ca/authenticate/kubeauth/kube_jwt.go b/security/pkg/server/ca/authenticate/kubeauth/kube_jwt.go
index 58d687c4b8..b2ab19bfa7 100644
--- a/security/pkg/server/ca/authenticate/kubeauth/kube_jwt.go
+++ b/security/pkg/server/ca/authenticate/kubeauth/kube_jwt.go
@@ -61,7 +61,8 @@ type KubeJWTAuthenticator struct {
 
 // NewKubeJWTAuthenticator creates a new kubeJWTAuthenticator.
 func NewKubeJWTAuthenticator(meshHolder mesh.Holder, client kubernetes.Interface, clusterID cluster.ID,
-	remoteKubeClientGetter RemoteKubeClientGetter, jwtPolicy string) *KubeJWTAuthenticator {
+	remoteKubeClientGetter RemoteKubeClientGetter, jwtPolicy string,
+) *KubeJWTAuthenticator {
 	return &KubeJWTAuthenticator{
 		meshHolder:             meshHolder,
 		jwtPolicy:              jwtPolicy,
diff --git a/security/pkg/server/ca/server.go b/security/pkg/server/ca/server.go
index f199081ad4..752fa8b50e 100644
--- a/security/pkg/server/ca/server.go
+++ b/security/pkg/server/ca/server.go
@@ -70,7 +70,8 @@ func getConnectionAddress(ctx context.Context) string {
 // the validity duration is the ValidityDuration in request, or default value if the given duration is invalid.
 // it is signed by the CA signing key.
 func (s *Server) CreateCertificate(ctx context.Context, request *pb.IstioCertificateRequest) (
-	*pb.IstioCertificateResponse, error) {
+	*pb.IstioCertificateResponse, error,
+) {
 	s.monitoring.CSR.Increment()
 	caller := Authenticate(ctx, s.Authenticators)
 	if caller == nil {
@@ -143,7 +144,8 @@ func (s *Server) Register(grpcServer *grpc.Server) {
 
 // New creates a new instance of `IstioCAServiceServer`
 func New(ca CertificateAuthority, ttl time.Duration,
-	authenticators []security.Authenticator) (*Server, error) {
+	authenticators []security.Authenticator,
+) (*Server, error) {
 	certBundle := ca.GetCAKeyCertBundle()
 	if len(certBundle.GetRootCertPem()) != 0 {
 		recordCertsExpiry(certBundle)
diff --git a/security/pkg/stsservice/server/server_test.go b/security/pkg/stsservice/server/server_test.go
index a7fe9ac8e5..dc8e2767ca 100644
--- a/security/pkg/stsservice/server/server_test.go
+++ b/security/pkg/stsservice/server/server_test.go
@@ -310,7 +310,8 @@ func genStsRequest(reqType stsReqType, serverAddr string) (req *http.Request) {
 }
 
 func genStsResponse(respType stsRespType, param stsservice.StsResponseParameters,
-	serverErr error, tokenInfo *stsservice.TokenInfo) (resp *http.Response) {
+	serverErr error, tokenInfo *stsservice.TokenInfo,
+) (resp *http.Response) {
 	resp = &http.Response{
 		Header: make(http.Header),
 	}
diff --git a/security/pkg/stsservice/tokenmanager/google/tokenexchangeplugin_test.go b/security/pkg/stsservice/tokenmanager/google/tokenexchangeplugin_test.go
index 497b73e067..37684d600a 100644
--- a/security/pkg/stsservice/tokenmanager/google/tokenexchangeplugin_test.go
+++ b/security/pkg/stsservice/tokenmanager/google/tokenexchangeplugin_test.go
@@ -75,7 +75,8 @@ func TestTokenExchangePlugin(t *testing.T) {
 }
 
 func verifyDumpStatus(t *testing.T, tCase string, dumpJSON []byte, lastStatus map[string]stsservice.TokenInfo,
-	expected []string) map[string]stsservice.TokenInfo {
+	expected []string,
+) map[string]stsservice.TokenInfo {
 	newStatus := &stsservice.TokensDump{}
 	if err := json.Unmarshal(dumpJSON, newStatus); err != nil {
 		t.Errorf("(Test case %s), failed to unmarshal status dump: %v", tCase, err)
diff --git a/tests/fuzz/ca_server_fuzzer.go b/tests/fuzz/ca_server_fuzzer.go
index f1be1d463a..2f3c2627ee 100644
--- a/tests/fuzz/ca_server_fuzzer.go
+++ b/tests/fuzz/ca_server_fuzzer.go
@@ -21,13 +21,12 @@
 	"context"
 	"fmt"
 
-	pb "istio.io/api/security/v1alpha1"
+	fuzz "github.com/AdaLogics/go-fuzz-headers"
 
+	pb "istio.io/api/security/v1alpha1"
 	"istio.io/istio/pkg/security"
 	mockca "istio.io/istio/security/pkg/pki/ca/mock"
 	caerror "istio.io/istio/security/pkg/pki/error"
-
-	fuzz "github.com/AdaLogics/go-fuzz-headers"
 )
 
 // FuzzCreateCertificate implements a fuzzer
diff --git a/tests/fuzz/kube_gateway_controller_fuzzer.go b/tests/fuzz/kube_gateway_controller_fuzzer.go
index 127f28a9dc..0303ad6e2f 100644
--- a/tests/fuzz/kube_gateway_controller_fuzzer.go
+++ b/tests/fuzz/kube_gateway_controller_fuzzer.go
@@ -19,6 +19,7 @@
 
 import (
 	"fmt"
+
 	fuzz "github.com/AdaLogics/go-fuzz-headers"
 )
 
diff --git a/tests/fuzz/security_authz_builder_fuzzer.go b/tests/fuzz/security_authz_builder_fuzzer.go
index be6581cf95..2d509e3760 100644
--- a/tests/fuzz/security_authz_builder_fuzzer.go
+++ b/tests/fuzz/security_authz_builder_fuzzer.go
@@ -18,10 +18,10 @@
 package builder
 
 import (
+	fuzz "github.com/AdaLogics/go-fuzz-headers"
+
 	"istio.io/istio/pilot/pkg/networking/plugin"
 	"istio.io/istio/pilot/pkg/security/trustdomain"
-
-	fuzz "github.com/AdaLogics/go-fuzz-headers"
 )
 
 func InternalFuzzBuildHTTP(data []byte) int {
diff --git a/tests/integration/helm/util.go b/tests/integration/helm/util.go
index 9a29831a4b..6a53316b23 100644
--- a/tests/integration/helm/util.go
+++ b/tests/integration/helm/util.go
@@ -69,7 +69,8 @@
 // InstallIstio install Istio using Helm charts with the provided
 // override values file and fails the tests on any failures.
 func InstallIstio(t test.Failer, cs cluster.Cluster,
-	h *helm.Helm, suffix, overrideValuesFile, relPath, version string, installGateways bool) {
+	h *helm.Helm, suffix, overrideValuesFile, relPath, version string, installGateways bool,
+) {
 	CreateNamespace(t, cs, IstioNamespace)
 
 	// Install base chart
@@ -106,7 +107,8 @@ func InstallIstio(t test.Failer, cs cluster.Cluster,
 // InstallIstioWithRevision install Istio using Helm charts with the provided
 // override values file and fails the tests on any failures.
 func InstallIstioWithRevision(t test.Failer, cs cluster.Cluster,
-	h *helm.Helm, fileSuffix, version, revision, overrideValuesFile string, upgradeBaseChart, useTestData bool) {
+	h *helm.Helm, fileSuffix, version, revision, overrideValuesFile string, upgradeBaseChart, useTestData bool,
+) {
 	CreateNamespace(t, cs, IstioNamespace)
 
 	// base chart may already be installed if the Istio was previously already installed
diff --git a/tests/integration/operator/switch_cr_test.go b/tests/integration/operator/switch_cr_test.go
index e08ab0be36..ae948ed4fd 100644
--- a/tests/integration/operator/switch_cr_test.go
+++ b/tests/integration/operator/switch_cr_test.go
@@ -280,7 +280,8 @@ func cleanupInClusterCRs(t framework.TestContext, cs cluster.Cluster) {
 }
 
 func installWithCRFile(t framework.TestContext, ctx resource.Context, cs cluster.Cluster,
-	istioCtl istioctl.Instance, profileName string, revision string) {
+	istioCtl istioctl.Instance, profileName string, revision string,
+) {
 	scopes.Framework.Infof(fmt.Sprintf("=== install istio with profile: %s===\n", profileName))
 	metadataYAML := `
 apiVersion: install.istio.io/v1alpha1
@@ -323,7 +324,8 @@ func installWithCRFile(t framework.TestContext, ctx resource.Context, cs cluster
 // verifyInstallation verify IOP CR status and compare in-cluster resources with generated ones.
 // It also returns the expected K8sObjects generated by manifest generate command.
 func verifyInstallation(t framework.TestContext, ctx resource.Context,
-	istioCtl istioctl.Instance, profileName string, revision string, cs cluster.Cluster) object.K8sObjects {
+	istioCtl istioctl.Instance, profileName string, revision string, cs cluster.Cluster,
+) object.K8sObjects {
 	scopes.Framework.Infof("=== verifying istio installation revision %s === ", revision)
 	if err := checkInstallStatus(cs, revision); err != nil {
 		t.Fatalf("IstioOperator status not healthy: %v", err)
@@ -357,7 +359,8 @@ func verifyInstallation(t framework.TestContext, ctx resource.Context,
 }
 
 func compareInClusterAndGeneratedResources(t framework.TestContext, cs cluster.Cluster, k8sObjects object.K8sObjects,
-	expectRemoved bool) {
+	expectRemoved bool,
+) {
 	// nolint:staticcheck
 	if k8sObjects == nil {
 		t.Fatalf("expected K8sObjects is nil")
diff --git a/tests/integration/pilot/grpc_probe_test.go b/tests/integration/pilot/grpc_probe_test.go
index c07be02a3e..d672f07219 100644
--- a/tests/integration/pilot/grpc_probe_test.go
+++ b/tests/integration/pilot/grpc_probe_test.go
@@ -63,7 +63,8 @@ func TestGRPCProbe(t *testing.T) {
 }
 
 func runGRPCProbeDeployment(ctx framework.TestContext, ns namespace.Instance, //nolint:interfacer
-	name string, rewrite bool, wantReady bool, openPort bool) {
+	name string, rewrite bool, wantReady bool, openPort bool,
+) {
 	ctx.Helper()
 
 	var grpcProbe echo.Instance
diff --git a/tests/integration/pilot/mcs/discoverability/discoverability_test.go b/tests/integration/pilot/mcs/discoverability/discoverability_test.go
index b3d2111717..b6d880f46c 100644
--- a/tests/integration/pilot/mcs/discoverability/discoverability_test.go
+++ b/tests/integration/pilot/mcs/discoverability/discoverability_test.go
@@ -204,7 +204,8 @@ func enableMCSServiceDiscovery(t resource.Context, cfg *istio.Config) {
 
 func runForAllClusterCombinations(
 	t framework.TestContext,
-	fn func(t framework.TestContext, from echo.Instance, to echo.Target)) {
+	fn func(t framework.TestContext, from echo.Instance, to echo.Target),
+) {
 	t.Helper()
 	echotest.New(t, echos.Instances).
 		WithDefaultFilters().
diff --git a/tests/integration/pilot/tcp_probe_test.go b/tests/integration/pilot/tcp_probe_test.go
index eee229bc85..4a14699ed8 100644
--- a/tests/integration/pilot/tcp_probe_test.go
+++ b/tests/integration/pilot/tcp_probe_test.go
@@ -50,7 +50,8 @@ func TestTcpProbe(t *testing.T) {
 }
 
 func runTCPProbeDeployment(ctx framework.TestContext, ns namespace.Instance, //nolint:interfacer
-	name string, rewrite bool, wantSuccess bool, openPort bool) {
+	name string, rewrite bool, wantSuccess bool, openPort bool,
+) {
 	ctx.Helper()
 
 	var tcpProbe echo.Instance
diff --git a/tests/integration/security/authorization_test.go b/tests/integration/security/authorization_test.go
index fad6b6c92f..4f62594c6e 100644
--- a/tests/integration/security/authorization_test.go
+++ b/tests/integration/security/authorization_test.go
@@ -229,7 +229,8 @@ func TestAuthorization_WorkloadSelector(t *testing.T) {
 			rootns := newRootNS(t)
 
 			newTestCase := func(from echo.Instance, to echo.Target, namePrefix, path string,
-				expectAllowed bool) func(t framework.TestContext) {
+				expectAllowed bool,
+			) func(t framework.TestContext) {
 				callCount := util.CallsPerCluster * to.WorkloadsOrFail(t).Len()
 				return func(t framework.TestContext) {
 					opts := echo.CallOptions{
@@ -1344,7 +1345,8 @@ func TestAuthorization_Audit(t *testing.T) {
 			}
 
 			newTestCase := func(applyPolicy func(t framework.TestContext), from echo.Instance, to echo.Target,
-				path string, expectAllowed bool) func(t framework.TestContext) {
+				path string, expectAllowed bool,
+			) func(t framework.TestContext) {
 				return func(t framework.TestContext) {
 					opts := echo.CallOptions{
 						To: to,
@@ -1474,7 +1476,8 @@ func TestAuthorization_Custom(t *testing.T) {
 				BuildOrFail(t)
 
 			newTestCase := func(from echo.Instance, to echo.Target, s scheme.Instance, port, path string, headers http.Header,
-				checker echo.Checker, expectAllowed bool) func(t framework.TestContext) {
+				checker echo.Checker, expectAllowed bool,
+			) func(t framework.TestContext) {
 				return func(t framework.TestContext) {
 					opts := echo.CallOptions{
 						To: to,
@@ -1569,7 +1572,8 @@ func TestAuthorization_Custom(t *testing.T) {
 			t.NewSubTest("ingress").Run(func(t framework.TestContext) {
 				ingr := ist.IngressFor(t.Clusters().Default())
 				newIngressTestCase := func(from, to echo.Instance, path string, h http.Header,
-					checker echo.Checker, expectAllowed bool) func(t framework.TestContext) {
+					checker echo.Checker, expectAllowed bool,
+				) func(t framework.TestContext) {
 					return func(t framework.TestContext) {
 						opts := echo.CallOptions{
 							To: to,
diff --git a/tests/integration/security/egress_gateway_origination_test.go b/tests/integration/security/egress_gateway_origination_test.go
index d7e35ab7a3..a827d0589a 100644
--- a/tests/integration/security/egress_gateway_origination_test.go
+++ b/tests/integration/security/egress_gateway_origination_test.go
@@ -328,7 +328,8 @@ func CreateGateway(t test.Failer, ctx resource.Context, clientNamespace namespac
 
 // Create the DestinationRule for TLS origination at Gateway by reading secret in istio-system namespace.
 func CreateDestinationRule(t framework.TestContext, to echo.Instances,
-	destinationRuleMode string, credentialName string) {
+	destinationRuleMode string, credentialName string,
+) {
 	args := map[string]interface{}{
 		"to":             to,
 		"Mode":           destinationRuleMode,
diff --git a/tests/integration/security/filebased_tls_origination/egress_gateway_origination_test.go b/tests/integration/security/filebased_tls_origination/egress_gateway_origination_test.go
index 4587efcdad..28342b7efb 100644
--- a/tests/integration/security/filebased_tls_origination/egress_gateway_origination_test.go
+++ b/tests/integration/security/filebased_tls_origination/egress_gateway_origination_test.go
@@ -213,7 +213,8 @@ func TestEgressGatewayTls(t *testing.T) {
 )
 
 func createDestinationRule(t framework.TestContext, serviceNamespace namespace.Instance,
-	destinationRuleMode string, fakeRootCert bool) bytes.Buffer {
+	destinationRuleMode string, fakeRootCert bool,
+) bytes.Buffer {
 	var destinationRuleToParse string
 	var rootCertPathToUse string
 	if destinationRuleMode == "MUTUAL" {
diff --git a/tests/integration/security/mtls_healthcheck_test.go b/tests/integration/security/mtls_healthcheck_test.go
index 88e577e2f5..466016c39e 100644
--- a/tests/integration/security/mtls_healthcheck_test.go
+++ b/tests/integration/security/mtls_healthcheck_test.go
@@ -52,7 +52,8 @@ func TestMtlsHealthCheck(t *testing.T) {
 }
 
 func runHealthCheckDeployment(ctx framework.TestContext, ns namespace.Instance, //nolint:interfacer
-	name string, rewrite bool) {
+	name string, rewrite bool,
+) {
 	ctx.Helper()
 	wantSuccess := rewrite
 	policyYAML := fmt.Sprintf(`apiVersion: security.istio.io/v1beta1
diff --git a/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go b/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go
index 026cba4260..73c6f5582c 100644
--- a/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go
+++ b/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go
@@ -88,7 +88,8 @@ func TestSdsEgressGatewayIstioMutual(t *testing.T) {
 }
 
 func doIstioMutualTest(
-	ctx framework.TestContext, ns namespace.Instance, configPath string, expectedCode int) {
+	ctx framework.TestContext, ns namespace.Instance, configPath string, expectedCode int,
+) {
 	var client echo.Instance
 	deployment.New(ctx).
 		With(&client, util.EchoConfig("client", ns, false, nil)).
diff --git a/tests/integration/security/sds_ingress/util/util.go b/tests/integration/security/sds_ingress/util/util.go
index 96147af639..ce319dab55 100644
--- a/tests/integration/security/sds_ingress/util/util.go
+++ b/tests/integration/security/sds_ingress/util/util.go
@@ -117,7 +117,8 @@ func IngressKubeSecretYAML(name, namespace string, ingressType CallType, ingress
 // and creates K8s secrets for ingress gateway.
 // nolint: interfacer
 func CreateIngressKubeSecret(t framework.TestContext, credName string,
-	ingressType CallType, ingressCred IngressCredential, isCompoundAndNotGeneric bool, clusters ...cluster.Cluster) {
+	ingressType CallType, ingressCred IngressCredential, isCompoundAndNotGeneric bool, clusters ...cluster.Cluster,
+) {
 	t.Helper()
 
 	// Get namespace for ingress gateway pod.
@@ -129,7 +130,8 @@ func CreateIngressKubeSecret(t framework.TestContext, credName string,
 // CreateIngressKubeSecretInNamespace  reads credential names from credNames and key/cert from ingressCred,
 // and creates K8s secrets for ingress gateway in the given namespace.
 func CreateIngressKubeSecretInNamespace(t framework.TestContext, credName string,
-	ingressType CallType, ingressCred IngressCredential, isCompoundAndNotGeneric bool, ns string, clusters ...cluster.Cluster) {
+	ingressType CallType, ingressCred IngressCredential, isCompoundAndNotGeneric bool, ns string, clusters ...cluster.Cluster,
+) {
 	t.Helper()
 
 	t.CleanupConditionally(func() {
@@ -274,17 +276,20 @@ type TLSContext struct {
 
 // SendRequestOrFail makes HTTPS request to ingress gateway to visit product page
 func SendRequestOrFail(ctx framework.TestContext, ing ingress.Instance, host string, path string,
-	callType CallType, tlsCtx TLSContext, exRsp ExpectedResponse) {
+	callType CallType, tlsCtx TLSContext, exRsp ExpectedResponse,
+) {
 	doSendRequestsOrFail(ctx, ing, host, path, callType, tlsCtx, exRsp, false /* useHTTP3 */)
 }
 
 func SendQUICRequestsOrFail(ctx framework.TestContext, ing ingress.Instance, host string, path string,
-	callType CallType, tlsCtx TLSContext, exRsp ExpectedResponse) {
+	callType CallType, tlsCtx TLSContext, exRsp ExpectedResponse,
+) {
 	doSendRequestsOrFail(ctx, ing, host, path, callType, tlsCtx, exRsp, true /* useHTTP3 */)
 }
 
 func doSendRequestsOrFail(ctx framework.TestContext, ing ingress.Instance, host string, path string,
-	callType CallType, tlsCtx TLSContext, exRsp ExpectedResponse, useHTTP3 bool) {
+	callType CallType, tlsCtx TLSContext, exRsp ExpectedResponse, useHTTP3 bool,
+) {
 	ctx.Helper()
 	opts := echo.CallOptions{
 		Timeout: time.Second,
@@ -335,7 +340,8 @@ func doSendRequestsOrFail(ctx framework.TestContext, ing ingress.Instance, host
 // RotateSecrets deletes kubernetes secrets by name in credNames and creates same secrets using key/cert
 // from ingressCred.
 func RotateSecrets(ctx framework.TestContext, credName string, // nolint:interfacer
-	ingressType CallType, ingressCred IngressCredential, isCompoundAndNotGeneric bool) {
+	ingressType CallType, ingressCred IngressCredential, isCompoundAndNotGeneric bool,
+) {
 	ctx.Helper()
 	c := ctx.Clusters().Default()
 	ist := istio.GetOrFail(ctx, ctx)
diff --git a/tools/bug-report/pkg/bugreport/bugreport.go b/tools/bug-report/pkg/bugreport/bugreport.go
index 9675cd76af..3375ba9d1d 100644
--- a/tools/bug-report/pkg/bugreport/bugreport.go
+++ b/tools/bug-report/pkg/bugreport/bugreport.go
@@ -356,7 +356,8 @@ func getFromCluster(f func(params *content.Params) (map[string]string, error), p
 // Runs if a goroutine, with errors reported through gErrors.
 // TODO(stewartbutler): output the logs to a more robust/complete structure.
 func getProxyLogs(client kube.ExtendedClient, config *config.BugReportConfig, resources *cluster2.Resources,
-	path, namespace, pod, container string, wg *sync.WaitGroup) {
+	path, namespace, pod, container string, wg *sync.WaitGroup,
+) {
 	wg.Add(1)
 	log.Infof("Waiting on logs %s", pod)
 	go func() {
@@ -375,7 +376,8 @@ func getProxyLogs(client kube.ExtendedClient, config *config.BugReportConfig, re
 // getIstiodLogs fetches Istiod logs for the given namespace/pod and writes the output.
 // Runs if a goroutine, with errors reported through gErrors.
 func getIstiodLogs(client kube.ExtendedClient, config *config.BugReportConfig, resources *cluster2.Resources,
-	namespace, pod string, wg *sync.WaitGroup) {
+	namespace, pod string, wg *sync.WaitGroup,
+) {
 	wg.Add(1)
 	log.Infof("Waiting on logs %s", pod)
 	go func() {
@@ -389,7 +391,8 @@ func getIstiodLogs(client kube.ExtendedClient, config *config.BugReportConfig, r
 
 // getOperatorLogs fetches istio-operator logs for the given namespace/pod and writes the output.
 func getOperatorLogs(client kube.ExtendedClient, config *config.BugReportConfig, resources *cluster2.Resources,
-	namespace, pod string, wg *sync.WaitGroup) {
+	namespace, pod string, wg *sync.WaitGroup,
+) {
 	wg.Add(1)
 	log.Infof("Waiting on logs %s", pod)
 	go func() {
@@ -403,7 +406,8 @@ func getOperatorLogs(client kube.ExtendedClient, config *config.BugReportConfig,
 
 // getLog fetches the logs for the given namespace/pod/container and returns the log text and stats for it.
 func getLog(client kube.ExtendedClient, resources *cluster2.Resources, config *config.BugReportConfig,
-	namespace, pod, container string) (string, *processlog.Stats, int, error) {
+	namespace, pod, container string,
+) (string, *processlog.Stats, int, error) {
 	log.Infof("Getting logs for %s/%s/%s...", namespace, pod, container)
 	clog, err := kubectlcmd.Logs(client, namespace, pod, container, false, config.DryRun)
 	if err != nil {
diff --git a/tools/bug-report/pkg/filter/filter.go b/tools/bug-report/pkg/filter/filter.go
index a300d270b3..1527b12885 100644
--- a/tools/bug-report/pkg/filter/filter.go
+++ b/tools/bug-report/pkg/filter/filter.go
@@ -40,7 +40,8 @@ func getMatchingPathsForSpec(config *config.BugReportConfig, cluster *cluster2.R
 }
 
 func getMatchingPathsForSpecImpl(config *config.BugReportConfig, cluster *cluster2.Resources, node map[string]interface{},
-	path path.Path, matchingPaths sets.Set) (sets.Set, error) {
+	path path.Path, matchingPaths sets.Set,
+) (sets.Set, error) {
 	for pe, n := range node {
 		np := append(path, pe)
 		if nn, ok := n.(map[string]interface{}); ok {
diff --git a/tools/istio-iptables/pkg/capture/run.go b/tools/istio-iptables/pkg/capture/run.go
index 5a1bfa8285..1d3087a6c0 100644
--- a/tools/istio-iptables/pkg/capture/run.go
+++ b/tools/istio-iptables/pkg/capture/run.go
@@ -4,7 +4,7 @@
 // you may not use this file except in compliance with the License.
 // You may obtain a copy of the License at
 //
-//     http://www.apache.org/licenses/LICENSE-2.0
+//	http://www.apache.org/licenses/LICENSE-2.0
 //
 // Unless required by applicable law or agreed to in writing, software
 // distributed under the License is distributed on an "AS IS" BASIS,
@@ -195,7 +195,8 @@ func (cfg *IptablesConfigurator) handleInboundPortsInclude() {
 func (cfg *IptablesConfigurator) handleOutboundIncludeRules(
 	rangeInclude NetworkRange,
 	appendRule func(command iptableslog.Command, chain string, table string, params ...string) *builder.IptablesBuilder,
-	insert func(command iptableslog.Command, chain string, table string, position int, params ...string) *builder.IptablesBuilder) {
+	insert func(command iptableslog.Command, chain string, table string, position int, params ...string) *builder.IptablesBuilder,
+) {
 	// Apply outbound IP inclusions.
 	if rangeInclude.IsWildcard {
 		// Wildcard specified. Redirect all remaining outbound traffic to Envoy.
@@ -618,7 +619,8 @@ func (f UDPRuleApplier) WithTable(table string) UDPRuleApplier {
 func HandleDNSUDP(
 	ops Ops, iptables *builder.IptablesBuilder, ext dep.Dependencies,
 	cmd, proxyUID, proxyGID string, dnsServersV4 []string, dnsServersV6 []string, captureAllDNS bool,
-	ownerGroupsFilter config.InterceptFilter) {
+	ownerGroupsFilter config.InterceptFilter,
+) {
 	f := UDPRuleApplier{
 		iptables: iptables,
 		ext:      ext,
@@ -677,7 +679,8 @@ func HandleDNSUDP(
 // Traffic that goes from istio to DNS servers and vice versa are zone 1 and traffic from
 // DNS client to istio and vice versa goes to zone 2
 func addConntrackZoneDNSUDP(
-	f UDPRuleApplier, proxyUID, proxyGID string, dnsServersV4 []string, dnsServersV6 []string, captureAllDNS bool) {
+	f UDPRuleApplier, proxyUID, proxyGID string, dnsServersV4 []string, dnsServersV6 []string, captureAllDNS bool,
+) {
 	// TODO: add ip6 as well
 	for _, uid := range split(proxyUID) {
 		// Packets with dst port 53 from istio to zone 1. These are Istio calls to upstream resolvers
diff --git a/tools/istio-iptables/pkg/capture/run_unspecified.go b/tools/istio-iptables/pkg/capture/run_unspecified.go
index 06de360e15..8817dfff73 100644
--- a/tools/istio-iptables/pkg/capture/run_unspecified.go
+++ b/tools/istio-iptables/pkg/capture/run_unspecified.go
@@ -7,7 +7,7 @@
 // you may not use this file except in compliance with the License.
 // You may obtain a copy of the License at
 //
-//     http://www.apache.org/licenses/LICENSE-2.0
+//	http://www.apache.org/licenses/LICENSE-2.0
 //
 // Unless required by applicable law or agreed to in writing, software
 // distributed under the License is distributed on an "AS IS" BASIS,
@@ -22,10 +22,8 @@
 	"istio.io/istio/tools/istio-iptables/pkg/config"
 )
 
-var (
-	// ErrNotImplemented is returned when a requested feature is not implemented.
-	ErrNotImplemented = errors.New("not implemented")
-)
+// ErrNotImplemented is returned when a requested feature is not implemented.
+var ErrNotImplemented = errors.New("not implemented")
 
 // configureTProxyRoutes configures ip firewall rules to enable TPROXY support.
 // See https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_filters/original_src_filter
-- 
2.35.3

