From f96b4b7a683deb954303788d69f781f01efcfd36 Mon Sep 17 00:00:00 2001
From: Rama Chavali <rama.rao@salesforce.com>
Date: Thu, 17 Mar 2022 10:17:51 +0530
Subject: do not send initial unhealthy endpoints on full push (#37854)

* do not send initial unhealthy endpoints on full push

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* add tests

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* fix test

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* lint

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* set istio endpoints only once

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* take a single lock

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* review comments

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>
---
 pilot/pkg/xds/eds.go      | 70 ++++++++++++++++++++++-----------------
 pilot/pkg/xds/eds_test.go | 16 ++++-----
 2 files changed, 48 insertions(+), 38 deletions(-)

diff --git a/pilot/pkg/xds/eds.go b/pilot/pkg/xds/eds.go
index 929d36d605..0180590bb0 100644
--- a/pilot/pkg/xds/eds.go
+++ b/pilot/pkg/xds/eds.go
@@ -160,36 +160,20 @@ func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string,
 	}
 
 	ep.mutex.Lock()
-	oldIstioEndpoints := ep.Shards[shard]
-	ep.Shards[shard] = istioEndpoints
-	// Check if ServiceAccounts have changed. We should do a full push if they have changed.
-	saUpdated := s.UpdateServiceAccount(ep, hostname)
+	defer ep.mutex.Unlock()
+	newIstioEndpoints := istioEndpoints
+	if features.SendUnhealthyEndpoints {
+		oldIstioEndpoints := ep.Shards[shard]
+		newIstioEndpoints = make([]*model.IstioEndpoint, 0, len(istioEndpoints))
 
-	// For existing endpoints, we need to do full push if service accounts change.
-	if saUpdated {
-		log.Infof("Full push, service accounts changed, %v", hostname)
-		pushType = FullPush
-	}
-	// Clear the cache here. While it would likely be cleared later when we trigger a push, a race
-	// condition is introduced where an XDS response may be generated before the update, but not
-	// completed until after a response after the update. Essentially, we transition from v0 -> v1 ->
-	// v0 -> invalidate -> v1. Reverting a change we pushed violates our contract of monotonically
-	// moving forward in version. In practice, this is pretty rare and self corrects nearly
-	// immediately. However, clearing the cache here has almost no impact on cache performance as we
-	// would clear it shortly after anyways.
-	s.Cache.Clear(map[model.ConfigKey]struct{}{{
-		Kind:      gvk.ServiceEntry,
-		Name:      hostname,
-		Namespace: namespace,
-	}: {}})
-	ep.mutex.Unlock()
-	if features.SendUnhealthyEndpoints && pushType == IncrementalPush {
 		// Check if new Endpoints are ready to be pushed. This check
 		// will ensure that if a new pod comes with a non ready endpoint,
 		// we do not unnecessarily push that config to Envoy.
 		// Please note that address is not a unique key. So this may not accurately
 		// identify based on health status and push too many times - which is ok since its an optimization.
 		emap := make(map[string]*model.IstioEndpoint, len(oldIstioEndpoints))
+		// Add new endpoints only if they are ever ready once to shards
+		// so that full push does not send them from shards.
 		for _, oie := range oldIstioEndpoints {
 			emap[oie.Address] = oie
 		}
@@ -197,24 +181,50 @@ func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string,
 		for _, nie := range istioEndpoints {
 			if oie, exists := emap[nie.Address]; exists {
 				// If endpoint exists already, we should push if it's health status changes.
-				needPush = oie.HealthStatus != nie.HealthStatus
-			} else {
+				if oie.HealthStatus != nie.HealthStatus {
+					needPush = true
+				}
+				newIstioEndpoints = append(newIstioEndpoints, nie)
+			} else if nie.HealthStatus == model.Healthy {
 				// If the endpoint does not exist in shards that means it is a
 				// new endpoint. Only send if it is healthy to avoid pushing endpoints
 				// that are not ready to start with.
-				needPush = nie.HealthStatus == model.Healthy
-			}
-			if needPush {
-				break
+				needPush = true
+				newIstioEndpoints = append(newIstioEndpoints, nie)
 			}
 		}
 
-		if !needPush {
+		if pushType != FullPush && !needPush {
 			log.Infof("No push, either old endpoint health status did not change or new endpoint came with unhealthy status, %v", hostname)
 			pushType = NoPush
 		}
+
 	}
 
+	ep.Shards[shard] = newIstioEndpoints
+
+	// Check if ServiceAccounts have changed. We should do a full push if they have changed.
+	saUpdated := s.UpdateServiceAccount(ep, hostname)
+
+	// For existing endpoints, we need to do full push if service accounts change.
+	if saUpdated {
+		log.Infof("Full push, service accounts changed, %v", hostname)
+		pushType = FullPush
+	}
+
+	// Clear the cache here. While it would likely be cleared later when we trigger a push, a race
+	// condition is introduced where an XDS response may be generated before the update, but not
+	// completed until after a response after the update. Essentially, we transition from v0 -> v1 ->
+	// v0 -> invalidate -> v1. Reverting a change we pushed violates our contract of monotonically
+	// moving forward in version. In practice, this is pretty rare and self corrects nearly
+	// immediately. However, clearing the cache here has almost no impact on cache performance as we
+	// would clear it shortly after anyways.
+	s.Cache.Clear(map[model.ConfigKey]struct{}{{
+		Kind:      gvk.ServiceEntry,
+		Name:      hostname,
+		Namespace: namespace,
+	}: {}})
+
 	return pushType
 }
 
diff --git a/pilot/pkg/xds/eds_test.go b/pilot/pkg/xds/eds_test.go
index 9fc733f19d..6e6c00f802 100644
--- a/pilot/pkg/xds/eds_test.go
+++ b/pilot/pkg/xds/eds_test.go
@@ -348,22 +348,21 @@ func TestEDSUnhealthyEndpoints(t *testing.T) {
 		t.Fatalf("Error in push %v", err)
 	}
 
-	// Validate that there are  no endpoints.
+	// Validate that we do not send initial unhealthy endpoints.
 	lbe := adscon.GetEndpoints()["outbound|53||unhealthy.svc.cluster.local"]
-	if lbe != nil && len(lbe.Endpoints) == 1 && len(lbe.Endpoints[0].LbEndpoints) > 1 {
-		t.Fatalf("one endpoint is expected for  %s,  but got %v", "unhealthy.svc.cluster.local", adscon.EndpointsJSON())
+	if lbe != nil && len(lbe.Endpoints) != 0 {
+		t.Fatalf("initial unhealthy endpoints are not expected to be sent %s,  but got %v", "unhealthy.svc.cluster.local", adscon.EndpointsJSON())
 	}
-
 	adscon.WaitClear()
 
-	// Set the unhealthy endpoint and validate Eds update is not triggered.
+	// Set additional unhealthy endpoint and validate Eds update is not triggered.
 	s.Discovery.MemRegistry.SetEndpoints("unhealthy.svc.cluster.local", "",
 		[]*model.IstioEndpoint{
 			{
 				Address:         "10.0.0.53",
 				EndpointPort:    53,
 				ServicePortName: "tcp-dns",
-				HealthStatus:    model.Healthy,
+				HealthStatus:    model.UnHealthy,
 			},
 			{
 				Address:         "10.0.0.54",
@@ -890,10 +889,10 @@ func edsUpdateInc(s *xds.FakeDiscoveryServer, adsc *adsc.ADSC, t *testing.T) {
 
 	// Update the endpoint with different SA - expect full
 	s.Discovery.MemRegistry.SetEndpoints(edsIncSvc, "",
-		newEndpointWithAccount("127.0.0.3", "account2", "v1"))
+		newEndpointWithAccount("127.0.0.2", "account2", "v1"))
 
 	edsFullUpdateCheck(adsc, t)
-	testTCPEndpoints("127.0.0.3", adsc, t)
+	testTCPEndpoints("127.0.0.2", adsc, t)
 
 	// Update the endpoint again, no SA change - expect incremental
 	s.Discovery.MemRegistry.SetEndpoints(edsIncSvc, "",
@@ -1244,6 +1243,7 @@ func addUnhealthyCluster(s *xds.FakeDiscoveryServer) {
 			Address:         "10.0.0.53",
 			EndpointPort:    53,
 			ServicePortName: "tcp-dns",
+			HealthStatus:    model.UnHealthy,
 		},
 		ServicePort: &model.Port{
 			Name:     "tcp-dns",
-- 
2.35.3

