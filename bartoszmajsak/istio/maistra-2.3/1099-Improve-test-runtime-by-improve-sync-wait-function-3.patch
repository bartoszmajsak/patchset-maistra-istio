From b51fa21636a72f9bf023b53b14231ebd0dcb1f7b Mon Sep 17 00:00:00 2001
From: John Howard <howardjohn@google.com>
Date: Fri, 22 Apr 2022 13:46:50 -0700
Subject: Improve test runtime by improve sync wait function (#38502)

* Improve test runtime by improve sync wait function

Currently, we are spending a lot of time just waiting in
WaitForCacheSync. Mostly in tests, but also in real code for fast sync
paths. This changes this to an exponential backoff. This improves the
controller unit tests from 20s to 2s - similar gains in other places

* cleanup dead code
---
 cni/pkg/repair/repaircontroller.go            |   4 +-
 pilot/pkg/bootstrap/server.go                 |   5 +-
 pilot/pkg/config/kube/crdclient/client.go     |   2 +-
 .../pkg/config/kube/crdclient/client_test.go  |   3 +-
 pilot/pkg/config/kube/gateway/controller.go   |   2 +-
 .../config/kube/ingress/conversion_test.go    |   4 +-
 .../config/kube/ingressv1/conversion_test.go  |   4 +-
 .../controller/autoserviceexportcontroller.go |   2 +-
 .../kube/controller/controller.go             |   2 +-
 .../kube/controller/controller_test.go        | 115 ++++--------------
 .../kube/controller/endpointslice_test.go     |   6 +-
 .../serviceregistry/kube/controller/fake.go   |   8 +-
 .../kube/controller/multicluster_test.go      |   3 +-
 .../kube/controller/namespacecontroller.go    |   2 +-
 .../kube/controller/network_test.go           |   8 +-
 .../kube/controller/pod_test.go               |  21 +---
 .../controller/serviceexportcache_test.go     |  10 +-
 .../controller/serviceimportcache_test.go     |  13 +-
 pilot/pkg/xds/fake.go                         |   5 +-
 pkg/config/analysis/local/istiod_analyze.go   |   3 +-
 pkg/config/mesh/kubemesh/watcher.go           |   5 +-
 pkg/kube/client.go                            |  57 ++++++---
 pkg/kube/configmapwatcher/configmapwatcher.go |   3 +-
 .../configmapwatcher/configmapwatcher_test.go |   3 +-
 pkg/kube/inject/watcher_test.go               |   3 +-
 pkg/kube/multicluster/secretcontroller.go     |   4 +-
 .../multicluster/secretcontroller_test.go     |   2 +-
 pkg/revisions/default_watcher.go              |   3 +-
 .../components/echo/kube/pod_controller.go    |   3 +-
 .../validation/controller/controller.go       |   2 +-
 security/pkg/k8s/chiron/controller.go         |   3 +-
 security/pkg/k8s/configutil_test.go           |   4 +-
 32 files changed, 116 insertions(+), 198 deletions(-)

diff --git a/cni/pkg/repair/repaircontroller.go b/cni/pkg/repair/repaircontroller.go
index 9540f24a7c..356b7a976c 100644
--- a/cni/pkg/repair/repaircontroller.go
+++ b/cni/pkg/repair/repaircontroller.go
@@ -26,6 +26,8 @@
 	client "k8s.io/client-go/kubernetes"
 	"k8s.io/client-go/tools/cache"
 	"k8s.io/client-go/util/workqueue"
+
+	"istio.io/istio/pkg/kube"
 )
 
 type Controller struct {
@@ -95,7 +97,7 @@ func (rc *Controller) mayAddToWorkQueue(obj interface{}) {
 
 func (rc *Controller) Run(stopCh <-chan struct{}) {
 	go rc.podController.Run(stopCh)
-	if !cache.WaitForCacheSync(stopCh, rc.podController.HasSynced) {
+	if !kube.WaitForCacheSync(stopCh, rc.podController.HasSynced) {
 		repairLog.Error("timed out waiting for pod caches to sync")
 		return
 	}
diff --git a/pilot/pkg/bootstrap/server.go b/pilot/pkg/bootstrap/server.go
index fada586ac4..8667685276 100644
--- a/pilot/pkg/bootstrap/server.go
+++ b/pilot/pkg/bootstrap/server.go
@@ -38,7 +38,6 @@
 	"google.golang.org/grpc/reflection"
 	v1 "k8s.io/client-go/kubernetes/typed/core/v1"
 	"k8s.io/client-go/rest"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/api/security/v1beta1"
 	kubecredentials "istio.io/istio/pilot/pkg/credentials/kube"
@@ -816,7 +815,7 @@ func (s *Server) addTerminatingStartFunc(fn server.Component) {
 func (s *Server) waitForCacheSync(stop <-chan struct{}) bool {
 	start := time.Now()
 	log.Info("Waiting for caches to be synced")
-	if !cache.WaitForCacheSync(stop, s.cachesSynced) {
+	if !kubelib.WaitForCacheSync(stop, s.cachesSynced) {
 		log.Errorf("Failed waiting for cache sync")
 		return false
 	}
@@ -827,7 +826,7 @@ func (s *Server) waitForCacheSync(stop <-chan struct{}) bool {
 	// condition where we are marked ready prior to updating the push context, leading to incomplete
 	// pushes.
 	expected := s.XDSServer.InboundUpdates.Load()
-	if !cache.WaitForCacheSync(stop, func() bool { return s.pushContextReady(expected) }) {
+	if !kubelib.WaitForCacheSync(stop, func() bool { return s.pushContextReady(expected) }) {
 		log.Errorf("Failed waiting for push context initialization")
 		return false
 	}
diff --git a/pilot/pkg/config/kube/crdclient/client.go b/pilot/pkg/config/kube/crdclient/client.go
index efcd88cd65..d558e31e3f 100644
--- a/pilot/pkg/config/kube/crdclient/client.go
+++ b/pilot/pkg/config/kube/crdclient/client.go
@@ -225,7 +225,7 @@ func (cl *Client) Run(stop <-chan struct{}) {
 	t0 := time.Now()
 	scope.Info("Starting Pilot K8S CRD controller")
 
-	if !cache.WaitForCacheSync(stop, cl.informerSynced) {
+	if !kube.WaitForCacheSync(stop, cl.informerSynced) {
 		scope.Error("Failed to sync Pilot K8S CRD controller cache")
 		return
 	}
diff --git a/pilot/pkg/config/kube/crdclient/client_test.go b/pilot/pkg/config/kube/crdclient/client_test.go
index b3e0d85fe9..9d14d5209f 100644
--- a/pilot/pkg/config/kube/crdclient/client_test.go
+++ b/pilot/pkg/config/kube/crdclient/client_test.go
@@ -25,7 +25,6 @@
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/types"
 	metadatafake "k8s.io/client-go/metadata/fake"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/api/meta/v1alpha1"
 	"istio.io/api/networking/v1alpha3"
@@ -51,7 +50,7 @@ func makeClient(t *testing.T, schemas collection.Schemas) (model.ConfigStoreCont
 	}
 	go config.Run(stop)
 	fake.RunAndWait(stop)
-	cache.WaitForCacheSync(stop, config.HasSynced)
+	kube.WaitForCacheSync(stop, config.HasSynced)
 	t.Cleanup(func() {
 		close(stop)
 	})
diff --git a/pilot/pkg/config/kube/gateway/controller.go b/pilot/pkg/config/kube/gateway/controller.go
index 69f2cb2de5..85caf1847a 100644
--- a/pilot/pkg/config/kube/gateway/controller.go
+++ b/pilot/pkg/config/kube/gateway/controller.go
@@ -279,7 +279,7 @@ func (c *Controller) Run(stop <-chan struct{}) {
 			gcc.Run(stop)
 		}
 	}()
-	cache.WaitForCacheSync(stop, c.namespaceInformer.HasSynced)
+	kube.WaitForCacheSync(stop, c.namespaceInformer.HasSynced)
 }
 
 func (c *Controller) SetWatchErrorHandler(handler func(r *cache.Reflector, err error)) error {
diff --git a/pilot/pkg/config/kube/ingress/conversion_test.go b/pilot/pkg/config/kube/ingress/conversion_test.go
index 9ce7063019..c833793b5d 100644
--- a/pilot/pkg/config/kube/ingress/conversion_test.go
+++ b/pilot/pkg/config/kube/ingress/conversion_test.go
@@ -33,7 +33,6 @@
 	"k8s.io/client-go/kubernetes/fake"
 	"k8s.io/client-go/kubernetes/scheme"
 	listerv1 "k8s.io/client-go/listers/core/v1"
-	"k8s.io/client-go/tools/cache"
 	"sigs.k8s.io/yaml"
 
 	meshconfig "istio.io/api/mesh/v1alpha1"
@@ -42,6 +41,7 @@
 	"istio.io/istio/pilot/test/util"
 	"istio.io/istio/pkg/config"
 	"istio.io/istio/pkg/config/mesh"
+	"istio.io/istio/pkg/kube"
 )
 
 func TestGoldenConversion(t *testing.T) {
@@ -486,6 +486,6 @@ func createFakeLister(ctx context.Context, objects ...runtime.Object) listerv1.S
 	informerFactory := informers.NewSharedInformerFactory(client, time.Hour)
 	svcInformer := informerFactory.Core().V1().Services().Informer()
 	go svcInformer.Run(ctx.Done())
-	cache.WaitForCacheSync(ctx.Done(), svcInformer.HasSynced)
+	kube.WaitForCacheSync(ctx.Done(), svcInformer.HasSynced)
 	return informerFactory.Core().V1().Services().Lister()
 }
diff --git a/pilot/pkg/config/kube/ingressv1/conversion_test.go b/pilot/pkg/config/kube/ingressv1/conversion_test.go
index 5721920ed0..e5f574859f 100644
--- a/pilot/pkg/config/kube/ingressv1/conversion_test.go
+++ b/pilot/pkg/config/kube/ingressv1/conversion_test.go
@@ -33,7 +33,6 @@
 	"k8s.io/client-go/kubernetes/fake"
 	"k8s.io/client-go/kubernetes/scheme"
 	listerv1 "k8s.io/client-go/listers/core/v1"
-	"k8s.io/client-go/tools/cache"
 	"sigs.k8s.io/yaml"
 
 	meshconfig "istio.io/api/mesh/v1alpha1"
@@ -42,6 +41,7 @@
 	"istio.io/istio/pilot/test/util"
 	"istio.io/istio/pkg/config"
 	"istio.io/istio/pkg/config/mesh"
+	"istio.io/istio/pkg/kube"
 )
 
 func TestGoldenConversion(t *testing.T) {
@@ -504,6 +504,6 @@ func createFakeLister(ctx context.Context, objects ...runtime.Object) listerv1.S
 	informerFactory := informers.NewSharedInformerFactory(client, time.Hour)
 	svcInformer := informerFactory.Core().V1().Services().Informer()
 	go svcInformer.Run(ctx.Done())
-	cache.WaitForCacheSync(ctx.Done(), svcInformer.HasSynced)
+	kube.WaitForCacheSync(ctx.Done(), svcInformer.HasSynced)
 	return informerFactory.Core().V1().Services().Lister()
 }
diff --git a/pilot/pkg/serviceregistry/kube/controller/autoserviceexportcontroller.go b/pilot/pkg/serviceregistry/kube/controller/autoserviceexportcontroller.go
index 7e4cf0620b..7969729a61 100644
--- a/pilot/pkg/serviceregistry/kube/controller/autoserviceexportcontroller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/autoserviceexportcontroller.go
@@ -106,7 +106,7 @@ func (c *autoServiceExportController) onServiceAdd(obj interface{}) {
 }
 
 func (c *autoServiceExportController) Run(stopCh <-chan struct{}) {
-	if !cache.WaitForCacheSync(stopCh, c.serviceInformer.HasSynced) {
+	if !kube.WaitForCacheSync(stopCh, c.serviceInformer.HasSynced) {
 		log.Errorf("%s failed to sync cache", c.logPrefix())
 		return
 	}
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller.go b/pilot/pkg/serviceregistry/kube/controller/controller.go
index 95ed01c03b..c3194c47b5 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller.go
@@ -826,7 +826,7 @@ func (c *Controller) Run(stop <-chan struct{}) {
 	}
 	c.informerInit.Store(true)
 
-	cache.WaitForCacheSync(stop, c.informersSynced)
+	kubelib.WaitForCacheSync(stop, c.informersSynced)
 	// after informer caches sync the first time, process resources in order
 	if err := c.SyncAll(); err != nil {
 		log.Errorf("one or more errors force-syncing resources: %v", err)
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller_test.go b/pilot/pkg/serviceregistry/kube/controller/controller_test.go
index 9292eacdee..3a1bd0cdae 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller_test.go
@@ -94,11 +94,7 @@ func TestServices(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			ctl, fx := NewFakeControllerWithOptions(FakeControllerOptions{NetworksWatcher: networksWatcher, Mode: mode})
-			go ctl.Run(ctl.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(ctl.stop, ctl.HasSynced)
-			defer ctl.Stop()
+			ctl, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{NetworksWatcher: networksWatcher, Mode: mode})
 			t.Parallel()
 			ns := "ns-test"
 
@@ -288,11 +284,8 @@ func TestController_GetPodLocality(t *testing.T) {
 			t.Parallel()
 			// Setup kube caches
 			// Pod locality only matters for Endpoints
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: EndpointsOnly})
+
 			addNodes(t, controller, tc.nodes...)
 			addPods(t, controller, fx, tc.pods...)
 
@@ -319,16 +312,13 @@ func TestGetProxyServiceInstances(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{
 				Mode:      mode,
 				ClusterID: clusterID,
 			})
 			// add a network ID to test endpoints include topology.istio.io/network label
 			controller.network = networkID
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+
 			p := generatePod("128.0.0.1", "pod1", "nsa", "foo", "node1", map[string]string{"app": "test-app"}, map[string]string{})
 			addPods(t, controller, fx, p)
 
@@ -802,11 +792,8 @@ func TestGetProxyServiceInstancesWithMultiIPsAndTargetPorts(t *testing.T) {
 			mode := mode
 			t.Run(fmt.Sprintf("%s_%s", c.name, name), func(t *testing.T) {
 				// Setup kube caches
-				controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
-				go controller.Run(controller.stop)
-				// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-				cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-				defer controller.Stop()
+				controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: mode})
+
 				addPods(t, controller, fx, c.pods...)
 
 				createServiceWithTargetPorts(controller, "svc1", "nsa",
@@ -839,11 +826,7 @@ func TestGetProxyServiceInstancesWithMultiIPsAndTargetPorts(t *testing.T) {
 }
 
 func TestGetProxyServiceInstances_WorkloadInstance(t *testing.T) {
-	ctl, fx := NewFakeControllerWithOptions(FakeControllerOptions{})
-	go ctl.Run(ctl.stop)
-	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-	cache.WaitForCacheSync(ctl.stop, ctl.HasSynced)
-	defer ctl.Stop()
+	ctl, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{})
 
 	createService(ctl, "ratings", "bookinfo-ratings",
 		map[string]string{
@@ -1075,11 +1058,7 @@ func TestController_GetIstioServiceAccounts(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: mode})
 
 			sa1 := "acct1"
 			sa2 := "acct2"
@@ -1142,11 +1121,8 @@ func TestController_Service(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: mode})
+
 			// Use a timeout to keep the test from hanging.
 
 			createService(controller, "svc1", "nsA",
@@ -1287,14 +1263,10 @@ func TestController_ServiceWithFixedDiscoveryNamespaces(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{
 				Mode:        mode,
 				MeshWatcher: meshWatcher,
 			})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
 
 			nsA := "nsA"
 			nsB := "nsB"
@@ -1454,16 +1426,12 @@ func TestController_ServiceWithChangingDiscoveryNamespaces(t *testing.T) {
 				meshWatcher.Mesh().DiscoverySelectors,
 			)
 
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{
 				Client:                    client,
 				Mode:                      mode,
 				MeshWatcher:               meshWatcher,
 				DiscoveryNamespacesFilter: discoveryNamespacesFilter,
 			})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
 
 			nsA := "nsA"
 			nsB := "nsB"
@@ -1594,11 +1562,7 @@ func TestController_ServiceWithChangingDiscoveryNamespaces(t *testing.T) {
 }
 
 func TestInstancesByPort_WorkloadInstances(t *testing.T) {
-	ctl, fx := NewFakeControllerWithOptions(FakeControllerOptions{})
-	go ctl.Run(ctl.stop)
-	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-	cache.WaitForCacheSync(ctl.stop, ctl.HasSynced)
-	defer ctl.Stop()
+	ctl, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{})
 
 	createServiceWithTargetPorts(ctl, "ratings", "bookinfo-ratings",
 		map[string]string{
@@ -1683,11 +1647,7 @@ func TestExternalNameServiceInstances(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: mode})
 			createExternalNameService(controller, "svc5", "nsA",
 				[]int32{1, 2, 3}, "foo.co", t, fx.Events)
 
@@ -1711,7 +1671,7 @@ func TestController_ExternalNameService(t *testing.T) {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			deleteWg := sync.WaitGroup{}
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{
 				Mode: mode,
 				ServiceHandler: func(_ *model.Service, e model.Event) {
 					if e == model.EventDelete {
@@ -1719,11 +1679,6 @@ func TestController_ExternalNameService(t *testing.T) {
 					}
 				},
 			})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
-			// Use a timeout to keep the test from hanging.
 
 			k8sSvcs := []*coreV1.Service{
 				createExternalNameService(controller, "svc1", "nsA",
@@ -2232,11 +2187,7 @@ func TestEndpointUpdate(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: mode})
 
 			pod1 := generatePod("128.0.0.1", "pod1", "nsA", "", "node1", map[string]string{"app": "prod-app"}, map[string]string{})
 			pods := []*coreV1.Pod{pod1}
@@ -2297,12 +2248,8 @@ func TestEndpointUpdateBeforePodUpdate(t *testing.T) {
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
-			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			// Setup kube caches
-			defer controller.Stop()
+			controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: mode})
+
 			addNodes(t, controller, generateNode("node1", map[string]string{NodeZoneLabel: "zone1", NodeRegionLabel: "region1", label.TopologySubzone.Name: "subzone1"}))
 			// Setup help functions to make the test more explicit
 			addPod := func(name, ip string) {
@@ -2451,11 +2398,7 @@ func TestEndpointUpdateBeforePodUpdate(t *testing.T) {
 }
 
 func TestWorkloadInstanceHandlerMultipleEndpoints(t *testing.T) {
-	controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{})
-	go controller.Run(controller.stop)
-	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-	cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-	defer controller.Stop()
+	controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{})
 
 	// Create an initial pod with a service, and endpoint.
 	pod1 := generatePod("172.0.1.1", "pod1", "nsA", "", "node1", map[string]string{"app": "prod-app"}, map[string]string{})
@@ -2547,11 +2490,7 @@ func TestWorkloadInstanceHandlerMultipleEndpoints(t *testing.T) {
 }
 
 func TestWorkloadInstanceHandler_WorkloadInstanceIndex(t *testing.T) {
-	ctl, _ := NewFakeControllerWithOptions(FakeControllerOptions{})
-	go ctl.Run(ctl.stop)
-	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-	cache.WaitForCacheSync(ctl.stop, ctl.HasSynced)
-	defer ctl.Stop()
+	ctl, _ := NewFakeControllerWithOptions(t, FakeControllerOptions{})
 
 	verifyGetByIP := func(address string, want []*model.WorkloadInstance) {
 		got := ctl.workloadInstancesIndex.GetByIP(address)
@@ -2642,11 +2581,7 @@ func TestKubeEndpointsControllerOnEvent(t *testing.T) {
 
 	for _, tc := range testCases {
 		t.Run(EndpointModeNames[tc.mode], func(t *testing.T) {
-			controller, _ := NewFakeControllerWithOptions(FakeControllerOptions{Mode: tc.mode})
-			go controller.Run(controller.stop)
-			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-			defer controller.Stop()
+			controller, _ := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: tc.mode})
 
 			if err := controller.endpoints.onEvent(tc.tombstone, model.EventDelete); err != nil {
 				t.Errorf("unexpected error: %v", err)
@@ -2656,11 +2591,7 @@ func TestKubeEndpointsControllerOnEvent(t *testing.T) {
 }
 
 func TestUpdateEdsCacheOnServiceUpdate(t *testing.T) {
-	controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{})
-	go controller.Run(controller.stop)
-	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-	cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-	defer controller.Stop()
+	controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{})
 
 	// Create an initial pod with a service, and endpoint.
 	pod1 := generatePod("172.0.1.1", "pod1", "nsA", "", "node1", map[string]string{"app": "prod-app"}, map[string]string{})
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go b/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
index 4af15379f9..1bec266573 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
@@ -20,7 +20,6 @@
 	"time"
 
 	coreV1 "k8s.io/api/core/v1"
-	"k8s.io/client-go/tools/cache"
 	mcs "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
@@ -77,10 +76,7 @@ func TestEndpointSliceFromMCSShouldBeIgnored(t *testing.T) {
 		appName = "prod-app"
 	)
 
-	controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointSliceOnly})
-	go controller.Run(controller.stop)
-	cache.WaitForCacheSync(controller.stop, controller.HasSynced)
-	defer controller.Stop()
+	controller, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: EndpointSliceOnly})
 
 	node := generateNode("node1", map[string]string{
 		NodeZoneLabel:              "zone1",
diff --git a/pilot/pkg/serviceregistry/kube/controller/fake.go b/pilot/pkg/serviceregistry/kube/controller/fake.go
index da8e04badb..961375c88d 100644
--- a/pilot/pkg/serviceregistry/kube/controller/fake.go
+++ b/pilot/pkg/serviceregistry/kube/controller/fake.go
@@ -174,7 +174,7 @@ type FakeController struct {
 	*Controller
 }
 
-func NewFakeControllerWithOptions(opts FakeControllerOptions) (*FakeController, *FakeXdsUpdater) {
+func NewFakeControllerWithOptions(t test.Failer, opts FakeControllerOptions) (*FakeController, *FakeXdsUpdater) {
 	xdsUpdater := opts.XDSUpdater
 	if xdsUpdater == nil {
 		xdsUpdater = NewFakeXDS()
@@ -213,12 +213,18 @@ func NewFakeControllerWithOptions(opts FakeControllerOptions) (*FakeController,
 	c.stop = opts.Stop
 	if c.stop == nil {
 		c.stop = make(chan struct{})
+		// If we created the stop, clean it up. Otherwise, caller is responsible
+		t.Cleanup(func() {
+			c.Stop()
+		})
 	}
 	opts.Client.RunAndWait(c.stop)
 	var fx *FakeXdsUpdater
 	if x, ok := xdsUpdater.(*FakeXdsUpdater); ok {
 		fx = x
 	}
+	go c.Run(c.stop)
+	kubelib.WaitForCacheSync(c.stop, c.HasSynced)
 
 	return &FakeController{c}, fx
 }
diff --git a/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go b/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go
index ac4b68d780..ba6bc8a78a 100644
--- a/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/multicluster_test.go
@@ -21,7 +21,6 @@
 
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/client-go/tools/cache"
 
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	"istio.io/istio/pilot/pkg/features"
@@ -82,7 +81,7 @@ func initController(client kube.ExtendedClient, ns string, stop <-chan struct{},
 	sc := multicluster.NewController(client, ns, "cluster-1")
 	sc.AddHandler(mc)
 	_ = sc.Run(stop)
-	cache.WaitForCacheSync(stop, sc.HasSynced)
+	kube.WaitForCacheSync(stop, sc.HasSynced)
 }
 
 func Test_KubeSecretController(t *testing.T) {
diff --git a/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go b/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go
index e29b06f012..cfaf36f4af 100644
--- a/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go
@@ -82,7 +82,7 @@ func NewNamespaceController(kubeClient kube.Client, caBundleWatcher *keycertbund
 
 // Run starts the NamespaceController until a value is sent to stopCh.
 func (nc *NamespaceController) Run(stopCh <-chan struct{}) {
-	if !cache.WaitForCacheSync(stopCh, nc.namespacesInformer.HasSynced, nc.configMapInformer.HasSynced) {
+	if !kube.WaitForCacheSync(stopCh, nc.namespacesInformer.HasSynced, nc.configMapInformer.HasSynced) {
 		log.Error("Failed to sync namespace controller cache")
 		return
 	}
diff --git a/pilot/pkg/serviceregistry/kube/controller/network_test.go b/pilot/pkg/serviceregistry/kube/controller/network_test.go
index 1da3389ff5..58d570f9f5 100644
--- a/pilot/pkg/serviceregistry/kube/controller/network_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/network_test.go
@@ -35,11 +35,7 @@
 
 func TestNetworkUpdateTriggers(t *testing.T) {
 	meshNetworks := mesh.NewFixedNetworksWatcher(nil)
-	c, _ := NewFakeControllerWithOptions(FakeControllerOptions{ClusterID: "Kubernetes", NetworksWatcher: meshNetworks, DomainSuffix: "cluster.local"})
-	defer close(c.stop)
-	go func() {
-		c.Run(c.stop)
-	}()
+	c, _ := NewFakeControllerWithOptions(t, FakeControllerOptions{ClusterID: "Kubernetes", NetworksWatcher: meshNetworks, DomainSuffix: "cluster.local"})
 
 	if len(c.NetworkGateways()) != 0 {
 		t.Fatal("did not expect any gateways yet")
@@ -76,7 +72,7 @@ func TestNetworkUpdateTriggers(t *testing.T) {
 				return fmt.Errorf("expected %d gateways but got %d", expectedGws, n)
 			}
 			return nil
-		}, retry.Timeout(5*time.Second), retry.Delay(500*time.Millisecond))
+		}, retry.Timeout(5*time.Second), retry.Delay(10*time.Millisecond))
 	}
 
 	t.Run("add meshnetworks", func(t *testing.T) {
diff --git a/pilot/pkg/serviceregistry/kube/controller/pod_test.go b/pilot/pkg/serviceregistry/kube/controller/pod_test.go
index dc8bc59c7f..2042c3da09 100644
--- a/pilot/pkg/serviceregistry/kube/controller/pod_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/pod_test.go
@@ -24,7 +24,6 @@
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/util/wait"
 	"k8s.io/client-go/kubernetes"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pkg/config/labels"
@@ -109,10 +108,7 @@ func TestPodCache(t *testing.T) {
 }
 
 func TestHostNetworkPod(t *testing.T) {
-	c, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
-	go c.Run(c.stop)
-	cache.WaitForCacheSync(c.stop, c.HasSynced)
-	defer c.Stop()
+	c, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: EndpointsOnly})
 	initTestEnv(t, c.client, fx)
 	createPod := func(ip, name string) {
 		addPods(t, c, fx, generatePod(ip, name, "ns", "1", "", map[string]string{}, map[string]string{}))
@@ -136,10 +132,7 @@ func TestHostNetworkPod(t *testing.T) {
 
 // Regression test for https://github.com/istio/istio/issues/20676
 func TestIPReuse(t *testing.T) {
-	c, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
-	go c.Run(c.stop)
-	cache.WaitForCacheSync(c.stop, c.HasSynced)
-	defer c.Stop()
+	c, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: EndpointsOnly})
 	initTestEnv(t, c.client, fx)
 
 	createPod := func(ip, name string) {
@@ -205,13 +198,10 @@ func waitForNode(c *FakeController, name string) error {
 }
 
 func testPodCache(t *testing.T) {
-	c, fx := NewFakeControllerWithOptions(FakeControllerOptions{
+	c, fx := NewFakeControllerWithOptions(t, FakeControllerOptions{
 		Mode:              EndpointsOnly,
 		WatchedNamespaces: "nsa,nsb",
 	})
-	go c.Run(c.stop)
-	cache.WaitForCacheSync(c.stop, c.HasSynced)
-	defer c.Stop()
 
 	initTestEnv(t, c.client, fx)
 
@@ -258,10 +248,7 @@ func testPodCache(t *testing.T) {
 // Checks that events from the watcher create the proper internal structures
 func TestPodCacheEvents(t *testing.T) {
 	t.Parallel()
-	c, _ := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
-	go c.Run(c.stop)
-	cache.WaitForCacheSync(c.stop, c.HasSynced)
-	defer c.Stop()
+	c, _ := NewFakeControllerWithOptions(t, FakeControllerOptions{Mode: EndpointsOnly})
 
 	ns := "default"
 	podCache := c.pods
diff --git a/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go b/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go
index a6b9bd0341..4a4955df78 100644
--- a/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go
@@ -24,7 +24,6 @@
 	v12 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
 	"k8s.io/apimachinery/pkg/types"
-	"k8s.io/client-go/tools/cache"
 	mcsapi "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
@@ -134,20 +133,13 @@ func newServiceExport() *unstructured.Unstructured {
 func newTestServiceExportCache(t *testing.T, clusterLocalMode ClusterLocalMode, endpointMode EndpointMode) (ec *serviceExportCacheImpl) {
 	t.Helper()
 
-	stopCh := make(chan struct{})
 	istiotest.SetBoolForTest(t, &features.EnableMCSServiceDiscovery, true)
 	istiotest.SetBoolForTest(t, &features.EnableMCSClusterLocal, clusterLocalMode == alwaysClusterLocal)
-	t.Cleanup(func() {
-		close(stopCh)
-	})
 
-	c, _ := NewFakeControllerWithOptions(FakeControllerOptions{
-		Stop:      stopCh,
+	c, _ := NewFakeControllerWithOptions(t, FakeControllerOptions{
 		ClusterID: testCluster,
 		Mode:      endpointMode,
 	})
-	go c.Run(c.stop)
-	cache.WaitForCacheSync(c.stop, c.HasSynced)
 
 	// Create the test service and endpoints.
 	createService(c, serviceExportName, serviceExportNamespace, map[string]string{},
diff --git a/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go b/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go
index f44a4e306d..a652946dee 100644
--- a/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go
@@ -28,7 +28,6 @@
 	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
 	"k8s.io/apimachinery/pkg/runtime"
 	"k8s.io/apimachinery/pkg/types"
-	"k8s.io/client-go/tools/cache"
 	mcsapi "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
@@ -136,13 +135,10 @@ func TestDeleteImportedService(t *testing.T) {
 			c1, ic := newTestServiceImportCache(t, mode)
 
 			// Create and run another controller.
-			c2, _ := NewFakeControllerWithOptions(FakeControllerOptions{
-				Stop:      c1.stop,
+			c2, _ := NewFakeControllerWithOptions(t, FakeControllerOptions{
 				ClusterID: "test-cluster2",
 				Mode:      mode,
 			})
-			go c2.Run(c2.stop)
-			cache.WaitForCacheSync(c2.stop, c2.HasSynced)
 
 			c1.opts.MeshServiceController.AddRegistryAndRun(c2, c2.stop)
 
@@ -207,19 +203,14 @@ func TestUpdateServiceImportVIPs(t *testing.T) {
 }
 
 func newTestServiceImportCache(t test.Failer, mode EndpointMode) (c *FakeController, ic *serviceImportCacheImpl) {
-	stopCh := make(chan struct{})
 	test.SetBoolForTest(t, &features.EnableMCSHost, true)
 	t.Cleanup(func() {
-		close(stopCh)
 	})
 
-	c, _ = NewFakeControllerWithOptions(FakeControllerOptions{
-		Stop:      stopCh,
+	c, _ = NewFakeControllerWithOptions(t, FakeControllerOptions{
 		ClusterID: serviceImportCluster,
 		Mode:      mode,
 	})
-	go c.Run(c.stop)
-	cache.WaitForCacheSync(c.stop, c.HasSynced)
 
 	ic = c.imports.(*serviceImportCacheImpl)
 	return
diff --git a/pilot/pkg/xds/fake.go b/pilot/pkg/xds/fake.go
index c7cbc400dc..76f0c83142 100644
--- a/pilot/pkg/xds/fake.go
+++ b/pilot/pkg/xds/fake.go
@@ -32,7 +32,6 @@
 	"k8s.io/apimachinery/pkg/runtime"
 	"k8s.io/client-go/kubernetes/fake"
 	"k8s.io/client-go/kubernetes/scheme"
-	"k8s.io/client-go/tools/cache"
 
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	"istio.io/istio/pilot/pkg/config/kube/gateway"
@@ -180,7 +179,7 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 		if opts.KubeClientModifier != nil {
 			opts.KubeClientModifier(client)
 		}
-		k8s, _ := kube.NewFakeControllerWithOptions(kube.FakeControllerOptions{
+		k8s, _ := kube.NewFakeControllerWithOptions(t, kube.FakeControllerOptions{
 			ServiceHandler:  serviceHandler,
 			Client:          client,
 			ClusterID:       k8sCluster,
@@ -320,7 +319,7 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 	cg.ServiceEntryRegistry.XdsUpdater = s
 	// Now that handlers are added, get everything started
 	cg.Run()
-	cache.WaitForCacheSync(stop,
+	kubelib.WaitForCacheSync(stop,
 		cg.Registry.HasSynced,
 		cg.Store().HasSynced)
 	cg.ServiceEntryRegistry.ResyncEDS()
diff --git a/pkg/config/analysis/local/istiod_analyze.go b/pkg/config/analysis/local/istiod_analyze.go
index a042b80f9a..7144aaaa87 100644
--- a/pkg/config/analysis/local/istiod_analyze.go
+++ b/pkg/config/analysis/local/istiod_analyze.go
@@ -127,8 +127,7 @@ func (sa *IstiodAnalyzer) ReAnalyze(cancel <-chan struct{}) (AnalysisResult, err
 	result.ExecutedAnalyzers = sa.analyzer.AnalyzerNames()
 	result.SkippedAnalyzers = sa.analyzer.RemoveSkipped(store.Schemas())
 
-	cache.WaitForCacheSync(cancel,
-		store.HasSynced)
+	kubelib.WaitForCacheSync(cancel, store.HasSynced)
 
 	ctx := NewContext(store, cancel, sa.collectionReporter)
 
diff --git a/pkg/config/mesh/kubemesh/watcher.go b/pkg/config/mesh/kubemesh/watcher.go
index aadf8fc6e6..a684578e69 100644
--- a/pkg/config/mesh/kubemesh/watcher.go
+++ b/pkg/config/mesh/kubemesh/watcher.go
@@ -18,7 +18,6 @@
 	"fmt"
 
 	v1 "k8s.io/api/core/v1"
-	"k8s.io/client-go/tools/cache"
 
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	"istio.io/istio/pkg/config/mesh"
@@ -58,7 +57,7 @@ func NewConfigMapWatcher(client kube.Client, namespace, name, key string, multiW
 	go c.Run(stop)
 
 	// Ensure the ConfigMap is initially loaded if present.
-	if !cache.WaitForCacheSync(stop, c.HasSynced) {
+	if !kube.WaitForCacheSync(stop, c.HasSynced) {
 		log.Error("failed to wait for cache sync")
 	}
 	return w
@@ -71,7 +70,7 @@ func AddUserMeshConfig(client kube.Client, watcher mesh.Watcher, namespace, key,
 	})
 
 	go c.Run(stop)
-	if !cache.WaitForCacheSync(stop, c.HasSynced) {
+	if !kube.WaitForCacheSync(stop, c.HasSynced) {
 		log.Error("failed to wait for cache sync")
 	}
 }
diff --git a/pkg/kube/client.go b/pkg/kube/client.go
index 20d17c3347..50302e2615 100644
--- a/pkg/kube/client.go
+++ b/pkg/kube/client.go
@@ -574,6 +574,48 @@ func fastWaitForCacheSync(stop <-chan struct{}, informerFactory reflectInformerS
 	})
 }
 
+// WaitForCacheSync waits until all caches are synced. This will return true only if things synced
+// successfully before the stop channel is closed. This function also lives in the Kubernetes cache
+// library. However, that library will poll with 100ms fixed interval. Often the cache syncs in a few
+// ms, but we are delayed a full 100ms. This is especially apparent in tests, which previously spent
+// most of their time just in the 100ms wait interval.
+//
+// To optimize this, this function performs exponential backoff. This is generally safe because
+// cache.InformerSynced functions are ~always quick to run. However, if the sync functions do perform
+// expensive checks this function may not be suitable.
+func WaitForCacheSync(stop <-chan struct{}, cacheSyncs ...cache.InformerSynced) bool {
+	max := time.Millisecond * 100
+	delay := time.Millisecond
+	f := func() bool {
+		for _, syncFunc := range cacheSyncs {
+			if !syncFunc() {
+				return false
+			}
+		}
+		return true
+	}
+	for {
+		select {
+		case <-stop:
+			return false
+		default:
+		}
+		res := f()
+		if res {
+			return true
+		}
+		delay *= 2
+		if delay > max {
+			delay = max
+		}
+		select {
+		case <-stop:
+			return false
+		case <-time.After(delay):
+		}
+	}
+}
+
 func fastWaitForCacheSyncDynamic(stop <-chan struct{}, informerFactory dynamicInformerSync) {
 	returnImmediately := make(chan struct{})
 	close(returnImmediately)
@@ -592,21 +634,6 @@ func fastWaitForCacheSyncDynamic(stop <-chan struct{}, informerFactory dynamicIn
 	})
 }
 
-// WaitForCacheSyncInterval waits for caches to populate, with explicitly configured interval
-func WaitForCacheSyncInterval(stopCh <-chan struct{}, interval time.Duration, cacheSyncs ...cache.InformerSynced) bool {
-	err := wait.PollImmediateUntil(interval,
-		func() (bool, error) {
-			for _, syncFunc := range cacheSyncs {
-				if !syncFunc() {
-					return false, nil
-				}
-			}
-			return true, nil
-		},
-		stopCh)
-	return err == nil
-}
-
 func (c *client) Revision() string {
 	return c.revision
 }
diff --git a/pkg/kube/configmapwatcher/configmapwatcher.go b/pkg/kube/configmapwatcher/configmapwatcher.go
index 550f4c0e5d..8a82abb421 100644
--- a/pkg/kube/configmapwatcher/configmapwatcher.go
+++ b/pkg/kube/configmapwatcher/configmapwatcher.go
@@ -26,7 +26,6 @@
 	"k8s.io/apimachinery/pkg/types"
 	"k8s.io/client-go/informers"
 	informersv1 "k8s.io/client-go/informers/core/v1"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pkg/kube"
 	"istio.io/istio/pkg/kube/controllers"
@@ -74,7 +73,7 @@ func NewController(client kube.Client, namespace, name string, callback func(*v1
 
 func (c *Controller) Run(stop <-chan struct{}) {
 	go c.informer.Informer().Run(stop)
-	if !cache.WaitForCacheSync(stop, c.informer.Informer().HasSynced) {
+	if !kube.WaitForCacheSync(stop, c.informer.Informer().HasSynced) {
 		log.Error("failed to wait for cache sync")
 		return
 	}
diff --git a/pkg/kube/configmapwatcher/configmapwatcher_test.go b/pkg/kube/configmapwatcher/configmapwatcher_test.go
index d74474163e..1249a785b0 100644
--- a/pkg/kube/configmapwatcher/configmapwatcher_test.go
+++ b/pkg/kube/configmapwatcher/configmapwatcher_test.go
@@ -24,7 +24,6 @@
 	. "github.com/onsi/gomega"
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pkg/kube"
 )
@@ -100,7 +99,7 @@ func Test_ConfigMapWatcher(t *testing.T) {
 	stop := make(chan struct{})
 	c := NewController(client, configMapNamespace, configMapName, callback)
 	go c.Run(stop)
-	cache.WaitForCacheSync(stop, c.HasSynced)
+	kube.WaitForCacheSync(stop, c.HasSynced)
 
 	cms := client.Kube().CoreV1().ConfigMaps(configMapNamespace)
 	for i, step := range steps {
diff --git a/pkg/kube/inject/watcher_test.go b/pkg/kube/inject/watcher_test.go
index 68d7561099..abec8e2b35 100644
--- a/pkg/kube/inject/watcher_test.go
+++ b/pkg/kube/inject/watcher_test.go
@@ -24,7 +24,6 @@
 	. "github.com/onsi/gomega"
 	v1 "k8s.io/api/core/v1"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pkg/kube"
 )
@@ -86,7 +85,7 @@ func TestNewConfigMapWatcher(t *testing.T) {
 	stop := make(chan struct{})
 	go w.Run(stop)
 	controller := w.(*configMapWatcher).c
-	cache.WaitForCacheSync(stop, controller.HasSynced)
+	kube.WaitForCacheSync(stop, controller.HasSynced)
 	client.RunAndWait(stop)
 
 	cms := client.Kube().CoreV1().ConfigMaps(namespace)
diff --git a/pkg/kube/multicluster/secretcontroller.go b/pkg/kube/multicluster/secretcontroller.go
index f7fbecb898..1d8ebd7343 100644
--- a/pkg/kube/multicluster/secretcontroller.go
+++ b/pkg/kube/multicluster/secretcontroller.go
@@ -90,7 +90,6 @@ type Controller struct {
 	handlers []ClusterHandler
 
 	once              sync.Once
-	syncInterval      time.Duration
 	remoteSyncTimeout atomic.Bool
 }
 
@@ -264,7 +263,6 @@ func NewController(kubeclientset kube.Client, namespace string, localClusterID c
 		localClusterClient: kubeclientset,
 		cs:                 newClustersStore(),
 		informer:           secretsInformer,
-		syncInterval:       100 * time.Millisecond,
 	}
 	controller.queue = controllers.NewQueue("multicluster secret", controllers.WithReconciler(controller.processItem))
 
@@ -286,7 +284,7 @@ func (c *Controller) Run(stopCh <-chan struct{}) error {
 
 		go c.informer.Run(stopCh)
 
-		if !kube.WaitForCacheSyncInterval(stopCh, c.syncInterval, c.informer.HasSynced) {
+		if !kube.WaitForCacheSync(stopCh, c.informer.HasSynced) {
 			log.Error("Failed to sync multicluster remote secrets controller cache")
 			return
 		}
diff --git a/pkg/kube/multicluster/secretcontroller_test.go b/pkg/kube/multicluster/secretcontroller_test.go
index 5cd747e70b..51ef3db053 100644
--- a/pkg/kube/multicluster/secretcontroller_test.go
+++ b/pkg/kube/multicluster/secretcontroller_test.go
@@ -147,7 +147,7 @@ func Test_SecretController(t *testing.T) {
 	t.Run("sync timeout", func(t *testing.T) {
 		retry.UntilOrFail(t, c.HasSynced, retry.Timeout(2*time.Second))
 	})
-	kube.WaitForCacheSyncInterval(stopCh, time.Microsecond, c.informer.HasSynced)
+	kube.WaitForCacheSync(stopCh, c.informer.HasSynced)
 	clientset.RunAndWait(stopCh)
 
 	for i, step := range steps {
diff --git a/pkg/revisions/default_watcher.go b/pkg/revisions/default_watcher.go
index 308bdbe35f..691b98b320 100644
--- a/pkg/revisions/default_watcher.go
+++ b/pkg/revisions/default_watcher.go
@@ -16,7 +16,6 @@
 
 import (
 	"sync"
-	"time"
 
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/types"
@@ -67,7 +66,7 @@ func NewDefaultWatcher(client kube.Client, revision string) DefaultWatcher {
 }
 
 func (p *defaultWatcher) Run(stopCh <-chan struct{}) {
-	if !kube.WaitForCacheSyncInterval(stopCh, time.Second, p.webhookInformer.HasSynced) {
+	if !kube.WaitForCacheSync(stopCh, p.webhookInformer.HasSynced) {
 		log.Errorf("failed to sync default watcher")
 		return
 	}
diff --git a/pkg/test/framework/components/echo/kube/pod_controller.go b/pkg/test/framework/components/echo/kube/pod_controller.go
index fb4c7b5192..a669935c73 100644
--- a/pkg/test/framework/components/echo/kube/pod_controller.go
+++ b/pkg/test/framework/components/echo/kube/pod_controller.go
@@ -21,6 +21,7 @@
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/client-go/tools/cache"
 
+	"istio.io/istio/pkg/kube"
 	"istio.io/istio/pkg/queue"
 	"istio.io/istio/pkg/test/framework/components/echo"
 	"istio.io/pkg/log"
@@ -98,7 +99,7 @@ func(options *metav1.ListOptions) {
 
 func (c *podController) Run(stop <-chan struct{}) {
 	go c.informer.Run(stop)
-	cache.WaitForCacheSync(stop, c.HasSynced)
+	kube.WaitForCacheSync(stop, c.HasSynced)
 	go c.q.Run(stop)
 }
 
diff --git a/pkg/webhooks/validation/controller/controller.go b/pkg/webhooks/validation/controller/controller.go
index f8c3da61ab..c1cfb9e066 100644
--- a/pkg/webhooks/validation/controller/controller.go
+++ b/pkg/webhooks/validation/controller/controller.go
@@ -218,7 +218,7 @@ func newController(
 func (c *Controller) Run(stop <-chan struct{}) {
 	defer c.queue.ShutDown()
 	go c.webhookInformer.Run(stop)
-	if !cache.WaitForCacheSync(stop, c.webhookInformer.HasSynced) {
+	if !kube.WaitForCacheSync(stop, c.webhookInformer.HasSynced) {
 		return
 	}
 	go c.startCaBundleWatcher(stop)
diff --git a/security/pkg/k8s/chiron/controller.go b/security/pkg/k8s/chiron/controller.go
index ea0a4e65b1..344f820168 100644
--- a/security/pkg/k8s/chiron/controller.go
+++ b/security/pkg/k8s/chiron/controller.go
@@ -30,6 +30,7 @@
 	clientset "k8s.io/client-go/kubernetes"
 	"k8s.io/client-go/tools/cache"
 
+	"istio.io/istio/pkg/kube"
 	"istio.io/istio/security/pkg/pki/ca"
 	"istio.io/istio/security/pkg/pki/util"
 	certutil "istio.io/istio/security/pkg/util"
@@ -162,7 +163,7 @@ func (wc *WebhookController) Run(stopCh <-chan struct{}) {
 		// upsertSecret to update and insert secret
 		// it throws error if the secret cache is not synchronized, but the secret exists in the system.
 		// Hence waiting for the cache is synced.
-		if !cache.WaitForCacheSync(stopCh, wc.scrtController.HasSynced) {
+		if !kube.WaitForCacheSync(stopCh, wc.scrtController.HasSynced) {
 			log.Error("failed to wait for cache sync")
 		}
 	}
diff --git a/security/pkg/k8s/configutil_test.go b/security/pkg/k8s/configutil_test.go
index a3704b5ff3..7b660c0122 100644
--- a/security/pkg/k8s/configutil_test.go
+++ b/security/pkg/k8s/configutil_test.go
@@ -30,9 +30,9 @@
 	informersv1 "k8s.io/client-go/informers/core/v1"
 	"k8s.io/client-go/kubernetes/fake"
 	ktesting "k8s.io/client-go/testing"
-	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pkg/config/constants"
+	"istio.io/istio/pkg/kube"
 )
 
 const (
@@ -303,6 +303,6 @@ func createFakeLister(kubeClient *fake.Clientset) informersv1.ConfigMapInformer
 	informerFactory := informers.NewSharedInformerFactory(kubeClient, time.Second)
 	configmapInformer := informerFactory.Core().V1().ConfigMaps().Informer()
 	go configmapInformer.Run(ctx.Done())
-	cache.WaitForCacheSync(ctx.Done(), configmapInformer.HasSynced)
+	kube.WaitForCacheSync(ctx.Done(), configmapInformer.HasSynced)
 	return informerFactory.Core().V1().ConfigMaps()
 }
-- 
2.35.3

