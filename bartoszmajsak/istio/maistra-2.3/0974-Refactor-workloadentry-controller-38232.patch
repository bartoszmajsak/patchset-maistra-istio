From 72aa6cb678025fb34a82ab21e0a68df80f4aa25c Mon Sep 17 00:00:00 2001
From: Zhonghu Xu <xuzhonghu@huawei.com>
Date: Wed, 6 Apr 2022 10:12:25 +0800
Subject: Refactor workloadentry controller  (#38232)

* Reafctor workloadentry controller to use the generic queue insteadd of k8s lib

* update

* update
---
 .../kube/gateway/deploymentcontroller.go      |   3 +-
 pilot/pkg/config/kube/gateway/gatewayclass.go |   2 +-
 pilot/pkg/config/kube/ingress/controller.go   |   3 +-
 pilot/pkg/config/kube/ingressv1/controller.go |   3 +-
 .../workloadentry/workloadentry_controller.go | 135 +++++-------------
 .../kube/controller/namespacecontroller.go    |   3 +-
 pkg/kube/configmapwatcher/configmapwatcher.go |   3 +-
 pkg/kube/controllers/queue.go                 |   8 +-
 pkg/kube/controllers/queue_test.go            |   2 +-
 pkg/kube/multicluster/secretcontroller.go     |   3 +-
 pkg/revisions/default_watcher.go              |   3 +-
 pkg/webhooks/webhookpatch.go                  |   3 +-
 12 files changed, 58 insertions(+), 113 deletions(-)

diff --git a/pilot/pkg/config/kube/gateway/deploymentcontroller.go b/pilot/pkg/config/kube/gateway/deploymentcontroller.go
index 5bb548f73d..4807e0aa23 100644
--- a/pilot/pkg/config/kube/gateway/deploymentcontroller.go
+++ b/pilot/pkg/config/kube/gateway/deploymentcontroller.go
@@ -143,7 +143,8 @@ func (d *DeploymentController) Run(stop <-chan struct{}) {
 }
 
 // Reconcile takes in the name of a Gateway and ensures the cluster is in the desired state
-func (d *DeploymentController) Reconcile(req types.NamespacedName) error {
+func (d *DeploymentController) Reconcile(key interface{}) error {
+	req := key.(types.NamespacedName)
 	log := log.WithLabels("gateway", req)
 
 	gw, err := d.gatewayLister.Gateways(req.Namespace).Get(req.Name)
diff --git a/pilot/pkg/config/kube/gateway/gatewayclass.go b/pilot/pkg/config/kube/gateway/gatewayclass.go
index e7cc2d3357..9dc5284a5e 100644
--- a/pilot/pkg/config/kube/gateway/gatewayclass.go
+++ b/pilot/pkg/config/kube/gateway/gatewayclass.go
@@ -62,7 +62,7 @@ func (c *ClassController) Run(stop <-chan struct{}) {
 	c.queue.Run(stop)
 }
 
-func (c *ClassController) Reconcile(_ types.NamespacedName) error {
+func (c *ClassController) Reconcile(_ interface{}) error {
 	_, err := c.classes.Get(DefaultClassName)
 	if err := controllers.IgnoreNotFound(err); err != nil {
 		log.Errorf("unable to fetch GatewayClass: %v", err)
diff --git a/pilot/pkg/config/kube/ingress/controller.go b/pilot/pkg/config/kube/ingress/controller.go
index 72f7f49b17..93247aca89 100644
--- a/pilot/pkg/config/kube/ingress/controller.go
+++ b/pilot/pkg/config/kube/ingress/controller.go
@@ -222,7 +222,8 @@ func (c *controller) shouldProcessIngressUpdate(ing *ingress.Ingress) (bool, err
 	return preProcessed, nil
 }
 
-func (c *controller) onEvent(item types.NamespacedName) error {
+func (c *controller) onEvent(key interface{}) error {
+	item := key.(types.NamespacedName)
 	event := model.EventUpdate
 	ing, err := c.ingressLister.Ingresses(item.Namespace).Get(item.Name)
 	if err != nil {
diff --git a/pilot/pkg/config/kube/ingressv1/controller.go b/pilot/pkg/config/kube/ingressv1/controller.go
index 82d480441e..ceeea7bf5c 100644
--- a/pilot/pkg/config/kube/ingressv1/controller.go
+++ b/pilot/pkg/config/kube/ingressv1/controller.go
@@ -175,7 +175,8 @@ func (c *controller) shouldProcessIngressUpdate(ing *knetworking.Ingress) (bool,
 	return preProcessed, nil
 }
 
-func (c *controller) onEvent(item types.NamespacedName) error {
+func (c *controller) onEvent(key interface{}) error {
+	item := key.(types.NamespacedName)
 	event := model.EventUpdate
 	ing, err := c.ingressLister.Ingresses(item.Namespace).Get(item.Name)
 	if err != nil {
diff --git a/pilot/pkg/controller/workloadentry/workloadentry_controller.go b/pilot/pkg/controller/workloadentry/workloadentry_controller.go
index b71951be0f..32c721b70b 100644
--- a/pilot/pkg/controller/workloadentry/workloadentry_controller.go
+++ b/pilot/pkg/controller/workloadentry/workloadentry_controller.go
@@ -22,7 +22,6 @@
 	"sync"
 	"time"
 
-	"github.com/cenkalti/backoff/v4"
 	"golang.org/x/time/rate"
 	"google.golang.org/grpc/codes"
 	grpcstatus "google.golang.org/grpc/status"
@@ -30,9 +29,6 @@
 	"k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	kubetypes "k8s.io/apimachinery/pkg/types"
-	"k8s.io/apimachinery/pkg/util/wait"
-	"k8s.io/client-go/tools/cache"
-	"k8s.io/client-go/util/workqueue"
 
 	"istio.io/api/meta/v1alpha1"
 	"istio.io/api/networking/v1alpha3"
@@ -42,6 +38,7 @@
 	"istio.io/istio/pilot/pkg/networking/util"
 	"istio.io/istio/pkg/config"
 	"istio.io/istio/pkg/config/schema/gvk"
+	"istio.io/istio/pkg/kube/controllers"
 	"istio.io/istio/pkg/queue"
 	istiolog "istio.io/pkg/log"
 	"istio.io/pkg/monitoring"
@@ -101,8 +98,6 @@ func init() {
 	//
 	// 5ms, 10ms, 20ms, 40ms, 80ms, 160ms, 320ms, 640ms, 1.3s, 2.6s, 5.1s, 10.2s, 20.4s, 41s, 82s
 	maxRetries = 15
-
-	workerNum = 5
 )
 
 type HealthEvent struct {
@@ -118,11 +113,6 @@ type HealthCondition struct {
 	condition *v1alpha1.IstioCondition
 }
 
-var keyFunc = func(obj interface{}) (string, error) {
-	condition := obj.(HealthCondition)
-	return makeProxyKey(condition.proxy), nil
-}
-
 var log = istiolog.RegisterScope("wle", "wle controller debugging", 0)
 
 type Controller struct {
@@ -137,7 +127,7 @@ type Controller struct {
 	// cleanup is to delete the workload entry
 
 	// queue contains workloadEntry that need to be unregistered
-	queue workqueue.RateLimitingInterface
+	queue controllers.Queue
 	// cleanupLimit rate limit's auto registered WorkloadEntry cleanup calls to k8s
 	cleanupLimit *rate.Limiter
 	// cleanupQueue delays the cleanup of auto registered WorkloadEntries to allow for grace period
@@ -153,7 +143,7 @@ type Controller struct {
 	maxConnectionAge time.Duration
 
 	// healthCondition is a fifo queue used for updating health check status
-	healthCondition cache.Queue
+	healthCondition controllers.Queue
 }
 
 type HealthStatus = v1alpha1.IstioCondition
@@ -166,16 +156,21 @@ func NewController(store model.ConfigStoreCache, instanceID string, maxConnAge t
 		if maxConnAge < 0 {
 			maxConnAge = time.Duration(math.MaxInt64)
 		}
-		return &Controller{
+		c := &Controller{
 			instanceID:       instanceID,
 			store:            store,
 			cleanupLimit:     rate.NewLimiter(rate.Limit(20), 1),
 			cleanupQueue:     queue.NewDelayed(),
-			queue:            workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),
 			adsConnections:   map[string]uint8{},
 			maxConnectionAge: maxConnAge,
-			healthCondition:  cache.NewFIFO(keyFunc),
 		}
+		c.queue = controllers.NewQueue("unregister_workloadentry",
+			controllers.WithMaxAttempts(maxRetries),
+			controllers.WithReconciler(c.unregisterWorkload))
+		c.healthCondition = controllers.NewQueue("healthcheck",
+			controllers.WithMaxAttempts(maxRetries),
+			controllers.WithReconciler(c.updateWorkloadEntryHealth))
+		return c
 	}
 	return nil
 }
@@ -189,38 +184,9 @@ func (c *Controller) Run(stop <-chan struct{}) {
 		go c.cleanupQueue.Run(stop)
 	}
 
-	for i := 0; i < workerNum; i++ {
-		go wait.Until(c.worker, time.Second, stop)
-	}
-
-	// start a new go routine updating health status
-	go func() {
-		b := backoff.NewExponentialBackOff()
-		for {
-			obj, err := c.healthCondition.Pop(c.updateWorkloadEntryHealth)
-			if err != nil {
-				if err == cache.ErrFIFOClosed {
-					return
-				}
-				log.Errorf(err)
-				next := b.NextBackOff()
-				time.AfterFunc(next, func() {
-					_ = c.healthCondition.AddIfNotPresent(obj)
-				})
-			} else {
-				b.Reset()
-			}
-		}
-	}()
-
+	go c.queue.Run(stop)
+	go c.healthCondition.Run(stop)
 	<-stop
-	c.queue.ShutDown()
-	c.healthCondition.Close()
-}
-
-func (c *Controller) worker() {
-	for c.processNextWorkItem() {
-	}
 }
 
 // workItem contains the state of a "disconnect" event used to unregister a workload.
@@ -231,23 +197,6 @@ type workItem struct {
 	origConTime time.Time
 }
 
-func (c *Controller) processNextWorkItem() bool {
-	item, quit := c.queue.Get()
-	if quit {
-		return false
-	}
-	defer c.queue.Done(item)
-
-	workItem, ok := item.(*workItem)
-	if !ok {
-		return true
-	}
-
-	err := c.unregisterWorkload(workItem.entryName, workItem.proxy, workItem.disConTime, workItem.origConTime)
-	c.handleErr(err, item)
-	return true
-}
-
 func setConnectMeta(c *config.Config, controller string, conTime time.Time) {
 	c.Annotations[WorkloadControllerAnnotation] = controller
 	c.Annotations[ConnectedAtAnnotation] = conTime.Format(timeFormat)
@@ -341,30 +290,36 @@ func (c *Controller) QueueUnregisterWorkload(proxy *model.Proxy, origConnect tim
 	delete(c.adsConnections, makeProxyKey(proxy))
 	c.mutex.Unlock()
 
-	disconTime := time.Now()
-	if err := c.unregisterWorkload(entryName, proxy, disconTime, origConnect); err != nil {
+	workload := &workItem{
+		entryName:   entryName,
+		proxy:       proxy,
+		disConTime:  time.Now(),
+		origConTime: origConnect,
+	}
+	if err := c.unregisterWorkload(workload); err != nil {
 		log.Errorf(err)
-		c.queue.AddRateLimited(&workItem{
-			entryName:   entryName,
-			proxy:       proxy,
-			disConTime:  disconTime,
-			origConTime: origConnect,
-		})
+		c.queue.Add(workload)
 	}
 }
 
-func (c *Controller) unregisterWorkload(entryName string, proxy *model.Proxy, disconTime, origConnTime time.Time) error {
+func (c *Controller) unregisterWorkload(item interface{}) error {
+	workItem, ok := item.(*workItem)
+	if !ok {
+		return nil
+	}
+
 	// unset controller, set disconnect time
-	cfg := c.store.Get(gvk.WorkloadEntry, entryName, proxy.Metadata.Namespace)
+	cfg := c.store.Get(gvk.WorkloadEntry, workItem.entryName, workItem.proxy.Metadata.Namespace)
 	if cfg == nil {
 		// return error and backoff retry to prevent workloadentry leak
 		// TODO(@hzxuzhonghu): update the Get interface, fallback to calling apiserver.
-		return fmt.Errorf("workloadentry %s/%s is not found, maybe deleted or because of propagate latency", proxy.Metadata.Namespace, entryName)
+		return fmt.Errorf("workloadentry %s/%s is not found, maybe deleted or because of propagate latency",
+			workItem.proxy.Metadata.Namespace, workItem.entryName)
 	}
 
 	// only queue a delete if this disconnect event is associated with the last connect event written to the worload entry
 	if mostRecentConn, err := time.Parse(timeFormat, cfg.Annotations[ConnectedAtAnnotation]); err == nil {
-		if mostRecentConn.After(origConnTime) {
+		if mostRecentConn.After(workItem.origConTime) {
 			// this disconnect event wasn't processed until after we successfully reconnected
 			return nil
 		}
@@ -377,25 +332,25 @@ func (c *Controller) unregisterWorkload(entryName string, proxy *model.Proxy, di
 	conTime, _ := time.Parse(timeFormat, cfg.Annotations[ConnectedAtAnnotation])
 	// The wle has reconnected to this istiod,
 	// this may happen when the unregister fails retry
-	if disconTime.Before(conTime) {
+	if workItem.disConTime.Before(conTime) {
 		return nil
 	}
 
 	wle := cfg.DeepCopy()
 	delete(wle.Annotations, ConnectedAtAnnotation)
-	wle.Annotations[DisconnectedAtAnnotation] = disconTime.Format(timeFormat)
+	wle.Annotations[DisconnectedAtAnnotation] = workItem.disConTime.Format(timeFormat)
 	// use update instead of patch to prevent race condition
 	if _, err := c.store.Update(wle); err != nil {
 		autoRegistrationErrors.Increment()
-		return fmt.Errorf("disconnect: failed updating WorkloadEntry %s/%s: %v", proxy.Metadata.Namespace, entryName, err)
+		return fmt.Errorf("disconnect: failed updating WorkloadEntry %s/%s: %v", workItem.proxy.Metadata.Namespace, workItem.entryName, err)
 	}
 
 	autoRegistrationUnregistrations.Increment()
 
 	// after grace period, check if the workload ever reconnected
-	ns := proxy.Metadata.Namespace
+	ns := workItem.proxy.Metadata.Namespace
 	c.cleanupQueue.PushDelayed(func() error {
-		wle := c.store.Get(gvk.WorkloadEntry, entryName, ns)
+		wle := c.store.Get(gvk.WorkloadEntry, workItem.entryName, ns)
 		if wle == nil {
 			return nil
 		}
@@ -419,7 +374,7 @@ func (c *Controller) QueueWorkloadEntryHealth(proxy *model.Proxy, event HealthEv
 	}
 
 	condition := transformHealthEvent(proxy, entryName, event)
-	_ = c.healthCondition.Add(condition)
+	c.healthCondition.Add(condition)
 }
 
 // updateWorkloadEntryHealth updates the associated WorkloadEntries health status
@@ -655,22 +610,6 @@ func workloadEntryFromGroup(name string, proxy *model.Proxy, groupCfg *config.Co
 	}
 }
 
-func (c *Controller) handleErr(err error, key interface{}) {
-	if err == nil {
-		c.queue.Forget(key)
-		return
-	}
-
-	if c.queue.NumRequeues(key) < maxRetries {
-		log.Debugf(err)
-		c.queue.AddRateLimited(key)
-		return
-	}
-
-	c.queue.Forget(key)
-	log.Errorf(err)
-}
-
 func makeProxyKey(proxy *model.Proxy) string {
 	return string(proxy.Metadata.Network) + proxy.IPAddresses[0]
 }
diff --git a/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go b/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go
index e29b06f012..079e6c8cc0 100644
--- a/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/namespacecontroller.go
@@ -110,7 +110,8 @@ func (nc *NamespaceController) startCaBundleWatcher(stop <-chan struct{}) {
 // insertDataForNamespace will add data into the configmap for the specified namespace
 // If the configmap is not found, it will be created.
 // If you know the current contents of the configmap, using UpdateDataInConfigMap is more efficient.
-func (nc *NamespaceController) insertDataForNamespace(o types.NamespacedName) error {
+func (nc *NamespaceController) insertDataForNamespace(key interface{}) error {
+	o := key.(types.NamespacedName)
 	ns := o.Namespace
 	if ns == "" {
 		// For Namespace object, it will not have o.Namespace field set
diff --git a/pkg/kube/configmapwatcher/configmapwatcher.go b/pkg/kube/configmapwatcher/configmapwatcher.go
index 6b5dfe85f8..2a846b7b6c 100644
--- a/pkg/kube/configmapwatcher/configmapwatcher.go
+++ b/pkg/kube/configmapwatcher/configmapwatcher.go
@@ -23,7 +23,6 @@
 	"k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/fields"
-	"k8s.io/apimachinery/pkg/types"
 	"k8s.io/client-go/informers"
 	informersv1 "k8s.io/client-go/informers/core/v1"
 	"k8s.io/client-go/tools/cache"
@@ -86,7 +85,7 @@ func (c *Controller) HasSynced() bool {
 	return c.queue.HasSynced()
 }
 
-func (c *Controller) processItem(types.NamespacedName) error {
+func (c *Controller) processItem(interface{}) error {
 	cm, err := c.informer.Lister().ConfigMaps(c.configMapNamespace).Get(c.configMapName)
 	if err != nil {
 		if !errors.IsNotFound(err) {
diff --git a/pkg/kube/controllers/queue.go b/pkg/kube/controllers/queue.go
index a834d77fa3..55e44da271 100644
--- a/pkg/kube/controllers/queue.go
+++ b/pkg/kube/controllers/queue.go
@@ -54,11 +54,11 @@ func WithMaxAttempts(n int) func(q *Queue) {
 	}
 }
 
-// WithReconciler defines the to handle items on the queue
-func WithReconciler(f func(name types.NamespacedName) error) func(q *Queue) {
+// WithReconciler defines the handler function to handle items in the queue.
+func WithReconciler(f func(key interface{}) error) func(q *Queue) {
 	return func(q *Queue) {
 		q.workFn = func(key interface{}) error {
-			return f(key.(types.NamespacedName))
+			return f(key)
 		}
 	}
 }
@@ -80,7 +80,7 @@ func NewQueue(name string, options ...func(*Queue)) Queue {
 }
 
 // Add an item to the queue.
-func (q Queue) Add(item types.NamespacedName) {
+func (q Queue) Add(item interface{}) {
 	q.queue.Add(item)
 }
 
diff --git a/pkg/kube/controllers/queue_test.go b/pkg/kube/controllers/queue_test.go
index a1407d74c5..1507eee310 100644
--- a/pkg/kube/controllers/queue_test.go
+++ b/pkg/kube/controllers/queue_test.go
@@ -25,7 +25,7 @@
 
 func TestQueue(t *testing.T) {
 	handles := atomic.NewInt32(0)
-	q := NewQueue("custom", WithReconciler(func(name types.NamespacedName) error {
+	q := NewQueue("custom", WithReconciler(func(key interface{}) error {
 		handles.Inc()
 		return nil
 	}))
diff --git a/pkg/kube/multicluster/secretcontroller.go b/pkg/kube/multicluster/secretcontroller.go
index f7fbecb898..6413f119d6 100644
--- a/pkg/kube/multicluster/secretcontroller.go
+++ b/pkg/kube/multicluster/secretcontroller.go
@@ -347,7 +347,8 @@ func (c *Controller) HasSynced() bool {
 	return synced
 }
 
-func (c *Controller) processItem(key types.NamespacedName) error {
+func (c *Controller) processItem(item interface{}) error {
+	key := item.(types.NamespacedName)
 	log.Infof("processing secret event for secret %s", key)
 	obj, exists, err := c.informer.GetIndexer().GetByKey(key.String())
 	if err != nil {
diff --git a/pkg/revisions/default_watcher.go b/pkg/revisions/default_watcher.go
index 308bdbe35f..73ab4458b7 100644
--- a/pkg/revisions/default_watcher.go
+++ b/pkg/revisions/default_watcher.go
@@ -100,7 +100,8 @@ func (p *defaultWatcher) notifyHandlers() {
 	}
 }
 
-func (p *defaultWatcher) setDefault(key types.NamespacedName) error {
+func (p *defaultWatcher) setDefault(item interface{}) error {
+	key := item.(types.NamespacedName)
 	revision := ""
 	wh, _, _ := p.webhookInformer.GetIndexer().GetByKey(key.Name)
 	if wh != nil {
diff --git a/pkg/webhooks/webhookpatch.go b/pkg/webhooks/webhookpatch.go
index f01220be2a..519b211f39 100644
--- a/pkg/webhooks/webhookpatch.go
+++ b/pkg/webhooks/webhookpatch.go
@@ -95,7 +95,8 @@ func (w *WebhookCertPatcher) HasSynced() bool {
 }
 
 // webhookPatchTask takes the result of patchMutatingWebhookConfig and modifies the result for use in task queue
-func (w *WebhookCertPatcher) webhookPatchTask(o types.NamespacedName) error {
+func (w *WebhookCertPatcher) webhookPatchTask(key interface{}) error {
+	o := key.(types.NamespacedName)
 	reportWebhookPatchAttempts(o.Name)
 	err := w.patchMutatingWebhookConfig(
 		w.client.AdmissionregistrationV1().MutatingWebhookConfigurations(),
-- 
2.35.3

