From 5bf146fced9189c65e00f738cb5e95e9a801198e Mon Sep 17 00:00:00 2001
From: Zhonghu Xu <xuzhonghu@huawei.com>
Date: Thu, 6 Jan 2022 02:02:48 +0800
Subject: Fix flake TestEndpointsDeduping, it is caused by running kube
 controller twice (#36675)

* Revert "configgentest: avoid running kubecontroller 2x (#36332)"

This reverts commit 26e13f9542634db505fb10813e8f9638d6a2353e.

* Fix TestEndpointsDeduping flake

* Run before wait sync

* rm debug log
---
 pilot/pkg/networking/core/v1alpha3/fake.go    |  7 +--
 .../kube/controller/controller_test.go        | 47 ++++++++++++++++++-
 .../kube/controller/endpointcontroller.go     |  2 +-
 .../kube/controller/endpointslice_test.go     |  3 ++
 .../serviceregistry/kube/controller/fake.go   | 34 +++-----------
 .../kube/controller/pod_test.go               |  9 ++++
 .../controller/serviceexportcache_test.go     |  3 ++
 .../controller/serviceimportcache_test.go     |  3 ++
 pilot/pkg/xds/fake.go                         | 19 +++-----
 9 files changed, 79 insertions(+), 48 deletions(-)

diff --git a/pilot/pkg/networking/core/v1alpha3/fake.go b/pilot/pkg/networking/core/v1alpha3/fake.go
index 991fc5c60e..e4051c4f1b 100644
--- a/pilot/pkg/networking/core/v1alpha3/fake.go
+++ b/pilot/pkg/networking/core/v1alpha3/fake.go
@@ -67,8 +67,6 @@ type TestOptions struct {
 	MeshConfig      *meshconfig.MeshConfig
 	NetworksWatcher mesh.NetworksWatcher
 
-	// Optionally provide a top-level aggregate registry with subregistries added. The ConfigGenTest will handle running it.
-	AggregateRegistry *aggregate.Controller
 	// Additional service registries to use. A ServiceEntry and memory registry will always be created.
 	ServiceRegistries []serviceregistry.Instance
 
@@ -122,10 +120,7 @@ func NewConfigGenTest(t test.Failer, opts TestOptions) *ConfigGenTest {
 		m = &def
 	}
 
-	serviceDiscovery := opts.AggregateRegistry
-	if serviceDiscovery == nil {
-		serviceDiscovery = aggregate.NewController(aggregate.Options{})
-	}
+	serviceDiscovery := aggregate.NewController(aggregate.Options{})
 	se := serviceentry.NewServiceDiscovery(
 		configController, model.MakeIstioStore(configStore),
 		&FakeXdsUpdater{}, serviceentry.WithClusterID(opts.ClusterID))
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller_test.go b/pilot/pkg/serviceregistry/kube/controller/controller_test.go
index d2af5b4f2d..f559503c6d 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller_test.go
@@ -89,11 +89,13 @@ func TestServices(t *testing.T) {
 			},
 		},
 	})
-
 	for mode, name := range EndpointModeNames {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			ctl, fx := NewFakeControllerWithOptions(FakeControllerOptions{NetworksWatcher: networksWatcher, Mode: mode})
+			go ctl.Run(ctl.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(ctl.stop, ctl.HasSynced)
 			defer ctl.Stop()
 			t.Parallel()
 			ns := "ns-test"
@@ -284,9 +286,13 @@ func TestController_GetPodLocality(t *testing.T) {
 		// https://github.com/golang/go/wiki/CommonMistakes#using-goroutines-on-loop-iterator-variables
 		tc := tc
 		t.Run(tc.name, func(t *testing.T) {
+			t.Parallel()
 			// Setup kube caches
 			// Pod locality only matters for Endpoints
 			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 			addNodes(t, controller, tc.nodes...)
 			addPods(t, controller, fx, tc.pods...)
@@ -320,6 +326,9 @@ func TestGetProxyServiceInstances(t *testing.T) {
 			})
 			// add a network ID to test endpoints include topology.istio.io/network label
 			controller.network = networkID
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 			p := generatePod("128.0.0.1", "pod1", "nsa", "foo", "node1", map[string]string{"app": "test-app"}, map[string]string{})
 			addPods(t, controller, fx, p)
@@ -797,6 +806,9 @@ func TestGetProxyServiceInstancesWithMultiIPsAndTargetPorts(t *testing.T) {
 			t.Run(fmt.Sprintf("%s_%s", c.name, name), func(t *testing.T) {
 				// Setup kube caches
 				controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
+				go controller.Run(controller.stop)
+				// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+				cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 				defer controller.Stop()
 				addPods(t, controller, fx, c.pods...)
 
@@ -838,6 +850,9 @@ func TestController_GetIstioServiceAccounts(t *testing.T) {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 
 			sa1 := "acct1"
@@ -902,6 +917,9 @@ func TestController_Service(t *testing.T) {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 			// Use a timeout to keep the test from hanging.
 
@@ -1047,6 +1065,9 @@ func TestController_ServiceWithFixedDiscoveryNamespaces(t *testing.T) {
 				Mode:        mode,
 				MeshWatcher: meshWatcher,
 			})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 
 			nsA := "nsA"
@@ -1213,6 +1234,9 @@ func TestController_ServiceWithChangingDiscoveryNamespaces(t *testing.T) {
 				MeshWatcher:               meshWatcher,
 				DiscoveryNamespacesFilter: discoveryNamespacesFilter,
 			})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 
 			nsA := "nsA"
@@ -1349,6 +1373,9 @@ func TestExternalNameServiceInstances(t *testing.T) {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 			createExternalNameService(controller, "svc5", "nsA",
 				[]int32{1, 2, 3}, "foo.co", t, fx.Events)
@@ -1381,6 +1408,9 @@ func TestController_ExternalNameService(t *testing.T) {
 					}
 				},
 			})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 			// Use a timeout to keep the test from hanging.
 
@@ -1892,6 +1922,9 @@ func TestEndpointUpdate(t *testing.T) {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 
 			pod1 := generatePod("128.0.0.1", "pod1", "nsA", "", "node1", map[string]string{"app": "prod-app"}, map[string]string{})
@@ -1954,6 +1987,9 @@ func TestEndpointUpdateBeforePodUpdate(t *testing.T) {
 		mode := mode
 		t.Run(name, func(t *testing.T) {
 			controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: mode})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			// Setup kube caches
 			defer controller.Stop()
 			addNodes(t, controller, generateNode("node1", map[string]string{NodeZoneLabel: "zone1", NodeRegionLabel: "region1", label.TopologySubzone.Name: "subzone1"}))
@@ -2105,6 +2141,9 @@ func TestEndpointUpdateBeforePodUpdate(t *testing.T) {
 
 func TestWorkloadInstanceHandlerMultipleEndpoints(t *testing.T) {
 	controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{})
+	go controller.Run(controller.stop)
+	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+	cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 	defer controller.Stop()
 
 	// Create an initial pod with a service, and endpoint.
@@ -2220,6 +2259,9 @@ func TestKubeEndpointsControllerOnEvent(t *testing.T) {
 	for _, tc := range testCases {
 		t.Run(EndpointModeNames[tc.mode], func(t *testing.T) {
 			controller, _ := NewFakeControllerWithOptions(FakeControllerOptions{Mode: tc.mode})
+			go controller.Run(controller.stop)
+			// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+			cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 			defer controller.Stop()
 
 			if err := controller.endpoints.onEvent(tc.tombstone, model.EventDelete); err != nil {
@@ -2231,6 +2273,9 @@ func TestKubeEndpointsControllerOnEvent(t *testing.T) {
 
 func TestUpdateEdsCacheOnServiceUpdate(t *testing.T) {
 	controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{})
+	go controller.Run(controller.stop)
+	// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
+	cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 	defer controller.Stop()
 
 	// Create an initial pod with a service, and endpoint.
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpointcontroller.go b/pilot/pkg/serviceregistry/kube/controller/endpointcontroller.go
index 66e4130a0a..5e77da92fb 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpointcontroller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpointcontroller.go
@@ -90,7 +90,7 @@ func processEndpointEvent(c *Controller, epc kubeEndpointsController, name strin
 
 func updateEDS(c *Controller, epc kubeEndpointsController, ep interface{}, event model.Event) {
 	namespacedName := epc.getServiceNamespacedName(ep)
-	log.Debugf("Handle EDS endpoint %s in namespace %s", namespacedName.Name, namespacedName.Namespace)
+	log.Debugf("Handle EDS endpoint %s %s %s in namespace %s", namespacedName.Name, event, namespacedName.Namespace)
 	var forgottenEndpointsByHost map[host.Name][]*model.IstioEndpoint
 	if event == model.EventDelete {
 		forgottenEndpointsByHost = epc.forgetEndpoint(ep)
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go b/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
index fbb7350273..47f0b262d3 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
@@ -20,6 +20,7 @@
 	"time"
 
 	coreV1 "k8s.io/api/core/v1"
+	"k8s.io/client-go/tools/cache"
 	mcs "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
@@ -78,6 +79,8 @@ func TestEndpointSliceFromMCSShouldBeIgnored(t *testing.T) {
 	)
 
 	controller, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointSliceOnly})
+	go controller.Run(controller.stop)
+	cache.WaitForCacheSync(controller.stop, controller.HasSynced)
 	defer controller.Stop()
 
 	node := generateNode("node1", map[string]string{
diff --git a/pilot/pkg/serviceregistry/kube/controller/fake.go b/pilot/pkg/serviceregistry/kube/controller/fake.go
index 564563eae3..02847831f6 100644
--- a/pilot/pkg/serviceregistry/kube/controller/fake.go
+++ b/pilot/pkg/serviceregistry/kube/controller/fake.go
@@ -17,8 +17,6 @@
 import (
 	"time"
 
-	"k8s.io/client-go/tools/cache"
-
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/serviceregistry/aggregate"
@@ -149,10 +147,7 @@ type FakeControllerOptions struct {
 	DomainSuffix              string
 	XDSUpdater                model.XDSUpdater
 	DiscoveryNamespacesFilter filter.DiscoveryNamespacesFilter
-
-	// when calling from NewFakeDiscoveryServer use the same aggregate controller we add other registries to
-	AggregateController *aggregate.Controller
-	Stop                chan struct{}
+	Stop                      chan struct{}
 }
 
 type FakeController struct {
@@ -169,18 +164,14 @@ func NewFakeControllerWithOptions(opts FakeControllerOptions) (*FakeController,
 	if opts.DomainSuffix != "" {
 		domainSuffix = opts.DomainSuffix
 	}
-	client := opts.Client
-	if client == nil {
-		client = kubelib.NewFakeClient()
+	if opts.Client == nil {
+		opts.Client = kubelib.NewFakeClient()
 	}
 	if opts.MeshWatcher == nil {
 		opts.MeshWatcher = mesh.NewFixedWatcher(&meshconfig.MeshConfig{})
 	}
 
-	meshServiceController := opts.AggregateController
-	if meshServiceController == nil {
-		meshServiceController = aggregate.NewController(aggregate.Options{MeshHolder: opts.MeshWatcher})
-	}
+	meshServiceController := aggregate.NewController(aggregate.Options{MeshHolder: opts.MeshWatcher})
 
 	options := Options{
 		DomainSuffix:              domainSuffix,
@@ -194,28 +185,17 @@ func NewFakeControllerWithOptions(opts FakeControllerOptions) (*FakeController,
 		DiscoveryNamespacesFilter: opts.DiscoveryNamespacesFilter,
 		MeshServiceController:     meshServiceController,
 	}
-	c := NewController(client, options)
+	c := NewController(opts.Client, options)
+	meshServiceController.AddRegistry(c)
 
 	if opts.ServiceHandler != nil {
 		c.AppendServiceHandler(opts.ServiceHandler)
 	}
-
-	// Run in initiation to prevent calling each test
-	// TODO: fix it, so we can remove `stop` channel
 	c.stop = opts.Stop
 	if c.stop == nil {
 		c.stop = make(chan struct{})
 	}
-	client.RunAndWait(c.stop)
-
-	// we created the aggregate here, so we're responsible for starting it
-	if opts.AggregateController == nil {
-		meshServiceController.AddRegistry(c)
-		go meshServiceController.Run(c.stop)
-		// Wait for the caches to sync, otherwise we may hit race conditions where events are dropped
-		cache.WaitForCacheSync(c.stop, c.HasSynced)
-	}
-
+	opts.Client.RunAndWait(c.stop)
 	var fx *FakeXdsUpdater
 	if x, ok := xdsUpdater.(*FakeXdsUpdater); ok {
 		fx = x
diff --git a/pilot/pkg/serviceregistry/kube/controller/pod_test.go b/pilot/pkg/serviceregistry/kube/controller/pod_test.go
index 084b853995..dc8bc59c7f 100644
--- a/pilot/pkg/serviceregistry/kube/controller/pod_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/pod_test.go
@@ -24,6 +24,7 @@
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/util/wait"
 	"k8s.io/client-go/kubernetes"
+	"k8s.io/client-go/tools/cache"
 
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pkg/config/labels"
@@ -109,6 +110,8 @@ func TestPodCache(t *testing.T) {
 
 func TestHostNetworkPod(t *testing.T) {
 	c, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
+	go c.Run(c.stop)
+	cache.WaitForCacheSync(c.stop, c.HasSynced)
 	defer c.Stop()
 	initTestEnv(t, c.client, fx)
 	createPod := func(ip, name string) {
@@ -134,6 +137,8 @@ func TestHostNetworkPod(t *testing.T) {
 // Regression test for https://github.com/istio/istio/issues/20676
 func TestIPReuse(t *testing.T) {
 	c, fx := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
+	go c.Run(c.stop)
+	cache.WaitForCacheSync(c.stop, c.HasSynced)
 	defer c.Stop()
 	initTestEnv(t, c.client, fx)
 
@@ -204,6 +209,8 @@ func testPodCache(t *testing.T) {
 		Mode:              EndpointsOnly,
 		WatchedNamespaces: "nsa,nsb",
 	})
+	go c.Run(c.stop)
+	cache.WaitForCacheSync(c.stop, c.HasSynced)
 	defer c.Stop()
 
 	initTestEnv(t, c.client, fx)
@@ -252,6 +259,8 @@ func testPodCache(t *testing.T) {
 func TestPodCacheEvents(t *testing.T) {
 	t.Parallel()
 	c, _ := NewFakeControllerWithOptions(FakeControllerOptions{Mode: EndpointsOnly})
+	go c.Run(c.stop)
+	cache.WaitForCacheSync(c.stop, c.HasSynced)
 	defer c.Stop()
 
 	ns := "default"
diff --git a/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go b/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go
index d5ce028690..e872f0e8e2 100644
--- a/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/serviceexportcache_test.go
@@ -24,6 +24,7 @@
 	v12 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
 	"k8s.io/apimachinery/pkg/types"
+	"k8s.io/client-go/tools/cache"
 	mcsapi "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
@@ -151,6 +152,8 @@ func newTestServiceExportCache(t *testing.T, clusterLocalMode ClusterLocalMode,
 		ClusterID: testCluster,
 		Mode:      endpointMode,
 	})
+	go c.Run(c.stop)
+	cache.WaitForCacheSync(c.stop, c.HasSynced)
 
 	// Create the test service and endpoints.
 	createService(c, serviceExportName, serviceExportNamespace, map[string]string{},
diff --git a/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go b/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go
index 795e9bf739..0b72ff499a 100644
--- a/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/serviceimportcache_test.go
@@ -28,6 +28,7 @@
 	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
 	"k8s.io/apimachinery/pkg/runtime"
 	"k8s.io/apimachinery/pkg/types"
+	"k8s.io/client-go/tools/cache"
 	mcsapi "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
@@ -212,6 +213,8 @@ func newTestServiceImportCache(mode EndpointMode) (c *FakeController, ic *servic
 		ClusterID: serviceImportCluster,
 		Mode:      mode,
 	})
+	go c.Run(c.stop)
+	cache.WaitForCacheSync(c.stop, c.HasSynced)
 
 	ic = c.imports.(*serviceImportCacheImpl)
 	return
diff --git a/pilot/pkg/xds/fake.go b/pilot/pkg/xds/fake.go
index b39b4b28d6..7f6a4e86fc 100644
--- a/pilot/pkg/xds/fake.go
+++ b/pilot/pkg/xds/fake.go
@@ -43,7 +43,6 @@
 	"istio.io/istio/pilot/pkg/networking/core/v1alpha3"
 	"istio.io/istio/pilot/pkg/networking/plugin"
 	"istio.io/istio/pilot/pkg/serviceregistry"
-	"istio.io/istio/pilot/pkg/serviceregistry/aggregate"
 	kube "istio.io/istio/pilot/pkg/serviceregistry/kube/controller"
 	v3 "istio.io/istio/pilot/pkg/xds/v3"
 	"istio.io/istio/pilot/test/xdstest"
@@ -173,7 +172,6 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 	}
 	creds := kubesecrets.NewMulticluster(opts.DefaultClusterName)
 	s.Generators[v3.SecretType] = NewSecretGen(creds, s.Cache, opts.DefaultClusterName)
-	aggregateRegistry := aggregate.NewController(aggregate.Options{})
 	for k8sCluster, objs := range k8sObjects {
 		client := kubelib.NewFakeClientWithVersion(opts.KubernetesVersion, objs...)
 		if opts.KubeClientModifier != nil {
@@ -187,9 +185,7 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 			XDSUpdater:      xdsUpdater,
 			NetworksWatcher: opts.NetworksWatcher,
 			Mode:            opts.KubernetesEndpointMode,
-			// we wait for the aggregate to sync
-			AggregateController: aggregateRegistry,
-			Stop:                stop,
+			Stop:            stop,
 		})
 		// start default client informers after creating ingress/secret controllers
 		if defaultKubeClient == nil || k8sCluster == opts.DefaultClusterName {
@@ -199,7 +195,6 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 			client.RunAndWait(stop)
 		}
 		registries = append(registries, k8s)
-		aggregateRegistry.AddRegistry(k8s)
 		if err := creds.ClusterAdded(&multicluster.Cluster{ID: k8sCluster, Client: client}, nil); err != nil {
 			t.Fatal(err)
 		}
@@ -221,7 +216,6 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 		ConfigTemplateInput: opts.ConfigTemplateInput,
 		MeshConfig:          opts.MeshConfig,
 		NetworksWatcher:     opts.NetworksWatcher,
-		AggregateRegistry:   aggregateRegistry,
 		ServiceRegistries:   registries,
 		PushContextLock:     &s.updateMutex,
 		ConfigStoreCaches:   []model.ConfigStoreCache{ingr},
@@ -309,17 +303,16 @@ func NewFakeDiscoveryServer(t test.Failer, opts FakeOptions) *FakeDiscoveryServe
 	// Start the discovery server
 	s.Start(stop)
 	cg.ServiceEntryRegistry.XdsUpdater = s
-	cg.ServiceEntryRegistry.ResyncEDS()
-
-	// Send an update. This ensures that even if there are no configs provided, the push context is
-	// initialized.
-	s.ConfigUpdate(&model.PushRequest{Full: true})
-
 	// Now that handlers are added, get everything started
 	cg.Run()
 	cache.WaitForCacheSync(stop,
 		cg.Registry.HasSynced,
 		cg.Store().HasSynced)
+	cg.ServiceEntryRegistry.ResyncEDS()
+
+	// Send an update. This ensures that even if there are no configs provided, the push context is
+	// initialized.
+	s.ConfigUpdate(&model.PushRequest{Full: true})
 
 	// Wait until initial updates are committed
 	c := s.InboundUpdates.Load()
-- 
2.35.3

