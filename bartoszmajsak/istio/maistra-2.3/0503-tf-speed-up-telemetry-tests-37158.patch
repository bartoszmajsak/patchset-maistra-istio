From 1ffd5fd66a0cc98abfa686ce9ea4eebb2f3d4e8c Mon Sep 17 00:00:00 2001
From: John Howard <howardjohn@google.com>
Date: Mon, 7 Feb 2022 11:57:17 -0800
Subject: tf: speed up telemetry tests (#37158)

* tf: speed up telemetry tests

* fix

* gen

* fix lint
---
 pkg/test/echo/cmd/client/main.go              |  60 +++---
 .../echo/cmd/echogen/testdata/golden.yaml     |   2 +-
 .../components/echo/kube/deployment.go        |   2 +-
 .../components/echo/kube/testdata/basic.yaml  |   2 +-
 .../kube/testdata/healthcheck-rewrite.yaml    |   2 +-
 .../multiple-istio-versions-no-proxy.yaml     |   4 +-
 .../testdata/multiple-istio-versions.yaml     |   4 +-
 .../echo/kube/testdata/multiversion.yaml      |   4 +-
 .../testdata/two-workloads-one-nosidecar.yaml |   4 +-
 .../framework/components/prometheus/kube.go   | 183 +++---------------
 .../components/prometheus/prometheus.go       |  20 +-
 pkg/test/framework/resource/context.go        |   2 +-
 pkg/test/framework/telemetry.go               |   2 +-
 tests/integration/pilot/analyze_test.go       |   2 +-
 .../sds_istio_mutual_egress_test.go           |  27 +--
 .../telemetry/outboundtrafficpolicy/helper.go | 137 ++++---------
 .../customize_metrics_test.go                 |  20 +-
 .../telemetry/stats/prometheus/stats.go       |  16 +-
 .../stats/prometheus/util_prometheus.go       |  77 ++------
 tests/integration/telemetry/util.go           |  13 +-
 20 files changed, 160 insertions(+), 423 deletions(-)

diff --git a/pkg/test/echo/cmd/client/main.go b/pkg/test/echo/cmd/client/main.go
index 5137e119c4..7919816e85 100644
--- a/pkg/test/echo/cmd/client/main.go
+++ b/pkg/test/echo/cmd/client/main.go
@@ -38,23 +38,24 @@
 )
 
 var (
-	count           int
-	timeout         time.Duration
-	qps             int
-	uds             string
-	headers         []string
-	msg             string
-	expect          string
-	expectSet       bool
-	method          string
-	http2           bool
-	http3           bool
-	alpn            []string
-	serverName      string
-	serverFirst     bool
-	followRedirects bool
-	clientCert      string
-	clientKey       string
+	count              int
+	timeout            time.Duration
+	qps                int
+	uds                string
+	headers            []string
+	msg                string
+	expect             string
+	expectSet          bool
+	method             string
+	http2              bool
+	http3              bool
+	insecureSkipVerify bool
+	alpn               []string
+	serverName         string
+	serverFirst        bool
+	followRedirects    bool
+	clientCert         string
+	clientKey          string
 
 	caFile string
 
@@ -135,6 +136,8 @@ func init() {
 		"send http requests as HTTP2 with prior knowledge")
 	rootCmd.PersistentFlags().BoolVar(&http3, "http3", false,
 		"send http requests as HTTP 3")
+	rootCmd.PersistentFlags().BoolVarP(&insecureSkipVerify, "insecure-skip-verify", "-k", insecureSkipVerify,
+		"do not verify TLS")
 	rootCmd.PersistentFlags().BoolVar(&serverFirst, "server-first", false,
 		"Treat as a server first protocol; do not send request until magic string is received")
 	rootCmd.PersistentFlags().BoolVarP(&followRedirects, "follow-redirects", "L", false,
@@ -163,17 +166,18 @@ func defaultScheme(u string) string {
 
 func getRequest(url string) (*proto.ForwardEchoRequest, error) {
 	request := &proto.ForwardEchoRequest{
-		Url:             defaultScheme(url),
-		TimeoutMicros:   common.DurationToMicros(timeout),
-		Count:           int32(count),
-		Qps:             int32(qps),
-		Message:         msg,
-		Http2:           http2,
-		Http3:           http3,
-		ServerFirst:     serverFirst,
-		FollowRedirects: followRedirects,
-		Method:          method,
-		ServerName:      serverName,
+		Url:                defaultScheme(url),
+		TimeoutMicros:      common.DurationToMicros(timeout),
+		Count:              int32(count),
+		Qps:                int32(qps),
+		Message:            msg,
+		Http2:              http2,
+		Http3:              http3,
+		ServerFirst:        serverFirst,
+		FollowRedirects:    followRedirects,
+		Method:             method,
+		ServerName:         serverName,
+		InsecureSkipVerify: insecureSkipVerify,
 	}
 
 	if expectSet {
diff --git a/pkg/test/framework/components/echo/cmd/echogen/testdata/golden.yaml b/pkg/test/framework/components/echo/cmd/echogen/testdata/golden.yaml
index 21aa8baf26..95469d4807 100644
--- a/pkg/test/framework/components/echo/cmd/echogen/testdata/golden.yaml
+++ b/pkg/test/framework/components/echo/cmd/echogen/testdata/golden.yaml
@@ -163,6 +163,6 @@ spec:
           runAsUser: 1338
         startupProbe:
           failureThreshold: 10
-          periodSeconds: 10
+          periodSeconds: 1
           tcpSocket:
             port: tcp-health-port
\ No newline at end of file
diff --git a/pkg/test/framework/components/echo/kube/deployment.go b/pkg/test/framework/components/echo/kube/deployment.go
index 374551f30b..2d2b972d4d 100644
--- a/pkg/test/framework/components/echo/kube/deployment.go
+++ b/pkg/test/framework/components/echo/kube/deployment.go
@@ -269,7 +269,7 @@
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 {{- end }}
 {{- if $.TLSSettings }}
diff --git a/pkg/test/framework/components/echo/kube/testdata/basic.yaml b/pkg/test/framework/components/echo/kube/testdata/basic.yaml
index d7e3c56c19..3045cfaeff 100644
--- a/pkg/test/framework/components/echo/kube/testdata/basic.yaml
+++ b/pkg/test/framework/components/echo/kube/testdata/basic.yaml
@@ -95,6 +95,6 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
diff --git a/pkg/test/framework/components/echo/kube/testdata/healthcheck-rewrite.yaml b/pkg/test/framework/components/echo/kube/testdata/healthcheck-rewrite.yaml
index 102702f9e4..db4432c702 100644
--- a/pkg/test/framework/components/echo/kube/testdata/healthcheck-rewrite.yaml
+++ b/pkg/test/framework/components/echo/kube/testdata/healthcheck-rewrite.yaml
@@ -93,6 +93,6 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
diff --git a/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions-no-proxy.yaml b/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions-no-proxy.yaml
index e3fc8e3579..dbd89ced6c 100644
--- a/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions-no-proxy.yaml
+++ b/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions-no-proxy.yaml
@@ -91,7 +91,7 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
 apiVersion: apps/v1
@@ -169,6 +169,6 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
diff --git a/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions.yaml b/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions.yaml
index 8f103a7228..ba80bf59b9 100644
--- a/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions.yaml
+++ b/pkg/test/framework/components/echo/kube/testdata/multiple-istio-versions.yaml
@@ -96,7 +96,7 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
 apiVersion: apps/v1
@@ -179,6 +179,6 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
diff --git a/pkg/test/framework/components/echo/kube/testdata/multiversion.yaml b/pkg/test/framework/components/echo/kube/testdata/multiversion.yaml
index e5854e829b..2a8ff7ddf3 100644
--- a/pkg/test/framework/components/echo/kube/testdata/multiversion.yaml
+++ b/pkg/test/framework/components/echo/kube/testdata/multiversion.yaml
@@ -101,7 +101,7 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
 apiVersion: apps/v1
@@ -182,6 +182,6 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
diff --git a/pkg/test/framework/components/echo/kube/testdata/two-workloads-one-nosidecar.yaml b/pkg/test/framework/components/echo/kube/testdata/two-workloads-one-nosidecar.yaml
index 2ec7024456..0a612d5edd 100644
--- a/pkg/test/framework/components/echo/kube/testdata/two-workloads-one-nosidecar.yaml
+++ b/pkg/test/framework/components/echo/kube/testdata/two-workloads-one-nosidecar.yaml
@@ -95,7 +95,7 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
 apiVersion: apps/v1
@@ -173,6 +173,6 @@ spec:
         startupProbe:
           tcpSocket:
             port: tcp-health-port
-          periodSeconds: 10
+          periodSeconds: 1
           failureThreshold: 10
 ---
diff --git a/pkg/test/framework/components/prometheus/kube.go b/pkg/test/framework/components/prometheus/kube.go
index 29ffee30b0..6c79554a1f 100644
--- a/pkg/test/framework/components/prometheus/kube.go
+++ b/pkg/test/framework/components/prometheus/kube.go
@@ -47,7 +47,7 @@
 
 var (
 	retryTimeout = retry.Timeout(time.Second * 120)
-	retryDelay   = retry.Delay(time.Second * 5)
+	retryDelay   = retry.Delay(time.Second * 1)
 
 	_ Instance  = &kubeComponent{}
 	_ io.Closer = &kubeComponent{}
@@ -78,7 +78,13 @@ func installPrometheus(ctx resource.Context, ns string) error {
 	if err != nil {
 		return err
 	}
-	return ctx.ConfigKube().ApplyYAML(ns, yaml)
+	if err := ctx.ConfigKube().ApplyYAMLNoCleanup(ns, yaml); err != nil {
+		return err
+	}
+	ctx.Cleanup(func() {
+		_ = ctx.ConfigKube().DeleteYAML(ns, yaml)
+	})
+	return nil
 }
 
 func newKube(ctx resource.Context, cfgIn Config) (Instance, error) {
@@ -150,85 +156,21 @@ func (c *kubeComponent) APIForCluster(cluster cluster.Cluster) prometheusApiV1.A
 	return c.api[cluster.Name()]
 }
 
-func (c *kubeComponent) WaitForQuiesce(format string, args ...interface{}) (model.Value, error) {
-	return c.WaitForQuiesceForCluster(c.clusters.Default(), format, args...)
-}
-
-func (c *kubeComponent) WaitForQuiesceForCluster(cluster cluster.Cluster, format string, args ...interface{}) (model.Value, error) {
-	var previous model.Value
-
-	time.Sleep(time.Second * 1)
-
-	value, err := retry.UntilComplete(func() (interface{}, bool, error) {
-		var err error
-		query, err := tmpl.Evaluate(fmt.Sprintf(format, args...), map[string]string{})
-		if err != nil {
-			return nil, true, err
-		}
-
-		scopes.Framework.Debugf("WaitForQuiesce running: %q", query)
-
-		var v model.Value
-
-		v, _, err = c.api[cluster.Name()].Query(context.Background(), query, time.Now())
-
-		if err != nil {
-			return nil, false, fmt.Errorf("error querying Prometheus: %v", err)
-		}
-		scopes.Framework.Debugf("WaitForQuiesce received: %v", v)
-
-		if previous == nil {
-			previous = v
-			return nil, false, nil
-		}
-
-		if !areEqual(v, previous) {
-			scopes.Framework.Debugf("WaitForQuiesce: \n%v\n!=\n%v\n", v, previous)
-			previous = v
-			return nil, false, fmt.Errorf("unable to quiesce for query: %q", query)
-		}
-
-		return v, true, nil
-	}, retryTimeout, retryDelay)
-
-	var v model.Value
-	if value != nil {
-		v = value.(model.Value)
-	}
-	return v, err
-}
-
-func (c *kubeComponent) WaitForQuiesceOrFail(t test.Failer, format string, args ...interface{}) model.Value {
-	return c.WaitForQuiesceOrFailForCluster(c.clusters.Default(), t, format, args...)
-}
-
-func (c *kubeComponent) WaitForQuiesceOrFailForCluster(cluster cluster.Cluster, t test.Failer, format string, args ...interface{}) model.Value {
-	v, err := c.WaitForQuiesceForCluster(cluster, format, args...)
-	if err != nil {
-		t.Fatal(err)
-	}
-	return v
-}
-
-func (c *kubeComponent) WaitForOneOrMore(format string, args ...interface{}) (model.Value, error) {
-	return c.WaitForOneOrMoreForCluster(c.clusters.Default(), format, args...)
-}
-
-func (c *kubeComponent) WaitForOneOrMoreForCluster(cluster cluster.Cluster, format string, args ...interface{}) (model.Value, error) {
+func (c *kubeComponent) Query(cluster cluster.Cluster, format string) (model.Value, error) {
 	value, err := retry.UntilComplete(func() (interface{}, bool, error) {
 		var err error
-		query, err := tmpl.Evaluate(fmt.Sprintf(format, args...), map[string]string{})
+		query, err := tmpl.Evaluate(format, map[string]string{})
 		if err != nil {
 			return nil, true, err
 		}
 
-		scopes.Framework.Debugf("WaitForOneOrMore running: %q", query)
+		scopes.Framework.Debugf("Query running: %q", query)
 
 		v, _, err := c.api[cluster.Name()].Query(context.Background(), query, time.Now())
 		if err != nil {
 			return nil, false, fmt.Errorf("error querying Prometheus: %v", err)
 		}
-		scopes.Framework.Debugf("WaitForOneOrMore received: %v", v)
+		scopes.Framework.Debugf("Query received: %v", v)
 
 		switch v.Type() {
 		case model.ValScalar, model.ValString:
@@ -236,7 +178,6 @@ func (c *kubeComponent) WaitForOneOrMoreForCluster(cluster cluster.Cluster, form
 
 		case model.ValVector:
 			value := v.(model.Vector)
-			value = reduce(value, map[string]string{})
 			if len(value) == 0 {
 				return nil, false, fmt.Errorf("value not found (query: %q)", query)
 			}
@@ -254,47 +195,40 @@ func (c *kubeComponent) WaitForOneOrMoreForCluster(cluster cluster.Cluster, form
 	return v, err
 }
 
-func (c *kubeComponent) WaitForOneOrMoreOrFail(t test.Failer, format string, args ...interface{}) model.Value {
-	return c.WaitForOneOrMoreOrFailForCluster(c.clusters.Default(), t, format, args...)
-}
-
-func (c *kubeComponent) WaitForOneOrMoreOrFailForCluster(cluster cluster.Cluster, t test.Failer, format string, args ...interface{}) model.Value {
-	val, err := c.WaitForOneOrMoreForCluster(cluster, format, args...)
+func (c *kubeComponent) QueryOrFail(t test.Failer, cluster cluster.Cluster, format string) model.Value {
+	val, err := c.Query(cluster, format)
 	if err != nil {
 		t.Fatal(err)
 	}
 	return val
 }
 
-func reduce(v model.Vector, labels map[string]string) model.Vector {
-	if labels == nil {
-		return v
+func (c *kubeComponent) QuerySum(cluster cluster.Cluster, query string) (float64, error) {
+	val, err := c.Query(cluster, query)
+	if err != nil {
+		return 0, err
 	}
-
-	reduced := make([]*model.Sample, 0)
-
-	for _, s := range v {
-		nameCount := len(labels)
-		for k, v := range s.Metric {
-			if labelVal, ok := labels[string(k)]; ok && labelVal == string(v) {
-				nameCount--
-			}
-		}
-		if nameCount == 0 {
-			reduced = append(reduced, s)
-		}
+	got, err := Sum(val)
+	if err != nil {
+		return 0, fmt.Errorf("could not find metric value: %v", err)
 	}
+	return got, nil
+}
 
-	return reduced
+func (c *kubeComponent) QuerySumOrFail(t test.Failer, cluster cluster.Cluster, query string) float64 {
+	v, err := c.QuerySum(cluster, query)
+	if err != nil {
+		t.Fatal("failed QuerySum: %v", err)
+	}
+	return v
 }
 
-func (c *kubeComponent) Sum(val model.Value, labels map[string]string) (float64, error) {
+func Sum(val model.Value) (float64, error) {
 	if val.Type() != model.ValVector {
 		return 0, fmt.Errorf("value not a model.Vector; was %s", val.Type().String())
 	}
 
 	value := val.(model.Vector)
-	value = reduce(value, labels)
 
 	valueCount := 0.0
 	for _, sample := range value {
@@ -304,15 +238,7 @@ func (c *kubeComponent) Sum(val model.Value, labels map[string]string) (float64,
 	if valueCount > 0.0 {
 		return valueCount, nil
 	}
-	return 0, fmt.Errorf("value not found for %#v", labels)
-}
-
-func (c *kubeComponent) SumOrFail(t test.Failer, val model.Value, labels map[string]string) float64 {
-	v, err := c.Sum(val, labels)
-	if err != nil {
-		t.Fatal(err)
-	}
-	return v
+	return 0, fmt.Errorf("value not found")
 }
 
 // Close implements io.Closer.
@@ -322,50 +248,3 @@ func (c *kubeComponent) Close() error {
 	}
 	return nil
 }
-
-// check equality without considering timestamps
-func areEqual(v1, v2 model.Value) bool {
-	if v1.Type() != v2.Type() {
-		return false
-	}
-
-	switch v1.Type() {
-	case model.ValNone:
-		return true
-	case model.ValString:
-		vs1 := v1.(*model.String)
-		vs2 := v2.(*model.String)
-		return vs1.Value == vs2.Value
-	case model.ValScalar:
-		ss1 := v1.(*model.Scalar)
-		ss2 := v2.(*model.Scalar)
-		return ss1.Value == ss2.Value
-	case model.ValVector:
-		vec1 := v1.(model.Vector)
-		vec2 := v2.(model.Vector)
-		if len(vec1) != len(vec2) {
-			scopes.Framework.Debugf("Prometheus.areEqual vector value size mismatch %d != %d", len(vec1), len(vec2))
-			return false
-		}
-
-		for i := 0; i < len(vec1); i++ {
-			if !vec1[i].Metric.Equal(vec2[i].Metric) {
-				scopes.Framework.Debugf(
-					"Prometheus.areEqual vector metric mismatch (at:%d): \n%v\n != \n%v\n",
-					i, vec1[i].Metric, vec2[i].Metric)
-				return false
-			}
-
-			if vec1[i].Value != vec2[i].Value {
-				scopes.Framework.Debugf(
-					"Prometheus.areEqual vector value mismatch (at:%d): %f != %f",
-					i, vec1[i].Value, vec2[i].Value)
-				return false
-			}
-		}
-		return true
-
-	default:
-		panic("unrecognized type " + v1.Type().String())
-	}
-}
diff --git a/pkg/test/framework/components/prometheus/prometheus.go b/pkg/test/framework/components/prometheus/prometheus.go
index 3467e7eea5..c423e24125 100644
--- a/pkg/test/framework/components/prometheus/prometheus.go
+++ b/pkg/test/framework/components/prometheus/prometheus.go
@@ -30,21 +30,13 @@ type Instance interface {
 	API() v1.API
 	APIForCluster(cluster cluster.Cluster) v1.API
 
-	// WaitForQuiesce runs the provided query periodically until the result gets stable.
-	WaitForQuiesce(fmt string, args ...interface{}) (prom.Value, error)
-	WaitForQuiesceOrFail(t test.Failer, fmt string, args ...interface{}) prom.Value
-	WaitForQuiesceForCluster(cluster cluster.Cluster, fmt string, args ...interface{}) (prom.Value, error)
-	WaitForQuiesceOrFailForCluster(cluster cluster.Cluster, t test.Failer, fmt string, args ...interface{}) prom.Value
+	// Query Run the provided query against the given cluster
+	Query(cluster cluster.Cluster, query string) (prom.Value, error)
+	QueryOrFail(t test.Failer, cluster cluster.Cluster, query string) prom.Value
 
-	// WaitForOneOrMore runs the provided query and waits until one (or more for vector) values are available.
-	WaitForOneOrMore(fmt string, args ...interface{}) (prom.Value, error)
-	WaitForOneOrMoreOrFail(t test.Failer, fmt string, args ...interface{}) prom.Value
-	WaitForOneOrMoreForCluster(cluster cluster.Cluster, fmt string, args ...interface{}) (prom.Value, error)
-	WaitForOneOrMoreOrFailForCluster(cluster cluster.Cluster, t test.Failer, fmt string, args ...interface{}) prom.Value
-
-	// Sum all the samples that has the given labels in the given vector value.
-	Sum(val prom.Value, labels map[string]string) (float64, error)
-	SumOrFail(t test.Failer, val prom.Value, labels map[string]string) float64
+	// QuerySum is a help around Query to compute the sum
+	QuerySum(cluster cluster.Cluster, query string) (float64, error)
+	QuerySumOrFail(t test.Failer, cluster cluster.Cluster, query string) float64
 }
 
 type Config struct {
diff --git a/pkg/test/framework/resource/context.go b/pkg/test/framework/resource/context.go
index 06745a569f..586f720f1c 100644
--- a/pkg/test/framework/resource/context.go
+++ b/pkg/test/framework/resource/context.go
@@ -69,7 +69,7 @@ type Context interface {
 	// Settings returns common settings
 	Settings() *Settings
 
-	// WhenDone runs the given function when the test context completes.
+	// ConditionalCleanup runs the given function when the test context completes.
 	// If -istio.test.nocleanup is set, this function will not be executed. To unconditionally cleanup, use Cleanup.
 	// This function may not (safely) access the test context.
 	ConditionalCleanup(fn func())
diff --git a/pkg/test/framework/telemetry.go b/pkg/test/framework/telemetry.go
index 00d8d09bde..b936e4e021 100644
--- a/pkg/test/framework/telemetry.go
+++ b/pkg/test/framework/telemetry.go
@@ -29,7 +29,7 @@
 )
 
 func init() {
-	flag.DurationVar(&TelemetryRetryDelay, "istio.test.telemetry.retryDelay", time.Second*3, "Default retry delay used in tests")
+	flag.DurationVar(&TelemetryRetryDelay, "istio.test.telemetry.retryDelay", time.Second*2, "Default retry delay used in tests")
 	flag.DurationVar(&TelemetryRetryTimeout, "istio.test.telemetry.retryTimeout", time.Second*80, "Default retry timeout used in tests")
 	flag.BoolVar(&UseRealStackdriver, "istio.test.telemetry.useRealStackdriver", false,
 		"controls whether to use real Stackdriver backend or not for Stackdriver integration test.")
diff --git a/tests/integration/pilot/analyze_test.go b/tests/integration/pilot/analyze_test.go
index b516939f81..3131f27076 100644
--- a/tests/integration/pilot/analyze_test.go
+++ b/tests/integration/pilot/analyze_test.go
@@ -453,7 +453,7 @@ func applyFileOrFail(t framework.TestContext, ns, filename string) {
 	if err := t.Clusters().Default().ApplyYAMLFiles(ns, filename); err != nil {
 		t.Fatal(err)
 	}
-	t.ConditionalCleanup(func() {
+	t.Cleanup(func() {
 		t.Clusters().Default().DeleteYAMLFiles(ns, filename)
 	})
 }
diff --git a/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go b/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go
index e043453ab4..10f7922709 100644
--- a/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go
+++ b/tests/integration/security/sds_egress/sds_istio_mutual_egress_test.go
@@ -145,35 +145,10 @@ func applySetupConfig(ctx framework.TestContext, ns namespace.Instance) {
 	}
 }
 
-func getMetric(ctx framework.TestContext, prometheus prometheus.Instance, query string) (float64, error) {
-	ctx.Helper()
-
-	value, err := prometheus.WaitForQuiesce(query)
-	if err != nil {
-		return 0, fmt.Errorf("failed to retrieve metric from prom with err: %v", err)
-	}
-
-	metric, err := prometheus.Sum(value, nil)
-	if err != nil {
-		ctx.Logf("value: %s", value.String())
-		return 0, fmt.Errorf("could not find metric value: %v", err)
-	}
-
-	return metric, nil
-}
-
 func getEgressRequestCountOrFail(ctx framework.TestContext, ns namespace.Instance, prom prometheus.Instance) int {
 	query := fmt.Sprintf("istio_requests_total{destination_app=\"%s\",source_workload_namespace=\"%s\"}",
 		egressName, ns.Name())
 	ctx.Helper()
 
-	reqCount, err := getMetric(ctx, prom, query)
-	if err != nil {
-		// assume that if the request failed, it was because there was no metric ingested
-		// if this is not the case, the test will fail down the road regardless
-		// checking for error based on string match could lead to future failure
-		reqCount = 0
-	}
-
-	return int(reqCount)
+	return int(prom.QuerySumOrFail(ctx, ctx.Clusters().Default(), query))
 }
diff --git a/tests/integration/telemetry/outboundtrafficpolicy/helper.go b/tests/integration/telemetry/outboundtrafficpolicy/helper.go
index 93b0d62edf..b2afb03887 100644
--- a/tests/integration/telemetry/outboundtrafficpolicy/helper.go
+++ b/tests/integration/telemetry/outboundtrafficpolicy/helper.go
@@ -18,18 +18,14 @@
 package outboundtrafficpolicy
 
 import (
-	"bytes"
 	"fmt"
-	"html/template"
 	"os"
 	"path"
 	"reflect"
 	"testing"
-	"time"
-
-	envoyAdmin "github.com/envoyproxy/go-control-plane/envoy/admin/v3"
 
 	"istio.io/istio/pkg/config/protocol"
+	echoclient "istio.io/istio/pkg/test/echo/client"
 	"istio.io/istio/pkg/test/echo/common"
 	"istio.io/istio/pkg/test/env"
 	"istio.io/istio/pkg/test/framework"
@@ -39,8 +35,7 @@
 	"istio.io/istio/pkg/test/framework/components/namespace"
 	"istio.io/istio/pkg/test/framework/components/prometheus"
 	"istio.io/istio/pkg/test/framework/resource"
-	"istio.io/istio/pkg/test/util/retry"
-	"istio.io/istio/pkg/test/util/structpath"
+	tmpl "istio.io/istio/pkg/test/util/tmpl"
 	promtest "istio.io/istio/tests/integration/telemetry/stats/prometheus"
 )
 
@@ -184,16 +179,8 @@ func (t TrafficPolicy) String() string {
 // We want to test "external" traffic. To do this without actually hitting an external endpoint,
 // we can import only the service namespace, so the apps are not known
 func createSidecarScope(t *testing.T, ctx resource.Context, tPolicy TrafficPolicy, appsNamespace namespace.Instance, serviceNamespace namespace.Instance) {
-	tmpl, err := template.New("SidecarScope").Parse(SidecarScope)
-	if err != nil {
-		t.Errorf("failed to create template: %v", err)
-	}
-
-	var buf bytes.Buffer
-	if err := tmpl.Execute(&buf, map[string]string{"ImportNamespace": serviceNamespace.Name(), "TrafficPolicyMode": tPolicy.String()}); err != nil {
-		t.Errorf("failed to create template: %v", err)
-	}
-	if err := ctx.ConfigIstio().ApplyYAML(appsNamespace.Name(), buf.String()); err != nil {
+	b := tmpl.EvaluateOrFail(t, SidecarScope, map[string]string{"ImportNamespace": serviceNamespace.Name(), "TrafficPolicyMode": tPolicy.String()})
+	if err := ctx.ConfigIstio().ApplyYAML(appsNamespace.Name(), b); err != nil {
 		t.Errorf("failed to apply service entries: %v", err)
 	}
 }
@@ -209,17 +196,9 @@ func mustReadCert(t *testing.T, f string) string {
 // We want to test "external" traffic. To do this without actually hitting an external endpoint,
 // we can import only the service namespace, so the apps are not known
 func createGateway(t *testing.T, ctx resource.Context, appsNamespace namespace.Instance, serviceNamespace namespace.Instance) {
-	tmpl, err := template.New("Gateway").Parse(Gateway)
-	if err != nil {
-		t.Fatalf("failed to create template: %v", err)
-	}
-
-	var buf bytes.Buffer
-	if err := tmpl.Execute(&buf, map[string]string{"AppNamespace": appsNamespace.Name()}); err != nil {
-		t.Fatalf("failed to create template: %v", err)
-	}
-	if err := ctx.ConfigIstio().ApplyYAML(serviceNamespace.Name(), buf.String()); err != nil {
-		t.Fatalf("failed to apply gateway: %v. template: %v", err, buf.String())
+	b := tmpl.EvaluateOrFail(t, Gateway, map[string]string{"AppNamespace": appsNamespace.Name()})
+	if err := ctx.ConfigIstio().ApplyYAML(serviceNamespace.Name(), b); err != nil {
+		t.Fatalf("failed to apply gateway: %v. template: %v", err, b)
 	}
 }
 
@@ -273,40 +252,37 @@ func RunExternalRequest(cases []*TestCase, prometheus prometheus.Instance, mode
 
 			for _, tc := range cases {
 				t.Run(tc.Name, func(t *testing.T) {
-					retry.UntilSuccessOrFail(t, func() error {
-						resp, err := client.Call(echo.CallOptions{
-							Target:   dest,
-							PortName: tc.PortName,
-							Headers: map[string][]string{
-								"Host": {tc.Host},
-							},
-							HTTP2: tc.HTTP2,
-						})
-
-						// the expected response from a blackhole test case will have err
-						// set; use the length of the expected code to ignore this condition
-						if err != nil && len(tc.Expected.ResponseCode) != 0 {
-							return fmt.Errorf("request failed: %v", err)
-						}
-
-						codes := make([]string, 0, len(resp))
-						for _, r := range resp {
-							codes = append(codes, r.Code)
-						}
-						if !reflect.DeepEqual(codes, tc.Expected.ResponseCode) {
-							return fmt.Errorf("got codes %q, expected %q", codes, tc.Expected.ResponseCode)
-						}
+					client.CallWithRetryOrFail(t, echo.CallOptions{
+						Target:   dest,
+						PortName: tc.PortName,
+						Headers: map[string][]string{
+							"Host": {tc.Host},
+						},
+						HTTP2: tc.HTTP2,
+						Validator: echo.ValidatorFunc(func(resp echoclient.ParsedResponses, err error) error {
+							// the expected response from a blackhole test case will have err
+							// set; use the length of the expected code to ignore this condition
+							if err != nil && len(tc.Expected.ResponseCode) != 0 {
+								return fmt.Errorf("request failed: %v", err)
+							}
+							codes := make([]string, 0, len(resp))
+							for _, r := range resp {
+								codes = append(codes, r.Code)
+							}
+							if !reflect.DeepEqual(codes, tc.Expected.ResponseCode) {
+								return fmt.Errorf("got codes %q, expected %q", codes, tc.Expected.ResponseCode)
+							}
 
-						for _, r := range resp {
-							for k, v := range tc.Expected.Metadata {
-								if got := r.RawResponse[k]; got != v {
-									return fmt.Errorf("expected metadata %v=%v, got %q", k, v, got)
+							for _, r := range resp {
+								for k, v := range tc.Expected.Metadata {
+									if got := r.RawResponse[k]; got != v {
+										return fmt.Errorf("expected metadata %v=%v, got %q", k, v, got)
+									}
 								}
 							}
-						}
-
-						return nil
-					}, retry.Delay(time.Second), retry.Timeout(20*time.Second))
+							return nil
+						}),
+					})
 
 					if tc.Expected.Metric != "" {
 						promtest.ValidateMetric(t, ctx.Clusters().Default(), prometheus, tc.Expected.PromQueryFormat, tc.Expected.Metric, 1)
@@ -326,6 +302,9 @@ func setupEcho(t *testing.T, ctx resource.Context, mode TrafficPolicy) (echo.Ins
 		Inject: true,
 	})
 
+	// External traffic should work even if we have service entries on the same ports
+	createSidecarScope(t, ctx, mode, appsNamespace, serviceNamespace)
+
 	var client, dest echo.Instance
 	echoboot.NewBuilder(ctx).
 		With(&client, echo.Config{
@@ -380,8 +359,6 @@ func setupEcho(t *testing.T, ctx resource.Context, mode TrafficPolicy) (echo.Ins
 			},
 		}).BuildOrFail(t)
 
-	// External traffic should work even if we have service entries on the same ports
-	createSidecarScope(t, ctx, mode, appsNamespace, serviceNamespace)
 	if err := ctx.ConfigIstio().ApplyYAML(serviceNamespace.Name(), ServiceEntry); err != nil {
 		t.Errorf("failed to apply service entries: %v", err)
 	}
@@ -389,43 +366,5 @@ func setupEcho(t *testing.T, ctx resource.Context, mode TrafficPolicy) (echo.Ins
 	if _, kube := ctx.Environment().(*kube.Environment); kube {
 		createGateway(t, ctx, appsNamespace, serviceNamespace)
 	}
-	if err := WaitUntilNotCallable(client, dest); err != nil {
-		t.Fatalf("failed to apply sidecar, %v", err)
-	}
 	return client, dest
 }
-
-func clusterName(target echo.Instance, port echo.Port) string {
-	cfg := target.Config()
-	return fmt.Sprintf("outbound|%d||%s.%s.svc.%s", port.ServicePort, cfg.Service, cfg.Namespace.Name(), cfg.Domain)
-}
-
-// Wait for the destination to NOT be callable by the client. This allows us to simulate external traffic.
-// This essentially just waits for the Sidecar to be applied, without sleeping.
-func WaitUntilNotCallable(c echo.Instance, dest echo.Instance) error {
-	accept := func(cfg *envoyAdmin.ConfigDump) (bool, error) {
-		validator := structpath.ForProto(cfg)
-		for _, port := range dest.Config().Ports {
-			clusterName := clusterName(dest, port)
-			// Ensure that we have an outbound configuration for the target port.
-			err := validator.NotExists("{.configs[*].dynamicActiveClusters[?(@.cluster.Name == '%s')]}", clusterName).Check()
-			if err != nil {
-				return false, err
-			}
-		}
-
-		return true, nil
-	}
-
-	workloads, _ := c.Workloads()
-	// Wait for the outbound config to be received by each workload from Pilot.
-	for _, w := range workloads {
-		if w.Sidecar() != nil {
-			if err := w.Sidecar().WaitForConfig(accept, retry.Timeout(time.Second*10)); err != nil {
-				return err
-			}
-		}
-	}
-
-	return nil
-}
diff --git a/tests/integration/telemetry/stats/prometheus/customizemetrics/customize_metrics_test.go b/tests/integration/telemetry/stats/prometheus/customizemetrics/customize_metrics_test.go
index 5c7fc32114..c7e22bf4f9 100644
--- a/tests/integration/telemetry/stats/prometheus/customizemetrics/customize_metrics_test.go
+++ b/tests/integration/telemetry/stats/prometheus/customizemetrics/customize_metrics_test.go
@@ -31,6 +31,7 @@
 	"istio.io/istio/pkg/test/framework/components/echo"
 	"istio.io/istio/pkg/test/framework/components/echo/echoboot"
 	"istio.io/istio/pkg/test/framework/components/istio"
+	"istio.io/istio/pkg/test/framework/components/istioctl"
 	"istio.io/istio/pkg/test/framework/components/namespace"
 	"istio.io/istio/pkg/test/framework/components/prometheus"
 	"istio.io/istio/pkg/test/framework/label"
@@ -39,6 +40,7 @@
 	"istio.io/istio/pkg/test/util/tmpl"
 	util "istio.io/istio/tests/integration/telemetry"
 	common "istio.io/istio/tests/integration/telemetry/stats/prometheus"
+	"istio.io/pkg/log"
 )
 
 var (
@@ -61,6 +63,11 @@ func TestCustomizeMetrics(t *testing.T) {
 		Features("observability.telemetry.request-classification").
 		Features("extensibility.wasm.remote-load").
 		Run(func(ctx framework.TestContext) {
+			ctx.Cleanup(func() {
+				if ctx.Failed() {
+					util.PromDump(ctx.Clusters().Default(), promInst, "istio_requests_total")
+				}
+			})
 			httpDestinationQuery := buildQuery(httpProtocol)
 			grpcDestinationQuery := buildQuery(grpcProtocol)
 			var httpMetricVal string
@@ -75,18 +82,16 @@ func TestCustomizeMetrics(t *testing.T) {
 				if !httpChecked {
 					httpMetricVal, err = common.QueryPrometheus(t, ctx.Clusters().Default(), httpDestinationQuery, promInst)
 					if err != nil {
-						t.Logf("http: prometheus values for istio_requests_total: \n%s", util.PromDump(ctx.Clusters().Default(), promInst, "istio_requests_total"))
 						return err
 					}
 					httpChecked = true
 				}
 				_, err = common.QueryPrometheus(t, ctx.Clusters().Default(), grpcDestinationQuery, promInst)
 				if err != nil {
-					t.Logf("grpc: prometheus values for istio_requests_total: \n%s", util.PromDump(ctx.Clusters().Default(), promInst, "istio_requests_total"))
 					return err
 				}
 				return nil
-			}, retry.Delay(6*time.Second), retry.Timeout(300*time.Second))
+			}, retry.Delay(1*time.Second), retry.Timeout(300*time.Second))
 			// check tag removed
 			if strings.Contains(httpMetricVal, removedTag) {
 				t.Errorf("failed to remove tag: %v", removedTag)
@@ -247,7 +252,14 @@ func setupEnvoyFilter(ctx resource.Context) error {
 		return err
 	}
 	// Ensure bootstrap patch is applied before starting echo.
-	time.Sleep(time.Minute)
+	ik, err := istioctl.New(ctx, istioctl.Config{Cluster: ctx.Clusters()[0]})
+	if err != nil {
+		return err
+	}
+	// calling istioctl invoke in parallel can cause issues due to heavy package-var usage
+	if err := ik.WaitForConfigs("istio-system", bootstrapPatch); err != nil {
+		log.Warnf("failed to wait for config: %v", err)
+	}
 	return nil
 }
 
diff --git a/tests/integration/telemetry/stats/prometheus/stats.go b/tests/integration/telemetry/stats/prometheus/stats.go
index c8a0a3f76a..c51bbb1b6d 100644
--- a/tests/integration/telemetry/stats/prometheus/stats.go
+++ b/tests/integration/telemetry/stats/prometheus/stats.go
@@ -111,24 +111,25 @@ func TestStatsFilter(t *testing.T, feature features.Feature) {
 							sourceCluster = c.Name()
 						}
 						sourceQuery, destinationQuery, appQuery := buildQuery(sourceCluster)
+						prom := GetPromInstance()
 						// Query client side metrics
-						if _, err := QueryPrometheus(t, c, sourceQuery, GetPromInstance()); err != nil {
+						if _, err := prom.QuerySum(c, sourceQuery); err != nil {
 							t.Logf("prometheus values for istio_requests_total for cluster %v: \n%s", c.Name(), util.PromDump(c, promInst, "istio_requests_total"))
 							return err
 						}
 						// Query client side metrics for non-injected server
 						outOfMeshServerQuery := buildOutOfMeshServerQuery(sourceCluster)
-						if _, err := QueryPrometheus(t, c, outOfMeshServerQuery, GetPromInstance()); err != nil {
+						if _, err := prom.QuerySum(c, outOfMeshServerQuery); err != nil {
 							t.Logf("prometheus values for istio_requests_total for cluster %v: \n%s", c.Name(), util.PromDump(c, promInst, "istio_requests_total"))
 							return err
 						}
 						// Query server side metrics.
-						if _, err := QueryPrometheus(t, c, destinationQuery, GetPromInstance()); err != nil {
+						if _, err := prom.QuerySum(c, destinationQuery); err != nil {
 							t.Logf("prometheus values for istio_requests_total for cluster %v: \n%s", c.Name(), util.PromDump(c, promInst, "istio_requests_total"))
 							return err
 						}
 						// This query will continue to increase due to readiness probe; don't wait for it to converge
-						if err := QueryFirstPrometheus(t, c, appQuery, GetPromInstance()); err != nil {
+						if _, err := prom.QuerySum(c, appQuery); err != nil {
 							t.Logf("prometheus values for istio_echo_http_requests_total for cluster %v: \n%s",
 								c.Name(), util.PromDump(c, promInst, "istio_echo_http_requests_total"))
 							return err
@@ -149,7 +150,7 @@ func TestStatsFilter(t *testing.T, feature features.Feature) {
 			// In addition, verifies that mocked prometheus could call metrics endpoint with proxy provisioned certs
 			for _, prom := range mockProm {
 				st := server.GetOrFail(ctx, echo.InCluster(prom.Config().Cluster))
-				_, err := prom.Call(echo.CallOptions{
+				prom.CallWithRetryOrFail(t, echo.CallOptions{
 					Address:            st.WorkloadsOrFail(t)[0].Address(),
 					Scheme:             scheme.HTTPS,
 					Port:               &echo.Port{ServicePort: 15014},
@@ -159,9 +160,6 @@ func TestStatsFilter(t *testing.T, feature features.Feature) {
 					CaCertFile:         "/etc/certs/custom/root-cert.pem",
 					InsecureSkipVerify: true,
 				})
-				if err != nil {
-					t.Fatalf("test failed: %v", err)
-				}
 			}
 		})
 }
@@ -186,7 +184,7 @@ func TestStatsTCPFilter(t *testing.T, feature features.Feature) {
 							sourceCluster = c.Name()
 						}
 						destinationQuery := buildTCPQuery(sourceCluster)
-						if _, err := QueryPrometheus(t, c, destinationQuery, GetPromInstance()); err != nil {
+						if _, err := GetPromInstance().Query(c, destinationQuery); err != nil {
 							t.Logf("prometheus values for istio_tcp_connections_opened_total: \n%s", util.PromDump(c, promInst, "istio_tcp_connections_opened_total"))
 							return err
 						}
diff --git a/tests/integration/telemetry/stats/prometheus/util_prometheus.go b/tests/integration/telemetry/stats/prometheus/util_prometheus.go
index 417eb29dd4..0ec31a3cf7 100644
--- a/tests/integration/telemetry/stats/prometheus/util_prometheus.go
+++ b/tests/integration/telemetry/stats/prometheus/util_prometheus.go
@@ -19,7 +19,6 @@
 
 import (
 	"fmt"
-	"strings"
 	"testing"
 	"time"
 
@@ -31,11 +30,12 @@
 // QueryPrometheus queries prometheus and returns the result once the query stabilizes
 func QueryPrometheus(t *testing.T, cluster cluster.Cluster, query string, promInst prometheus.Instance) (string, error) {
 	t.Logf("query prometheus with: %v", query)
-	val, err := promInst.WaitForQuiesceForCluster(cluster, query)
+
+	val, err := promInst.Query(cluster, query)
 	if err != nil {
 		return "", err
 	}
-	got, err := promInst.Sum(val, nil)
+	got, err := prometheus.Sum(val)
 	if err != nil {
 		t.Logf("value: %s", val.String())
 		return "", fmt.Errorf("could not find metric value: %v", err)
@@ -44,69 +44,16 @@ func QueryPrometheus(t *testing.T, cluster cluster.Cluster, query string, promIn
 	return val.String(), nil
 }
 
-// QueryFirstPrometheus queries prometheus and returns the result once a timeseries exists
-func QueryFirstPrometheus(t *testing.T, cluster cluster.Cluster, query string, promInst prometheus.Instance) error {
-	t.Logf("query prometheus with: %v", query)
-	val, err := promInst.WaitForOneOrMoreForCluster(cluster, query)
-	if err != nil {
-		return err
-	}
-	got, err := promInst.Sum(val, nil)
-	if err != nil {
-		t.Logf("value: %s", val.String())
-		return fmt.Errorf("could not find metric value: %v", err)
-	}
-	t.Logf("get value %v", got)
-	return nil
-}
-
 func ValidateMetric(t *testing.T, cluster cluster.Cluster, prometheus prometheus.Instance, query, metricName string, want float64) {
-	var got float64
 	retry.UntilSuccessOrFail(t, func() error {
-		var err error
-		got, err = getMetric(t, cluster, prometheus, query, metricName)
-		return err
+		got, err := prometheus.QuerySum(cluster, query)
+		t.Logf("%s: %f", metricName, got)
+		if err != nil {
+			return err
+		}
+		if got < want {
+			return fmt.Errorf("bad metric value: got %f, want at least %f", got, want)
+		}
+		return nil
 	}, retry.Delay(time.Second), retry.Timeout(2*time.Minute))
-
-	t.Logf("%s: %f", metricName, got)
-	if got < want {
-		t.Logf("prometheus values for %s:\n%s", metricName, PromDump(cluster, prometheus, metricName))
-		t.Errorf("bad metric value: got %f, want at least %f", got, want)
-	}
-}
-
-func getMetric(t *testing.T, cluster cluster.Cluster, prometheus prometheus.Instance, query, metricName string) (float64, error) {
-	t.Helper()
-
-	t.Logf("prometheus query: %s", query)
-	value, err := prometheus.WaitForQuiesceForCluster(cluster, query)
-	if err != nil {
-		return 0, fmt.Errorf("could not get metrics from prometheus: %v", err)
-	}
-
-	got, err := prometheus.Sum(value, nil)
-	if err != nil {
-		t.Logf("value: %s", value.String())
-		t.Logf("prometheus values for %s:\n%s", metricName, PromDump(cluster, prometheus, metricName))
-		return 0, fmt.Errorf("could not find metric value: %v", err)
-	}
-
-	return got, nil
-}
-
-// promDump gets all of the recorded values for a metric by name and generates a report of the values.
-// used for debugging of failures to provide a comprehensive view of traffic experienced.
-func PromDump(cluster cluster.Cluster, prometheus prometheus.Instance, metric string) string {
-	return PromDumpWithAttributes(cluster, prometheus, metric, nil)
-}
-
-// promDumpWithAttributes is used to get all of the recorded values of a metric for particular attributes.
-// Attributes have to be of format %s=\"%s\"
-// nolint: unparam
-func PromDumpWithAttributes(cluster cluster.Cluster, prometheus prometheus.Instance, metric string, attributes []string) string {
-	if value, err := prometheus.WaitForQuiesceForCluster(cluster, fmt.Sprintf("%s{%s}", metric, strings.Join(attributes, ", "))); err == nil {
-		return value.String()
-	}
-
-	return ""
 }
diff --git a/tests/integration/telemetry/util.go b/tests/integration/telemetry/util.go
index 178cf910fe..ec4be31cee 100644
--- a/tests/integration/telemetry/util.go
+++ b/tests/integration/telemetry/util.go
@@ -19,8 +19,6 @@
 
 import (
 	"context"
-	"fmt"
-	"strings"
 
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 
@@ -29,17 +27,10 @@
 	"istio.io/istio/pkg/test/framework/components/prometheus"
 )
 
-// promDump gets all of the recorded values for a metric by name and generates a report of the values.
+// PromDump gets all of the recorded values for a metric by name and generates a report of the values.
 // used for debugging of failures to provide a comprehensive view of traffic experienced.
 func PromDump(cluster cluster.Cluster, prometheus prometheus.Instance, metric string) string {
-	return PromDumpWithAttributes(cluster, prometheus, metric, nil)
-}
-
-// promDumpWithAttributes is used to get all of the recorded values of a metric for particular attributes.
-// Attributes have to be of format %s=\"%s\"
-// nolint: unparam
-func PromDumpWithAttributes(cluster cluster.Cluster, prometheus prometheus.Instance, metric string, attributes []string) string {
-	if value, err := prometheus.WaitForQuiesceForCluster(cluster, fmt.Sprintf("%s{%s}", metric, strings.Join(attributes, ", "))); err == nil {
+	if value, err := prometheus.Query(cluster, metric); err == nil {
 		return value.String()
 	}
 
-- 
2.35.3

