From be3ca0bbb3f06f4151d6353a37bb91e20cfd811c Mon Sep 17 00:00:00 2001
From: Steven Landow <landow@google.com>
Date: Thu, 14 Apr 2022 11:51:05 -0700
Subject: refactor endpointshards index into model (#38143)

* refactor endpontshards out of xds

Change-Id: I034fb1eea7cbfb3db32e765d50cc144f1873948c

* make shardkey a struct

Change-Id: I63463e6c70cfe1c18c4c532e89b14bf336b20016

* fix sets import

Change-Id: I9882e8efdac3f712d34ae6689a086e50ad3fdaaa

* address comments

Change-Id: Id33afdc26566ecf70a2b6a749af50984714a3933

* move key sorting into model

Change-Id: Iaf087aeaf4783c31d7f01355f1d4ec8dcf2459a3

* update comment

Change-Id: I35364eed069a5005c819b85fd496874c530e6155

* sk as struct fixups

Change-Id: I9fba314e8e4d6c81d97c7a23759c949e69d5f9a0

* lint

Change-Id: I06b6a0c6632187eb2bb20ada14ade8000acc5142

* apply gencheck patch

Change-Id: I0cd33d8c6bcad266a390ef2bba6da7bfdef4b43a

* Update pilot/pkg/xds/debug.go

Co-authored-by: John Howard <howardjohn@google.com>

* change lock in endpoint builder

Change-Id: I676f8fc9b5740bc8ac78980babd3b7136e34f32c

Co-authored-by: John Howard <howardjohn@google.com>
---
 pilot/pkg/model/endpointshards.go             | 216 ++++++++++++++++++
 pilot/pkg/model/push_context.go               |  27 ---
 pilot/pkg/networking/core/v1alpha3/fake.go    |   2 +-
 .../serviceregistry/kube/controller/fake.go   |   2 +-
 pilot/pkg/serviceregistry/memory/discovery.go |   4 +-
 pilot/pkg/xds/debug.go                        |   4 +-
 pilot/pkg/xds/discovery.go                    |  51 ++---
 pilot/pkg/xds/eds.go                          | 114 +--------
 pilot/pkg/xds/eds_test.go                     |  63 ++---
 pilot/pkg/xds/endpoint_builder.go             |  22 +-
 pilot/pkg/xds/ep_filters_test.go              |  20 +-
 11 files changed, 295 insertions(+), 230 deletions(-)
 create mode 100644 pilot/pkg/model/endpointshards.go

diff --git a/pilot/pkg/model/endpointshards.go b/pilot/pkg/model/endpointshards.go
new file mode 100644
index 0000000000..d3874a1aad
--- /dev/null
+++ b/pilot/pkg/model/endpointshards.go
@@ -0,0 +1,216 @@
+// Copyright Istio Authors
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+
+package model
+
+import (
+	"fmt"
+	"sort"
+	"sync"
+
+	"istio.io/istio/pilot/pkg/serviceregistry/provider"
+	"istio.io/istio/pkg/cluster"
+	"istio.io/istio/pkg/config/schema/gvk"
+	"istio.io/istio/pkg/util/sets"
+)
+
+// shardRegistry is a simplified interface for registries that can produce a shard key
+type shardRegistry interface {
+	Cluster() cluster.ID
+	Provider() provider.ID
+}
+
+// ShardKeyFromRegistry computes the shard key based on provider type and cluster id.
+func ShardKeyFromRegistry(instance shardRegistry) ShardKey {
+	return ShardKey{Cluster: instance.Cluster(), Provider: instance.Provider()}
+}
+
+// ShardKey is the key for EndpointShards made of a key with the format "cluster/provider"
+type ShardKey struct {
+	Cluster  cluster.ID
+	Provider provider.ID
+}
+
+func (sk ShardKey) String() string {
+	return fmt.Sprintf("%s/%s", sk.Provider, sk.Cluster)
+}
+
+// EndpointShards holds the set of endpoint shards of a service. Registries update
+// individual shards incrementally. The shards are aggregated and split into
+// clusters when a push for the specific cluster is needed.
+type EndpointShards struct {
+	// mutex protecting below map.
+	sync.RWMutex
+
+	// Shards is used to track the shards. EDS updates are grouped by shard.
+	// Current implementation uses the registry name as key - in multicluster this is the
+	// name of the k8s cluster, derived from the config (secret).
+	Shards map[ShardKey][]*IstioEndpoint
+
+	// ServiceAccounts has the concatenation of all service accounts seen so far in endpoints.
+	// This is updated on push, based on shards. If the previous list is different than
+	// current list, a full push will be forced, to trigger a secure naming update.
+	// Due to the larger time, it is still possible that connection errors will occur while
+	// CDS is updated.
+	ServiceAccounts sets.Set
+}
+
+// Keys gives a sorted list of keys for EndpointShards.Shards.
+// Calls to Keys should be guarded with a lock on the EndpointShards.
+func (es *EndpointShards) Keys() []ShardKey {
+	// len(shards) ~= number of remote clusters which isn't too large, doing this sort frequently
+	// shouldn't be too problematic. If it becomes an issue we can cache it in the EndpointShards struct.
+	keys := make([]ShardKey, 0, len(es.Shards))
+	for k := range es.Shards {
+		keys = append(keys, k)
+	}
+	if len(keys) >= 2 {
+		sort.Slice(keys, func(i, j int) bool {
+			if keys[i].Provider == keys[j].Provider {
+				return keys[i].Cluster < keys[j].Cluster
+			}
+			return keys[i].Provider < keys[j].Provider
+		})
+	}
+	return keys
+}
+
+// EndpointIndex is a mutex protected index of endpoint shards
+type EndpointIndex struct {
+	mu sync.RWMutex
+	// keyed by svc then ns
+	shardsBySvc map[string]map[string]*EndpointShards
+	// We'll need to clear the cache in-sync with endpoint shards modifications.
+	cache XdsCache
+}
+
+func NewEndpointIndex() *EndpointIndex {
+	return &EndpointIndex{
+		shardsBySvc: make(map[string]map[string]*EndpointShards),
+	}
+}
+
+func (e *EndpointIndex) SetCache(cache XdsCache) {
+	e.mu.Lock()
+	defer e.mu.Unlock()
+	e.cache = cache
+}
+
+// must be called with lock
+func (e *EndpointIndex) clearCacheForService(svc, ns string) {
+	if e.cache == nil {
+		return
+	}
+	e.cache.Clear(map[ConfigKey]struct{}{{
+		Kind:      gvk.ServiceEntry,
+		Name:      svc,
+		Namespace: ns,
+	}: {}})
+}
+
+// Shardz returns a copy of the global map of shards but does NOT copy the underlying individual EndpointShards.
+func (e *EndpointIndex) Shardz() map[string]map[string]*EndpointShards {
+	e.mu.RLock()
+	defer e.mu.RUnlock()
+	out := make(map[string]map[string]*EndpointShards, len(e.shardsBySvc))
+	for svcKey, v := range e.shardsBySvc {
+		out[svcKey] = make(map[string]*EndpointShards, len(v))
+		for nsKey, v := range v {
+			out[svcKey][nsKey] = v
+		}
+	}
+	return out
+}
+
+// ShardsForService returns the shards and true if they are found, or returns nil, false.
+func (e *EndpointIndex) ShardsForService(serviceName, namespace string) (*EndpointShards, bool) {
+	e.mu.RLock()
+	defer e.mu.RUnlock()
+	byNs, ok := e.shardsBySvc[serviceName]
+	if !ok {
+		return nil, false
+	}
+	shards, ok := byNs[namespace]
+	return shards, ok
+}
+
+// GetOrCreateEndpointShard returns the shards. The second return parameter will be true if this service was seen
+// for the first time.
+func (e *EndpointIndex) GetOrCreateEndpointShard(serviceName, namespace string) (*EndpointShards, bool) {
+	e.mu.Lock()
+	defer e.mu.Unlock()
+
+	if _, exists := e.shardsBySvc[serviceName]; !exists {
+		e.shardsBySvc[serviceName] = map[string]*EndpointShards{}
+	}
+	if ep, exists := e.shardsBySvc[serviceName][namespace]; exists {
+		return ep, false
+	}
+	// This endpoint is for a service that was not previously loaded.
+	ep := &EndpointShards{
+		Shards:          map[ShardKey][]*IstioEndpoint{},
+		ServiceAccounts: sets.Set{},
+	}
+	e.shardsBySvc[serviceName][namespace] = ep
+	// Clear the cache here to avoid race in cache writes.
+	e.clearCacheForService(serviceName, namespace)
+	return ep, true
+}
+
+func (e *EndpointIndex) DeleteServiceShard(shard ShardKey, serviceName, namespace string, preserveKeys bool) {
+	e.mu.Lock()
+	defer e.mu.Unlock()
+	e.deleteServiceInner(shard, serviceName, namespace, preserveKeys)
+}
+
+func (e *EndpointIndex) DeleteShard(shardKey ShardKey) {
+	e.mu.Lock()
+	defer e.mu.Unlock()
+	for svc, shardsByNamespace := range e.shardsBySvc {
+		for ns := range shardsByNamespace {
+			e.deleteServiceInner(shardKey, svc, ns, false)
+		}
+	}
+	e.cache.ClearAll()
+}
+
+// must be called with lock
+func (e *EndpointIndex) deleteServiceInner(shard ShardKey, serviceName, namespace string, preserveKeys bool) {
+	if e.shardsBySvc[serviceName] == nil ||
+		e.shardsBySvc[serviceName][namespace] == nil {
+		return
+	}
+	epShards := e.shardsBySvc[serviceName][namespace]
+	epShards.Lock()
+	delete(epShards.Shards, shard)
+	epShards.ServiceAccounts = sets.Set{}
+	for _, shard := range epShards.Shards {
+		for _, ep := range shard {
+			if ep.ServiceAccount != "" {
+				epShards.ServiceAccounts.Insert(ep.ServiceAccount)
+			}
+		}
+	}
+	// Clear the cache here to avoid race in cache writes.
+	e.clearCacheForService(serviceName, namespace)
+	if !preserveKeys {
+		if len(epShards.Shards) == 0 {
+			delete(e.shardsBySvc[serviceName], namespace)
+		}
+		if len(e.shardsBySvc[serviceName]) == 0 {
+			delete(e.shardsBySvc, serviceName)
+		}
+	}
+	epShards.Unlock()
+}
diff --git a/pilot/pkg/model/push_context.go b/pilot/pkg/model/push_context.go
index c86b935fee..bdeb77ba6d 100644
--- a/pilot/pkg/model/push_context.go
+++ b/pilot/pkg/model/push_context.go
@@ -31,7 +31,6 @@
 	meshconfig "istio.io/api/mesh/v1alpha1"
 	networking "istio.io/api/networking/v1alpha3"
 	"istio.io/istio/pilot/pkg/features"
-	"istio.io/istio/pilot/pkg/serviceregistry/provider"
 	"istio.io/istio/pkg/cluster"
 	"istio.io/istio/pkg/config"
 	"istio.io/istio/pkg/config/constants"
@@ -307,32 +306,6 @@ type XDSUpdater interface {
 	RemoveShard(shardKey ShardKey)
 }
 
-// shardRegistry is a simplified interface for registries that can produce a shard key
-type shardRegistry interface {
-	Cluster() cluster.ID
-	Provider() provider.ID
-}
-
-func NewShardKey(cluster cluster.ID, provider provider.ID) ShardKey {
-	return ShardKey(fmt.Sprintf("%s/%s", cluster, provider))
-}
-
-// ShardKeyFromRegistry computes the shard key based on provider type and cluster id.
-func ShardKeyFromRegistry(instance shardRegistry) ShardKey {
-	return NewShardKey(instance.Cluster(), instance.Provider())
-}
-
-// ShardKey is the key for EndpointShards made of a key with the format "cluster/provider"
-type ShardKey string
-
-func (sk ShardKey) Cluster() cluster.ID {
-	p := strings.Split(string(sk), "/")
-	if len(p) < 1 {
-		return ""
-	}
-	return cluster.ID(p[0])
-}
-
 // PushRequest defines a request to push to proxies
 // It is used to send updates to the config update debouncer and pass to the PushQueue.
 type PushRequest struct {
diff --git a/pilot/pkg/networking/core/v1alpha3/fake.go b/pilot/pkg/networking/core/v1alpha3/fake.go
index 614ee1e962..50e2a043ec 100644
--- a/pilot/pkg/networking/core/v1alpha3/fake.go
+++ b/pilot/pkg/networking/core/v1alpha3/fake.go
@@ -137,7 +137,7 @@ func NewConfigGenTest(t test.Failer, opts TestOptions) *ConfigGenTest {
 		msd.AddInstance(instance.Service.Hostname, instance)
 	}
 	msd.AddGateways(opts.Gateways...)
-	msd.ClusterID = string(provider.Mock)
+	msd.ClusterID = cluster2.ID(provider.Mock)
 	serviceDiscovery.AddRegistry(serviceregistry.Simple{
 		ClusterID:        cluster2.ID(provider.Mock),
 		ProviderID:       provider.Mock,
diff --git a/pilot/pkg/serviceregistry/kube/controller/fake.go b/pilot/pkg/serviceregistry/kube/controller/fake.go
index 60b73e1862..60a36759c9 100644
--- a/pilot/pkg/serviceregistry/kube/controller/fake.go
+++ b/pilot/pkg/serviceregistry/kube/controller/fake.go
@@ -109,7 +109,7 @@ func (fx *FakeXdsUpdater) SvcUpdate(_ model.ShardKey, hostname string, _ string,
 
 func (fx *FakeXdsUpdater) RemoveShard(shardKey model.ShardKey) {
 	select {
-	case fx.Events <- FakeXdsEvent{Type: "removeShard", ID: string(shardKey)}:
+	case fx.Events <- FakeXdsEvent{Type: "removeShard", ID: shardKey.String()}:
 	default:
 	}
 }
diff --git a/pilot/pkg/serviceregistry/memory/discovery.go b/pilot/pkg/serviceregistry/memory/discovery.go
index 2f266e63ed..dfc0f5047f 100644
--- a/pilot/pkg/serviceregistry/memory/discovery.go
+++ b/pilot/pkg/serviceregistry/memory/discovery.go
@@ -68,7 +68,7 @@ type ServiceDiscovery struct {
 	WantGetProxyServiceInstances []*model.ServiceInstance
 	InstancesError               error
 	Controller                   model.Controller
-	ClusterID                    string
+	ClusterID                    cluster.ID
 
 	// Used by GetProxyWorkloadLabels
 	ip2workloadLabels map[string]labels.Instance
@@ -99,7 +99,7 @@ func NewServiceDiscovery(services ...*model.Service) *ServiceDiscovery {
 }
 
 func (sd *ServiceDiscovery) shardKey() model.ShardKey {
-	return model.NewShardKey(cluster.ID(sd.ClusterID), provider.Mock)
+	return model.ShardKey{Cluster: sd.ClusterID, Provider: provider.Mock}
 }
 
 func (sd *ServiceDiscovery) AddWorkload(ip string, labels labels.Instance) {
diff --git a/pilot/pkg/xds/debug.go b/pilot/pkg/xds/debug.go
index 4094bff953..ecf4346c42 100644
--- a/pilot/pkg/xds/debug.go
+++ b/pilot/pkg/xds/debug.go
@@ -302,9 +302,7 @@ func (s *DiscoveryServer) registryz(w http.ResponseWriter, req *http.Request) {
 // the full push.
 func (s *DiscoveryServer) endpointShardz(w http.ResponseWriter, req *http.Request) {
 	w.Header().Add("Content-Type", "application/json")
-	s.mutex.RLock()
-	out, _ := json.MarshalIndent(s.EndpointShardsByService, " ", " ")
-	s.mutex.RUnlock()
+	out, _ := json.MarshalIndent(s.EndpointIndex.Shardz(), " ", " ")
 	_, _ = w.Write(out)
 }
 
diff --git a/pilot/pkg/xds/discovery.go b/pilot/pkg/xds/discovery.go
index 3e167098bc..abcadb7369 100644
--- a/pilot/pkg/xds/discovery.go
+++ b/pilot/pkg/xds/discovery.go
@@ -41,7 +41,6 @@
 	v3 "istio.io/istio/pilot/pkg/xds/v3"
 	"istio.io/istio/pkg/cluster"
 	"istio.io/istio/pkg/security"
-	"istio.io/istio/pkg/util/sets"
 )
 
 var (
@@ -109,11 +108,9 @@ type DiscoveryServer struct {
 	// the push context, which means that the next push to a proxy will receive this configuration.
 	CommittedUpdates *atomic.Int64
 
-	// mutex used for protecting shards.
-	mutex sync.RWMutex
 	// EndpointShards for a service. This is a global (per-server) list, built from
 	// incremental updates. This is keyed by service and namespace
-	EndpointShardsByService map[string]map[string]*EndpointShards
+	EndpointIndex *model.EndpointIndex
 
 	// pushChannel is the buffer used for debouncing.
 	// after debouncing the pushRequest will be sent to pushQueue
@@ -162,42 +159,22 @@ type DiscoveryServer struct {
 	ClusterAliases map[cluster.ID]cluster.ID
 }
 
-// EndpointShards holds the set of endpoint shards of a service. Registries update
-// individual shards incrementally. The shards are aggregated and split into
-// clusters when a push for the specific cluster is needed.
-type EndpointShards struct {
-	// mutex protecting below map.
-	mutex sync.RWMutex
-
-	// Shards is used to track the shards. EDS updates are grouped by shard.
-	// Current implementation uses the registry name as key - in multicluster this is the
-	// name of the k8s cluster, derived from the config (secret).
-	Shards map[model.ShardKey][]*model.IstioEndpoint
-
-	// ServiceAccounts has the concatenation of all service accounts seen so far in endpoints.
-	// This is updated on push, based on shards. If the previous list is different than
-	// current list, a full push will be forced, to trigger a secure naming update.
-	// Due to the larger time, it is still possible that connection errors will occur while
-	// CDS is updated.
-	ServiceAccounts sets.Set
-}
-
 // NewDiscoveryServer creates DiscoveryServer that sources data from Pilot's internal mesh data structures
 func NewDiscoveryServer(env *model.Environment, plugins []string, instanceID string, systemNameSpace string,
 	clusterAliases map[string]string) *DiscoveryServer {
 	out := &DiscoveryServer{
-		Env:                     env,
-		Generators:              map[string]model.XdsResourceGenerator{},
-		ProxyNeedsPush:          DefaultProxyNeedsPush,
-		EndpointShardsByService: map[string]map[string]*EndpointShards{},
-		concurrentPushLimit:     make(chan struct{}, features.PushThrottle),
-		requestRateLimit:        rate.NewLimiter(rate.Limit(features.RequestLimit), 1),
-		InboundUpdates:          atomic.NewInt64(0),
-		CommittedUpdates:        atomic.NewInt64(0),
-		pushChannel:             make(chan *model.PushRequest, 10),
-		pushQueue:               NewPushQueue(),
-		debugHandlers:           map[string]string{},
-		adsClients:              map[string]*Connection{},
+		Env:                 env,
+		Generators:          map[string]model.XdsResourceGenerator{},
+		ProxyNeedsPush:      DefaultProxyNeedsPush,
+		EndpointIndex:       model.NewEndpointIndex(),
+		concurrentPushLimit: make(chan struct{}, features.PushThrottle),
+		requestRateLimit:    rate.NewLimiter(rate.Limit(features.RequestLimit), 1),
+		InboundUpdates:      atomic.NewInt64(0),
+		CommittedUpdates:    atomic.NewInt64(0),
+		pushChannel:         make(chan *model.PushRequest, 10),
+		pushQueue:           NewPushQueue(),
+		debugHandlers:       map[string]string{},
+		adsClients:          map[string]*Connection{},
 		debounceOptions: debounceOptions{
 			debounceAfter:     features.DebounceAfter,
 			debounceMax:       features.DebounceMax,
@@ -216,6 +193,8 @@ func NewDiscoveryServer(env *model.Environment, plugins []string, instanceID str
 
 	if features.EnableXDSCaching {
 		out.Cache = model.NewXdsCache()
+		// clear the cache as endpoint shards are modified to avoid cache write race
+		out.EndpointIndex.SetCache(out.Cache)
 	}
 
 	out.ConfigGenerator = core.NewConfigGenerator(plugins, out.Cache)
diff --git a/pilot/pkg/xds/eds.go b/pilot/pkg/xds/eds.go
index af07fceb34..7bd07de98a 100644
--- a/pilot/pkg/xds/eds.go
+++ b/pilot/pkg/xds/eds.go
@@ -86,11 +86,11 @@ func (s *DiscoveryServer) UpdateServiceShards(push *model.PushContext) error {
 
 // SvcUpdate is a callback from service discovery when service info changes.
 func (s *DiscoveryServer) SvcUpdate(shard model.ShardKey, hostname string, namespace string, event model.Event) {
-	// When a service deleted, we should cleanup the endpoint shards and also remove keys from EndpointShardsByService to
+	// When a service deleted, we should cleanup the endpoint shards and also remove keys from EndpointIndex to
 	// prevent memory leaks.
 	if event == model.EventDelete {
 		inboundServiceDeletes.Increment()
-		s.deleteService(shard, hostname, namespace)
+		s.EndpointIndex.DeleteServiceShard(shard, hostname, namespace, false)
 	} else {
 		inboundServiceUpdates.Increment()
 	}
@@ -141,25 +141,25 @@ func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string,
 	istioEndpoints []*model.IstioEndpoint) PushType {
 	if len(istioEndpoints) == 0 {
 		// Should delete the service EndpointShards when endpoints become zero to prevent memory leak,
-		// but we should not delete the keys from EndpointShardsByService map - that will trigger
+		// but we should not delete the keys from EndpointIndex map - that will trigger
 		// unnecessary full push which can become a real problem if a pod is in crashloop and thus endpoints
 		// flip flopping between 1 and 0.
-		s.deleteEndpointShards(shard, hostname, namespace)
+		s.EndpointIndex.DeleteServiceShard(shard, hostname, namespace, true)
 		log.Infof("Incremental push, service %s at shard %v has no endpoints", hostname, shard)
 		return IncrementalPush
 	}
 
 	pushType := IncrementalPush
 	// Find endpoint shard for this service, if it is available - otherwise create a new one.
-	ep, created := s.getOrCreateEndpointShard(hostname, namespace)
+	ep, created := s.EndpointIndex.GetOrCreateEndpointShard(hostname, namespace)
 	// If we create a new endpoint shard, that means we have not seen the service earlier. We should do a full push.
 	if created {
 		log.Infof("Full push, new service %s/%s", namespace, hostname)
 		pushType = FullPush
 	}
 
-	ep.mutex.Lock()
-	defer ep.mutex.Unlock()
+	ep.Lock()
+	defer ep.Unlock()
 	newIstioEndpoints := istioEndpoints
 	if features.SendUnhealthyEndpoints {
 		oldIstioEndpoints := ep.Shards[shard]
@@ -240,104 +240,12 @@ func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string,
 }
 
 func (s *DiscoveryServer) RemoveShard(shardKey model.ShardKey) {
-	s.mutex.Lock()
-	defer s.mutex.Unlock()
-	for svc, shardsByNamespace := range s.EndpointShardsByService {
-		for ns := range shardsByNamespace {
-			s.deleteServiceInner(shardKey, svc, ns)
-		}
-	}
-	s.Cache.ClearAll()
-}
-
-func (s *DiscoveryServer) getOrCreateEndpointShard(serviceName, namespace string) (*EndpointShards, bool) {
-	s.mutex.Lock()
-	defer s.mutex.Unlock()
-
-	if _, exists := s.EndpointShardsByService[serviceName]; !exists {
-		s.EndpointShardsByService[serviceName] = map[string]*EndpointShards{}
-	}
-	if ep, exists := s.EndpointShardsByService[serviceName][namespace]; exists {
-		return ep, false
-	}
-	// This endpoint is for a service that was not previously loaded.
-	ep := &EndpointShards{
-		Shards:          map[model.ShardKey][]*model.IstioEndpoint{},
-		ServiceAccounts: sets.Set{},
-	}
-	s.EndpointShardsByService[serviceName][namespace] = ep
-	// Clear the cache here to avoid race in cache writes (see edsCacheUpdate for details).
-	s.Cache.Clear(map[model.ConfigKey]struct{}{{
-		Kind:      gvk.ServiceEntry,
-		Name:      serviceName,
-		Namespace: namespace,
-	}: {}})
-	return ep, true
-}
-
-// deleteEndpointShards deletes matching endpoint shards from EndpointShardsByService map. This is called when
-// endpoints are deleted.
-func (s *DiscoveryServer) deleteEndpointShards(shard model.ShardKey, serviceName, namespace string) {
-	s.mutex.Lock()
-	defer s.mutex.Unlock()
-	if s.EndpointShardsByService[serviceName] != nil &&
-		s.EndpointShardsByService[serviceName][namespace] != nil {
-		epShards := s.EndpointShardsByService[serviceName][namespace]
-		epShards.mutex.Lock()
-		delete(epShards.Shards, shard)
-		epShards.ServiceAccounts = sets.Set{}
-		for _, shard := range epShards.Shards {
-			for _, ep := range shard {
-				if ep.ServiceAccount != "" {
-					epShards.ServiceAccounts.Insert(ep.ServiceAccount)
-				}
-			}
-		}
-		// Clear the cache here to avoid race in cache writes (see edsCacheUpdate for details).
-		s.Cache.Clear(map[model.ConfigKey]struct{}{{
-			Kind:      gvk.ServiceEntry,
-			Name:      serviceName,
-			Namespace: namespace,
-		}: {}})
-		epShards.mutex.Unlock()
-	}
-}
-
-// deleteService deletes all service related references from EndpointShardsByService. This is called
-// when a service is deleted.
-func (s *DiscoveryServer) deleteService(shard model.ShardKey, serviceName, namespace string) {
-	s.mutex.Lock()
-	defer s.mutex.Unlock()
-	s.deleteServiceInner(shard, serviceName, namespace)
-}
-
-func (s *DiscoveryServer) deleteServiceInner(shard model.ShardKey, serviceName, namespace string) {
-	if s.EndpointShardsByService[serviceName] != nil &&
-		s.EndpointShardsByService[serviceName][namespace] != nil {
-		epShards := s.EndpointShardsByService[serviceName][namespace]
-		epShards.mutex.Lock()
-		delete(epShards.Shards, shard)
-		shardsLen := len(epShards.Shards)
-		s.UpdateServiceAccount(epShards, serviceName)
-		// Clear the cache here to avoid race in cache writes (see edsCacheUpdate for details).
-		s.Cache.Clear(map[model.ConfigKey]struct{}{{
-			Kind:      gvk.ServiceEntry,
-			Name:      serviceName,
-			Namespace: namespace,
-		}: {}})
-		epShards.mutex.Unlock()
-		if shardsLen == 0 {
-			delete(s.EndpointShardsByService[serviceName], namespace)
-		}
-		if len(s.EndpointShardsByService[serviceName]) == 0 {
-			delete(s.EndpointShardsByService, serviceName)
-		}
-	}
+	s.EndpointIndex.DeleteShard(shardKey)
 }
 
 // UpdateServiceAccount updates the service endpoints' sa when service/endpoint event happens.
 // Note: it is not concurrent safe.
-func (s *DiscoveryServer) UpdateServiceAccount(shards *EndpointShards, serviceName string) bool {
+func (s *DiscoveryServer) UpdateServiceAccount(shards *model.EndpointShards, serviceName string) bool {
 	oldServiceAccount := shards.ServiceAccounts
 	serviceAccounts := sets.Set{}
 	for _, epShards := range shards.Shards {
@@ -387,9 +295,7 @@ func (s *DiscoveryServer) llbEndpointAndOptionsForCluster(b EndpointBuilder) ([]
 		return nil, nil
 	}
 
-	s.mutex.RLock()
-	epShards, f := s.EndpointShardsByService[string(b.hostname)][b.service.Attributes.Namespace]
-	s.mutex.RUnlock()
+	epShards, f := s.EndpointIndex.ShardsForService(string(b.hostname), b.service.Attributes.Namespace)
 	if !f {
 		// Shouldn't happen here
 		log.Debugf("can not find the endpointShards for cluster %s", b.clusterName)
diff --git a/pilot/pkg/xds/eds_test.go b/pilot/pkg/xds/eds_test.go
index cde2ac22ae..83863eae24 100644
--- a/pilot/pkg/xds/eds_test.go
+++ b/pilot/pkg/xds/eds_test.go
@@ -577,9 +577,9 @@ func TestEndpointFlipFlops(t *testing.T) {
 		t.Fatalf("There should be no endpoints for outbound|8080||flipflop.com. Endpoints:\n%v", adscConn.EndpointsJSON())
 	}
 
-	// Validate that keys in service still exist in EndpointShardsByService - this prevents full push.
-	if len(s.Discovery.EndpointShardsByService["flipflop.com"]) == 0 {
-		t.Fatalf("Expected service key %s to be present in EndpointShardsByService. But missing %v", "flipflop.com", s.Discovery.EndpointShardsByService)
+	// Validate that keys in service still exist in EndpointIndex - this prevents full push.
+	if _, ok := s.Discovery.EndpointIndex.ShardsForService("flipflop.com", ""); !ok {
+		t.Fatalf("Expected service key %s to be present in EndpointIndex. But missing %v", "flipflop.com", s.Discovery.EndpointIndex.Shardz())
 	}
 
 	// Set the endpoints again and validate it does not trigger full push.
@@ -605,7 +605,7 @@ func TestEndpointFlipFlops(t *testing.T) {
 	testEndpoints("10.10.1.1", "outbound|8080||flipflop.com", adscConn, t)
 }
 
-// Validate that deleting a service clears entries from EndpointShardsByService.
+// Validate that deleting a service clears entries from EndpointIndex.
 func TestDeleteService(t *testing.T) {
 	s := xds.NewFakeDiscoveryServer(t, xds.FakeOptions{})
 	addEdsCluster(s, "removeservice.com", "http", "10.0.0.53", 8080)
@@ -617,12 +617,17 @@ func TestDeleteService(t *testing.T) {
 
 	s.Discovery.MemRegistry.RemoveService("removeservice.com")
 
-	if len(s.Discovery.EndpointShardsByService["removeservice.com"]) != 0 {
-		t.Fatalf("Expected service key %s to be deleted in EndpointShardsByService. But is still there %v",
-			"removeservice.com", s.Discovery.EndpointShardsByService)
+	if _, ok := s.Discovery.EndpointIndex.ShardsForService("removeservice.com", ""); ok {
+		t.Fatalf("Expected service key %s to be deleted in EndpointIndex. But is still there %v",
+			"removeservice.com", s.Discovery.EndpointIndex.Shardz())
 	}
 }
 
+var (
+	c1Key = model.ShardKey{Cluster: "c1"}
+	c2Key = model.ShardKey{Cluster: "c2"}
+)
+
 func TestUpdateServiceAccount(t *testing.T) {
 	cluster1Endppoints := []*model.IstioEndpoint{
 		{Address: "10.172.0.1", ServiceAccount: "sa1"},
@@ -637,19 +642,19 @@ func TestUpdateServiceAccount(t *testing.T) {
 	}{
 		{
 			name:      "added new endpoint",
-			shardKey:  "c1",
+			shardKey:  c1Key,
 			endpoints: append(cluster1Endppoints, &model.IstioEndpoint{Address: "10.172.0.3", ServiceAccount: "sa1"}),
 			expect:    false,
 		},
 		{
 			name:      "added new sa",
-			shardKey:  "c1",
+			shardKey:  c1Key,
 			endpoints: append(cluster1Endppoints, &model.IstioEndpoint{Address: "10.172.0.3", ServiceAccount: "sa2"}),
 			expect:    true,
 		},
 		{
 			name:     "updated endpoints address",
-			shardKey: "c1",
+			shardKey: c1Key,
 			endpoints: []*model.IstioEndpoint{
 				{Address: "10.172.0.5", ServiceAccount: "sa1"},
 				{Address: "10.172.0.2", ServiceAccount: "sa-vm1"},
@@ -658,7 +663,7 @@ func TestUpdateServiceAccount(t *testing.T) {
 		},
 		{
 			name:     "deleted one endpoint with unique sa",
-			shardKey: "c1",
+			shardKey: c1Key,
 			endpoints: []*model.IstioEndpoint{
 				{Address: "10.172.0.1", ServiceAccount: "sa1"},
 			},
@@ -666,7 +671,7 @@ func TestUpdateServiceAccount(t *testing.T) {
 		},
 		{
 			name:     "deleted one endpoint with duplicate sa",
-			shardKey: "c1",
+			shardKey: c1Key,
 			endpoints: []*model.IstioEndpoint{
 				{Address: "10.172.0.2", ServiceAccount: "sa-vm1"},
 			},
@@ -674,7 +679,7 @@ func TestUpdateServiceAccount(t *testing.T) {
 		},
 		{
 			name:      "deleted endpoints",
-			shardKey:  "c1",
+			shardKey:  c1Key,
 			endpoints: nil,
 			expect:    true,
 		},
@@ -683,10 +688,10 @@ func TestUpdateServiceAccount(t *testing.T) {
 	for _, tc := range testCases {
 		t.Run(tc.name, func(t *testing.T) {
 			s := new(xds.DiscoveryServer)
-			originalEndpointsShard := &xds.EndpointShards{
+			originalEndpointsShard := &model.EndpointShards{
 				Shards: map[model.ShardKey][]*model.IstioEndpoint{
-					"c1": cluster1Endppoints,
-					"c2": {{Address: "10.244.0.1", ServiceAccount: "sa1"}, {Address: "10.244.0.2", ServiceAccount: "sa-vm2"}},
+					c1Key: cluster1Endppoints,
+					c2Key: {{Address: "10.244.0.1", ServiceAccount: "sa1"}, {Address: "10.244.0.2", ServiceAccount: "sa-vm2"}},
 				},
 				ServiceAccounts: map[string]struct{}{
 					"sa1":    {},
@@ -708,21 +713,19 @@ func TestZeroEndpointShardSA(t *testing.T) {
 		{Address: "10.172.0.1", ServiceAccount: "sa1"},
 	}
 	s := new(xds.DiscoveryServer)
-	originalEndpointsShard := &xds.EndpointShards{
-		Shards: map[model.ShardKey][]*model.IstioEndpoint{
-			"c1": cluster1Endppoints,
-		},
-		ServiceAccounts: map[string]struct{}{
-			"sa1": {},
-		},
-	}
-	s.EndpointShardsByService = make(map[string]map[string]*xds.EndpointShards)
-	s.EndpointShardsByService["test"] = make(map[string]*xds.EndpointShards)
-	s.EndpointShardsByService["test"]["test"] = originalEndpointsShard
 	s.Cache = model.DisabledCache{}
-	s.EDSCacheUpdate("c1", "test", "test", []*model.IstioEndpoint{})
-	if len(s.EndpointShardsByService["test"]["test"].ServiceAccounts) != 0 {
-		t.Errorf("endpoint shard service accounts got %v want 0", len(s.EndpointShardsByService["test"]["test"].ServiceAccounts))
+	s.EndpointIndex = model.NewEndpointIndex()
+	originalEndpointsShard, _ := s.EndpointIndex.GetOrCreateEndpointShard("test", "test")
+	originalEndpointsShard.Shards = map[model.ShardKey][]*model.IstioEndpoint{
+		c1Key: cluster1Endppoints,
+	}
+	originalEndpointsShard.ServiceAccounts = map[string]struct{}{
+		"sa1": {},
+	}
+	s.EDSCacheUpdate(c1Key, "test", "test", []*model.IstioEndpoint{})
+	modifiedShard, _ := s.EndpointIndex.GetOrCreateEndpointShard("test", "test")
+	if len(modifiedShard.ServiceAccounts) != 0 {
+		t.Errorf("endpoint shard service accounts got %v want 0", len(modifiedShard.ServiceAccounts))
 	}
 }
 
diff --git a/pilot/pkg/xds/endpoint_builder.go b/pilot/pkg/xds/endpoint_builder.go
index f57c5f9848..ce16d9a9f7 100644
--- a/pilot/pkg/xds/endpoint_builder.go
+++ b/pilot/pkg/xds/endpoint_builder.go
@@ -255,7 +255,7 @@ func (e *LocLbEndpointsAndOptions) AssertInvarianceInTest() {
 
 // build LocalityLbEndpoints for a cluster from existing EndpointShards.
 func (b *EndpointBuilder) buildLocalityLbEndpointsFromShards(
-	shards *EndpointShards,
+	shards *model.EndpointShards,
 	svcPort *model.Port,
 ) []*LocLbEndpointsAndOptions {
 	localityEpMap := make(map[string]*LocLbEndpointsAndOptions)
@@ -266,25 +266,15 @@ func (b *EndpointBuilder) buildLocalityLbEndpointsFromShards(
 	// and should, therefore, not be accessed from outside the cluster.
 	isClusterLocal := b.clusterLocal
 
-	shards.mutex.Lock()
-	// Extract shard keys so we can iterate in order. This ensures a stable EDS output. Since
-	// len(shards) ~= number of remote clusters which isn't too large, doing this sort shouldn't be
-	// too problematic. If it becomes an issue we can cache it in the EndpointShards struct.
-	keys := make([]model.ShardKey, 0, len(shards.Shards))
-	for k := range shards.Shards {
-		keys = append(keys, k)
-	}
-	if len(keys) >= 2 {
-		sort.Slice(keys, func(i, j int) bool {
-			return keys[i] < keys[j]
-		})
-	}
+	shards.Lock()
+	// Extract shard keys so we can iterate in order. This ensures a stable EDS output.
+	keys := shards.Keys()
 	// The shards are updated independently, now need to filter and merge for this cluster
 	for _, shardKey := range keys {
 		endpoints := shards.Shards[shardKey]
 		// If the downstream service is configured as cluster-local, only include endpoints that
 		// reside in the same cluster.
-		if isClusterLocal && (shardKey.Cluster() != b.clusterID) {
+		if isClusterLocal && (shardKey.Cluster != b.clusterID) {
 			continue
 		}
 		for _, ep := range endpoints {
@@ -331,7 +321,7 @@ func (b *EndpointBuilder) buildLocalityLbEndpointsFromShards(
 			locLbEps.append(ep, ep.EnvoyEndpoint, ep.TunnelAbility)
 		}
 	}
-	shards.mutex.Unlock()
+	shards.Unlock()
 
 	locEps := make([]*LocLbEndpointsAndOptions, 0, len(localityEpMap))
 	locs := make([]string, 0, len(localityEpMap))
diff --git a/pilot/pkg/xds/ep_filters_test.go b/pilot/pkg/xds/ep_filters_test.go
index 614f0bae0d..40a4c02931 100644
--- a/pilot/pkg/xds/ep_filters_test.go
+++ b/pilot/pkg/xds/ep_filters_test.go
@@ -736,21 +736,21 @@ func environment(t test.Failer, c ...config.Config) *FakeDiscoveryServer {
 //  - 1 endpoints in network4
 //
 // All endpoints are part of service example.ns.svc.cluster.local on port 80 (http).
-func testShards() *EndpointShards {
-	shards := &EndpointShards{Shards: map[model.ShardKey][]*model.IstioEndpoint{
+func testShards() *model.EndpointShards {
+	shards := &model.EndpointShards{Shards: map[model.ShardKey][]*model.IstioEndpoint{
 		// network1 has one endpoint in each cluster
-		"cluster1a": {
+		{Cluster: "cluster1a"}: {
 			{Network: "network1", Address: "10.0.0.1"},
 		},
-		"cluster1b": {
+		{Cluster: "cluster1b"}: {
 			{Network: "network1", Address: "10.0.0.2"},
 		},
 
 		// network2 has an imbalance of endpoints between its clusters
-		"cluster2a": {
+		{Cluster: "cluster2a"}: {
 			{Network: "network2", Address: "20.0.0.1"},
 		},
-		"cluster2b": {
+		{Cluster: "cluster2b"}: {
 			{Network: "network2", Address: "20.0.0.2"},
 			{Network: "network2", Address: "20.0.0.3"},
 		},
@@ -759,12 +759,12 @@ func testShards() *EndpointShards {
 
 		// network4 has a single endpoint, but not gateway so it will always
 		// be considered directly reachable.
-		"cluster4": {
+		{Cluster: "cluster4"}: {
 			{Network: "network4", Address: "40.0.0.1"},
 		},
 	}}
 	// apply common properties
-	for clusterID, shard := range shards.Shards {
+	for sk, shard := range shards.Shards {
 		for i, ep := range shard {
 			ep.ServicePortName = "http"
 			ep.Namespace = "ns"
@@ -772,8 +772,8 @@ func testShards() *EndpointShards {
 			ep.EndpointPort = 8080
 			ep.TLSMode = "istio"
 			ep.Labels = map[string]string{"app": "example"}
-			ep.Locality.ClusterID = cluster.ID(clusterID)
-			shards.Shards[clusterID][i] = ep
+			ep.Locality.ClusterID = sk.Cluster
+			shards.Shards[sk][i] = ep
 		}
 	}
 	return shards
-- 
2.35.3

