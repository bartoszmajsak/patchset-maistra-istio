From 201260076b379fd817225784e61c838aa83355e9 Mon Sep 17 00:00:00 2001
From: Nathan Mittler <nmittler@gmail.com>
Date: Tue, 26 Apr 2022 14:08:06 -0700
Subject: [tf] Fix deadlock for Parallel tests (#38576)

I ran into a deadlock condition when running a number of nested parallel tests. Basically, some tests were stuck logging `Stuck waiting for parent test suites to terminate...`.

I recreated the issue in framework_test.go and the fix in this PR also fixes this test.
---
 .../framework/integration/framework_test.go   | 306 ++++++++++++------
 pkg/test/framework/suitecontext.go            |   4 +-
 pkg/test/framework/test.go                    |  64 +---
 pkg/test/framework/testcontext.go             |  58 +---
 4 files changed, 234 insertions(+), 198 deletions(-)

diff --git a/pkg/test/framework/integration/framework_test.go b/pkg/test/framework/integration/framework_test.go
index ced1224bf8..c62d1c8fcb 100644
--- a/pkg/test/framework/integration/framework_test.go
+++ b/pkg/test/framework/integration/framework_test.go
@@ -16,125 +16,239 @@
 
 import (
 	"fmt"
-	"sync"
 	"testing"
 	"time"
 
-	"go.uber.org/atomic"
-
 	"istio.io/istio/pkg/test/framework"
 )
 
+func TestSynchronous(t *testing.T) {
+	// Root is always run sync.
+	newLevels().
+		// Level 1: bf=5(sync)
+		Add(5, false).
+		// Level 2: bf=2(sync)
+		Add(2, false).
+		// Level 3: bf=2(sync)
+		Add(2, false).
+		Build().
+		Run(t)
+}
+
 func TestParallel(t *testing.T) {
-	var top, l1a, l1b, l2a, l2b, l2c, l2d *component
+	// Root is always run sync.
+	newLevels().
+		// Level 1: bf=20(parallel)
+		Add(20, true).
+		// Level 2: bf=5(parallel)
+		Add(5, true).
+		// Level 3: bf=2(parallel)
+		Add(2, true).
+		Build().
+		Run(t)
+}
+
+func TestMix(t *testing.T) {
+	// Root is always run sync.
+	newLevels().
+		// Level 1: bf=20(parallel)
+		Add(20, true).
+		// Level 2: bf=5(sync)
+		Add(5, false).
+		// Level 3: bf=2(parallel)
+		Add(2, true).
+		Build().
+		Run(t)
+}
+
+func newLevels() levels {
+	return levels{
+		{
+			name: "root",
+		},
+	}
+}
+
+type level struct {
+	name string
+
+	// branchFactor indicates how the parent should branch for this level
+	// (i.e. how many child tests should be created per parent test in the previous level).
+	branchFactor int
+
+	// runParallel if true, all tests in this level will be run in parallel.
+	runParallel bool
+}
+
+func (l level) runParallelString() string {
+	if l.runParallel {
+		return "parallel"
+	}
+	return "sync"
+}
 
-	closeTimes := make(map[string]time.Time)
-	mutex := &sync.Mutex{}
-	closeHandler := func(c *component) {
-		mutex.Lock()
-		defer mutex.Unlock()
-		closeTimes[c.name] = time.Now()
+type levels []level
+
+func (ls levels) Add(branchFactor int, runParallel bool) levels {
+	return append(ls, level{
+		name:         fmt.Sprintf("level%d", len(ls)),
+		branchFactor: branchFactor,
+		runParallel:  runParallel,
+	})
+}
 
-		// Sleep briefly to force time separation between close events.
-		time.Sleep(100 * time.Millisecond)
+func (ls levels) Build() *test {
+	// Create the root test, which will always be run synchronously.
+	root := &test{
+		name: ls[0].name + "[sync]",
 	}
 
-	assertClosedBefore := func(t *testing.T, c1, c2 *component) {
-		t.Helper()
-		if !closeTimes[c1.name].Before(closeTimes[c2.name]) {
-			t.Fatalf("%s closed after %s", c1.name, c2.name)
+	testsInPrevLevel := []*test{root}
+	var testsInCurrentLevel []*test
+	for _, l := range ls[1:] {
+		for _, parent := range testsInPrevLevel {
+			parent.runChildrenParallel = l.runParallel
+			for i := 0; i < l.branchFactor; i++ {
+				child := &test{
+					name: fmt.Sprintf("%s_%d[%s]", l.name, len(testsInCurrentLevel), l.runParallelString()),
+				}
+				parent.children = append(parent.children, child)
+				testsInCurrentLevel = append(testsInCurrentLevel, child)
+			}
 		}
+
+		// Update for next iteration.
+		testsInPrevLevel = testsInCurrentLevel
+		testsInCurrentLevel = nil
 	}
 
+	return root
+}
+
+type test struct {
+	name                string
+	c                   *component
+	runStart            time.Time
+	runEnd              time.Time
+	cleanupStart        time.Time
+	cleanupEnd          time.Time
+	componentCloseStart time.Time
+	componentCloseEnd   time.Time
+
+	runChildrenParallel bool
+	children            []*test
+}
+
+func (tst *test) Run(t *testing.T) {
+	t.Helper()
 	framework.NewTest(t).
-		Run(func(ctx framework.TestContext) {
-			ctx.NewSubTest("top").
-				Run(func(ctx framework.TestContext) {
-					// NOTE: top can't be parallel for this test since it will exit before the children run,
-					// which means we won't be able to verify the results here.
-					top = newComponent(ctx, ctx.Name(), closeHandler)
-
-					ctx.NewSubTest("l1a").
-						RunParallel(func(ctx framework.TestContext) {
-							l1a = newComponent(ctx, ctx.Name(), closeHandler)
-
-							ctx.NewSubTest("l2a").
-								RunParallel(func(ctx framework.TestContext) {
-									l2a = newComponent(ctx, ctx.Name(), closeHandler)
-								})
-
-							ctx.NewSubTest("l2b").
-								RunParallel(func(ctx framework.TestContext) {
-									l2b = newComponent(ctx, ctx.Name(), closeHandler)
-								})
-						})
-
-					ctx.NewSubTest("l1b").
-						RunParallel(func(ctx framework.TestContext) {
-							l1b = newComponent(ctx, ctx.Name(), closeHandler)
-
-							ctx.NewSubTest("l2c").
-								RunParallel(func(ctx framework.TestContext) {
-									l2c = newComponent(ctx, ctx.Name(), closeHandler)
-								})
-
-							ctx.NewSubTest("l2d").
-								RunParallel(func(ctx framework.TestContext) {
-									l2d = newComponent(ctx, ctx.Name(), closeHandler)
-								})
-						})
-				})
+		Features("infrastructure.framework").
+		Run(func(t framework.TestContext) {
+			t.NewSubTest(tst.name).Run(tst.runInternal)
 		})
 
-	assertClosedBefore(t, l2a, l1a)
-	assertClosedBefore(t, l2b, l1a)
-	assertClosedBefore(t, l2c, l1b)
-	assertClosedBefore(t, l2d, l1b)
-	assertClosedBefore(t, l1a, top)
-	assertClosedBefore(t, l1b, top)
-}
-
-// Validate that cleanup is done synchronously for asynchronous tests
-func TestParallelWhenDone(t *testing.T) {
-	errors := make(chan error, 10)
-	framework.NewTest(t).Features("infrastructure.framework").
-		Run(func(ctx framework.TestContext) {
-			runCheck(ctx, errors, "root")
-			ctx.NewSubTest("nested-serial").Run(func(ctx framework.TestContext) {
-				runCheck(ctx, errors, "nested-serial")
-			})
-			ctx.NewSubTest("nested-parallel").RunParallel(func(ctx framework.TestContext) {
-				runCheck(ctx, errors, "nested-parallel")
-			})
-		})
+	if err := tst.doCheck(); err != nil {
+		t.Fatal(err)
+	}
+}
 
-	for {
-		select {
-		case e := <-errors:
-			t.Error(e)
-		default:
-			return
+func (tst *test) runInternal(t framework.TestContext) {
+	tst.runStart = time.Now()
+	t.Cleanup(tst.cleanup)
+	tst.c = newComponent(t, t.Name(), tst.componentClosed)
+
+	if len(tst.children) == 0 {
+		doWork()
+	} else {
+		for _, child := range tst.children {
+			subTest := t.NewSubTest(child.name)
+			if tst.runChildrenParallel {
+				subTest.RunParallel(child.runInternal)
+			} else {
+				subTest.Run(child.runInternal)
+			}
 		}
 	}
+	tst.runEnd = time.Now()
+}
+
+func (tst *test) timeRange() timeRange {
+	return timeRange{
+		start: tst.runStart,
+		end:   tst.cleanupEnd,
+	}
 }
 
-func runCheck(ctx framework.TestContext, errors chan error, name string) {
-	currentTest := atomic.NewString("")
-	for _, c := range []string{"a", "b", "c"} {
-		c := c
-		ctx.NewSubTest(c).Run(func(ctx framework.TestContext) {
-			// Store the test name. We will check this in ConditionalCleanup to ensure we call finish cleanup before the next test runs
-			currentTest.Store(c)
-			ctx.ConditionalCleanup(func() {
-				time.Sleep(time.Millisecond * 100)
-				if ct := currentTest.Load(); ct != c {
-					errors <- fmt.Errorf("expected current test for %s to be %s but was %s", name, c, ct)
+func (tst *test) doCheck() error {
+	// Make sure the component was closed after the test's run method exited.
+	if tst.componentCloseStart.Before(tst.runEnd) {
+		return fmt.Errorf("test %s: componentCloseStart (%v) occurred before runEnd (%v)",
+			tst.name, tst.componentCloseStart, tst.runEnd)
+	}
+
+	// Make sure the component was closed before the cleanup for this test was performed.
+	if tst.cleanupStart.Before(tst.componentCloseEnd) {
+		return fmt.Errorf("test %s: closeStart (%v) occurred before componentCloseEnd (%v)",
+			tst.name, tst.cleanupStart, tst.componentCloseEnd)
+	}
+
+	// Now make sure children cleanup occurred after cleanup for this test.
+	for _, child := range tst.children {
+		if child.cleanupEnd.After(tst.cleanupStart) {
+			return fmt.Errorf("child %s cleanupEnd (%v) occurred after parent %s cleanupStart (%v)",
+				child.name, child.cleanupEnd, tst.name, tst.cleanupStart)
+		}
+	}
+
+	if !tst.runChildrenParallel {
+		// The children were run synchronously. Make sure they don't overlap in time.
+		for i := 0; i < len(tst.children); i++ {
+			ci := tst.children[i]
+			ciRange := ci.timeRange()
+			for j := i + 1; j < len(tst.children); j++ {
+				cj := tst.children[j]
+				cjRange := cj.timeRange()
+				if ciRange.overlaps(cjRange) {
+					return fmt.Errorf("test %s: child %s[%s] overlaps with child %s[%s]",
+						tst.name, ci.name, ciRange, cj.name, cjRange)
 				}
-			})
-			for _, st := range []string{"p1", "p2"} {
-				ctx.NewSubTest(st).
-					RunParallel(func(ctx framework.TestContext) {})
 			}
-		})
+		}
+	}
+
+	// Now check the children.
+	for _, child := range tst.children {
+		if err := child.doCheck(); err != nil {
+			return err
+		}
 	}
+	return nil
+}
+
+func (tst *test) componentClosed(*component) {
+	tst.componentCloseStart = time.Now()
+	doWork()
+	tst.componentCloseEnd = time.Now()
+}
+
+func (tst *test) cleanup() {
+	tst.cleanupStart = time.Now()
+	doWork()
+	tst.cleanupEnd = time.Now()
+}
+
+func doWork() {
+	time.Sleep(time.Millisecond * 10)
+}
+
+type timeRange struct {
+	start, end time.Time
+}
+
+func (r timeRange) overlaps(o timeRange) bool {
+	return r.start.Before(o.end) && r.end.After(o.start)
+}
+
+func (r timeRange) String() string {
+	return fmt.Sprintf("start=%v, end=%v", r.start, r.end)
 }
diff --git a/pkg/test/framework/suitecontext.go b/pkg/test/framework/suitecontext.go
index aa558a89ed..5bb3bda48b 100644
--- a/pkg/test/framework/suitecontext.go
+++ b/pkg/test/framework/suitecontext.go
@@ -33,7 +33,7 @@
 	"istio.io/istio/pkg/util/sets"
 )
 
-// suiteContext contains suite-level items used during runtime.
+// SuiteContext contains suite-level items used during runtime.
 type SuiteContext interface {
 	resource.Context
 }
@@ -224,8 +224,6 @@ type TestOutcome struct {
 }
 
 func (s *suiteContext) registerOutcome(test *testImpl) {
-	s.outcomeMu.Lock()
-	defer s.outcomeMu.Unlock()
 	o := Passed
 	if test.notImplemented {
 		o = NotImplemented
diff --git a/pkg/test/framework/test.go b/pkg/test/framework/test.go
index 7cc9b186af..9dd7cd5622 100644
--- a/pkg/test/framework/test.go
+++ b/pkg/test/framework/test.go
@@ -16,7 +16,6 @@
 
 import (
 	"fmt"
-	"sync"
 	"testing"
 	"time"
 
@@ -117,18 +116,8 @@ type testImpl struct {
 	minIstioVersion      string
 
 	ctx *testContext
-
-	// Indicates that at least one child test is being run in parallel. In Go, when
-	// t.Parallel() is called on a test, execution is halted until the parent test exits.
-	// Only after that point, are the Parallel children are resumed. Because the parent test
-	// must exit before the Parallel children do, we have to defer closing the parent's
-	// testcontext until after the children have completed.
-	hasParallelChildren bool
 }
 
-// globalCleanupLock defines a global wait group to synchronize cleanup of test suites
-var globalParentLock = new(sync.Map)
-
 // NewTest returns a new test wrapper for running a single test.
 func NewTest(t *testing.T) Test {
 	if analyze() {
@@ -246,7 +235,6 @@ func (t *testImpl) doRun(ctx *testContext, fn func(ctx TestContext), parallel bo
 	// we check kube for min clusters, these assume we're talking about real multicluster.
 	// it's possible to have 1 kube cluster then 1 non-kube cluster (vm for example)
 	if t.requiredMinClusters > 0 && len(t.s.Environment().Clusters().Kube()) < t.requiredMinClusters {
-		ctx.Done()
 		t.goTest.Skipf("Skipping %q: number of clusters %d is below required min %d",
 			t.goTest.Name(), len(t.s.Environment().Clusters()), t.requiredMinClusters)
 		return
@@ -254,7 +242,6 @@ func (t *testImpl) doRun(ctx *testContext, fn func(ctx TestContext), parallel bo
 
 	// max clusters doesn't check kube only, the test may be written in a way that doesn't loop over all of Clusters()
 	if t.requiredMaxClusters > 0 && len(t.s.Environment().Clusters()) > t.requiredMaxClusters {
-		ctx.Done()
 		t.goTest.Skipf("Skipping %q: number of clusters %d is above required max %d",
 			t.goTest.Name(), len(t.s.Environment().Clusters()), t.requiredMaxClusters)
 		return
@@ -263,7 +250,6 @@ func (t *testImpl) doRun(ctx *testContext, fn func(ctx TestContext), parallel bo
 	if t.requireLocalIstiod {
 		for _, c := range ctx.Clusters() {
 			if !c.IsPrimary() {
-				ctx.Done()
 				t.goTest.Skipf(fmt.Sprintf("Skipping %q: cluster %s is not using a local control plane",
 					t.goTest.Name(), c.Name()))
 				return
@@ -272,7 +258,6 @@ func (t *testImpl) doRun(ctx *testContext, fn func(ctx TestContext), parallel bo
 	}
 
 	if t.requireSingleNetwork && t.s.Environment().IsMultinetwork() {
-		ctx.Done()
 		t.goTest.Skipf(fmt.Sprintf("Skipping %q: only single network allowed",
 			t.goTest.Name()))
 		return
@@ -280,56 +265,35 @@ func (t *testImpl) doRun(ctx *testContext, fn func(ctx TestContext), parallel bo
 
 	if t.minIstioVersion != "" {
 		if !t.ctx.Settings().Revisions.AtLeast(resource.IstioVersion(t.minIstioVersion)) {
-			ctx.Done()
 			t.goTest.Skipf("Skipping %q: running with min Istio version %q, test requires at least %s",
 				t.goTest.Name(), t.ctx.Settings().Revisions.Minimum(), t.minIstioVersion)
 		}
 	}
 
 	start := time.Now()
-
 	scopes.Framework.Infof("=== BEGIN: Test: '%s[%s]' ===", rt.suiteContext().Settings().TestID, t.goTest.Name())
 
 	// Initial setup if we're running in Parallel.
 	if parallel {
-		// Inform the parent, who will need to call ctx.Done asynchronously.
-		if t.parent != nil {
-			t.parent.hasParallelChildren = true
-		}
-
 		// Run the underlying Go test in parallel. This will not return until the parent
 		// test (if there is one) exits.
 		t.goTest.Parallel()
 	}
 
-	defer func() {
-		doneFn := func() {
-			message := "passed"
-			if t.goTest.Failed() {
-				message = "failed"
-			}
-			end := time.Now()
-			scopes.Framework.Infof("=== DONE (%s):  Test: '%s[%s] (%v)' ===",
-				message,
-				rt.suiteContext().Settings().TestID,
-				t.goTest.Name(),
-				end.Sub(start))
-			rt.suiteContext().registerOutcome(t)
-			ctx.Done()
-			if t.hasParallelChildren {
-				globalParentLock.Delete(t)
-			}
+	// Register the cleanup function for when the Go test completes.
+	t.goTest.Cleanup(func() {
+		message := "passed"
+		if t.goTest.Failed() {
+			message = "failed"
 		}
-		if t.hasParallelChildren {
-			// If a child is running in parallel, it won't continue until this test returns.
-			// Since ctx.Done() will block until the child test is complete, we run ctx.Done()
-			// asynchronously.
-			globalParentLock.Store(t, struct{}{})
-			go doneFn()
-		} else {
-			doneFn()
-		}
-	}()
-
+		scopes.Framework.Infof("=== DONE (%s):  Test: '%s[%s] (%v)' ===",
+			message,
+			rt.suiteContext().Settings().TestID,
+			t.goTest.Name(),
+			time.Since(start))
+		rt.suiteContext().registerOutcome(t)
+	})
+
+	// Run the user's test function.
 	fn(ctx)
 }
diff --git a/pkg/test/framework/testcontext.go b/pkg/test/framework/testcontext.go
index 3996e614b5..39a3ee5205 100644
--- a/pkg/test/framework/testcontext.go
+++ b/pkg/test/framework/testcontext.go
@@ -20,7 +20,6 @@
 	"os"
 	"path"
 	"testing"
-	"time"
 
 	"istio.io/istio/pkg/test"
 	"istio.io/istio/pkg/test/framework/components/cluster"
@@ -56,11 +55,8 @@ type TestContext interface {
 	// SkipDumping will skip dumping debug logs/configs/etc for this scope only (child scopes are not skipped).
 	SkipDumping()
 
-	// Done should be called when this context is no longer needed. It triggers the asynchronous cleanup of any
-	// allocated resources.
-	Done()
-
 	// Methods for interacting with the underlying *testing.T.
+
 	Error(args ...interface{})
 	Errorf(format string, args ...interface{})
 	Failed() bool
@@ -99,48 +95,7 @@ type testContext struct {
 	workDir string
 }
 
-// Before executing a new context, we should wait for existing contexts to terminate if they are NOT parents of this context.
-// This is to workaround termination of functions run with RunParallel. When this is used, child tests will not run until the parent
-// has terminated. This means that the parent cannot synchronously cleanup, or it would block its children. However, if we do async cleanup,
-// then new tests can unexpectedly start during the cleanup of another. This may lead to odd results, like a test cleanup undoing the setup of a future test.
-// To workaround this, we maintain a set of all contexts currently terminating. Before starting the context, we will search this set;
-// if any non-parent contexts are found, we will wait.
-func waitForParents(test *testImpl) {
-	iterations := 0
-	for {
-		iterations++
-		done := true
-		globalParentLock.Range(func(key, value interface{}) bool {
-			k := key.(*testImpl)
-			current := test
-			for current != nil {
-				if current == k {
-					return true
-				}
-				current = current.parent
-
-			}
-			// We found an item in the list, and we are *not* a child of it. This means another test hierarchy has exclusive access right now
-			// Wait until they are finished before proceeding
-			done = false
-			return true
-		})
-		if done {
-			return
-		}
-		time.Sleep(time.Millisecond * 50)
-		// Add some logging in case something locks up so we can debug
-		if iterations%10 == 0 {
-			globalParentLock.Range(func(key, value interface{}) bool {
-				scopes.Framework.Warnf("Stuck waiting for parent test suites to terminate... %v is blocking", key.(*testImpl).goTest.Name())
-				return true
-			})
-		}
-	}
-}
-
 func newTestContext(test *testImpl, goTest *testing.T, s *suiteContext, parentScope *scope, labels label.Set) *testContext {
-	waitForParents(test)
 	id := s.allocateContextID(goTest.Name())
 
 	allLabels := s.suiteLabels.Merge(labels)
@@ -174,7 +129,7 @@ func newTestContext(test *testImpl, goTest *testing.T, s *suiteContext, parentSc
 	}
 
 	scopeID := fmt.Sprintf("[%s]", id)
-	return &testContext{
+	ctx := &testContext{
 		id:         id,
 		test:       test,
 		T:          goTest,
@@ -183,6 +138,11 @@ func newTestContext(test *testImpl, goTest *testing.T, s *suiteContext, parentSc
 		workDir:    workDir,
 		FileWriter: yml.NewFileWriter(workDir),
 	}
+
+	// Register the cleanup handler for the context.
+	goTest.Cleanup(ctx.close)
+
+	return ctx
 }
 
 func (c *testContext) Settings() *resource.Settings {
@@ -312,7 +272,7 @@ func (c *testContext) Cleanup(fn func()) {
 	}})
 }
 
-func (c *testContext) Done() {
+func (c *testContext) close() {
 	if c.Failed() && c.Settings().CIMode {
 		scopes.Framework.Debugf("Begin dumping testContext: %q", c.id)
 		// make sure we dump suite-level resources, but don't dump sibling tests or their children
@@ -414,7 +374,7 @@ func (c *closer) Close() error {
 	return c.fn()
 }
 
-func (c *testContext) RecordTraceEvent(key string, value interface{}) {
+func (c *testContext) RecordTraceEvent(string, interface{}) {
 	// Currently, only supported at suite level.
 	panic("TODO: implement tracing in test context")
 }
-- 
2.35.3

