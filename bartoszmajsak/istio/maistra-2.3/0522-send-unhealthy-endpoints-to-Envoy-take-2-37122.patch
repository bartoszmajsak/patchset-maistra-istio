From e56cb98a43f019ce6e6c86541a98493ca1c92f43 Mon Sep 17 00:00:00 2001
From: Rama Chavali <rama.rao@salesforce.com>
Date: Wed, 9 Feb 2022 12:22:25 +0530
Subject: send unhealthy endpoints to Envoy (take 2) (#37122)

* send unready endpoints as unhealthy

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* comment

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* set ready

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* set more ready and debug log

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* move to enum

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* lint

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* do not push if only not ready endpoints are there for service

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* fix ut

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* lint

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* add unit test

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* refactor, add feature flag and release notes

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* fix lint and release notes

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* lint

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* fix comments

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* address comments

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* minor change

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* move inside lock

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* update instancesByport to use unhealthy addresses

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* fix ut

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>

* fix lint

Signed-off-by: Rama Chavali <rama.rao@salesforce.com>
---
 pilot/pkg/features/pilot.go                   |   6 +
 pilot/pkg/model/service.go                    |  13 ++
 .../kube/controller/endpoints.go              | 102 +++++++------
 .../kube/controller/endpointslice.go          |  15 +-
 pilot/pkg/xds/eds.go                          |  92 +++++++++---
 pilot/pkg/xds/eds_test.go                     | 134 ++++++++++++++++++
 pilot/pkg/xds/endpoint_builder.go             |   5 +
 releasenotes/notes/36274.yaml                 |   7 +
 8 files changed, 309 insertions(+), 65 deletions(-)
 create mode 100644 releasenotes/notes/36274.yaml

diff --git a/pilot/pkg/features/pilot.go b/pilot/pkg/features/pilot.go
index a258d9488f..5f57fc662d 100644
--- a/pilot/pkg/features/pilot.go
+++ b/pilot/pkg/features/pilot.go
@@ -105,6 +105,12 @@
 			" EDS pushes may be delayed, but there will be fewer pushes. By default this is enabled",
 	).Get()
 
+	SendUnhealthyEndpoints = env.RegisterBoolVar(
+		"PILOT_SEND_UNHEALTHY_ENDPOINTS",
+		true,
+		"If enabled, Pilot will include unhealthy endpoints in EDS pushes and even if they are sent Envoy does not use them for load balancing.",
+	).Get()
+
 	// HTTP10 will add "accept_http_10" to http outbound listeners. Can also be set only for specific sidecars via meta.
 	HTTP10 = env.RegisterBoolVar(
 		"PILOT_HTTP10",
diff --git a/pilot/pkg/model/service.go b/pilot/pkg/model/service.go
index efa393f46f..d470f0e30a 100644
--- a/pilot/pkg/model/service.go
+++ b/pilot/pkg/model/service.go
@@ -387,6 +387,16 @@ type Locality struct {
 	ClusterID cluster.ID
 }
 
+// Endpoint health status.
+type HealthStatus int32
+
+const (
+	// Healthy.
+	Healthy HealthStatus = 0
+	// Unhealthy.
+	UnHealthy HealthStatus = 1
+)
+
 // IstioEndpoint defines a network address (IP:port) associated with an instance of the
 // service. A service has one or more instances each running in a
 // container/VM/pod. If a service has multiple ports, then the same
@@ -458,6 +468,9 @@ type IstioEndpoint struct {
 
 	// Determines the discoverability of this endpoint throughout the mesh.
 	DiscoverabilityPolicy EndpointDiscoverabilityPolicy `json:"-"`
+
+	// Indicatesthe endpoint health status.
+	HealthStatus HealthStatus
 }
 
 // GetLoadBalancingWeight returns the weight for this endpoint, normalized to always be > 0.
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpoints.go b/pilot/pkg/serviceregistry/kube/controller/endpoints.go
index 2536d22e4e..be535123f3 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpoints.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpoints.go
@@ -22,6 +22,7 @@
 	listerv1 "k8s.io/client-go/listers/core/v1"
 	"k8s.io/client-go/tools/cache"
 
+	"istio.io/istio/pilot/pkg/features"
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/serviceregistry/kube"
 	"istio.io/istio/pilot/pkg/serviceregistry/kube/controller/filter"
@@ -125,37 +126,11 @@ func (e *endpointsController) InstancesByPort(c *Controller, svc *model.Service,
 	ep := item.(*v1.Endpoints)
 	var out []*model.ServiceInstance
 	for _, ss := range ep.Subsets {
-		for _, ea := range ss.Addresses {
-			var podLabels labels.Instance
-			pod, expectedPod := getPod(c, ea.IP, &metav1.ObjectMeta{Name: ep.Name, Namespace: ep.Namespace}, ea.TargetRef, svc.Hostname)
-			if pod == nil && expectedPod {
-				continue
-			}
-			if pod != nil {
-				podLabels = pod.Labels
-			}
-			// check that one of the input labels is a subset of the labels
-			if !labelsList.HasSubsetOf(podLabels) {
-				continue
-			}
-
-			builder := NewEndpointBuilder(c, pod)
-
-			// identify the port by name. K8S EndpointPort uses the service port name
-			for _, port := range ss.Ports {
-				if port.Name == "" || // 'name optional if single port is defined'
-					svcPort.Name == port.Name {
-					istioEndpoint := builder.buildIstioEndpoint(ea.IP, port.Port, svcPort.Name, discoverabilityPolicy)
-					out = append(out, &model.ServiceInstance{
-						Endpoint:    istioEndpoint,
-						ServicePort: svcPort,
-						Service:     svc,
-					})
-				}
-			}
+		out = append(out, e.buildServiceInstances(ep, ss, ss.Addresses, svc, discoverabilityPolicy, labelsList, svcPort, model.Healthy)...)
+		if features.SendUnhealthyEndpoints {
+			out = append(out, e.buildServiceInstances(ep, ss, ss.NotReadyAddresses, svc, discoverabilityPolicy, labelsList, svcPort, model.UnHealthy)...)
 		}
 	}
-
 	return out
 }
 
@@ -199,21 +174,68 @@ func (e *endpointsController) buildIstioEndpoints(endpoint interface{}, host hos
 	discoverabilityPolicy := e.c.exports.EndpointDiscoverabilityPolicy(e.c.GetService(host))
 
 	for _, ss := range ep.Subsets {
-		for _, ea := range ss.Addresses {
-			pod, expectedPod := getPod(e.c, ea.IP, &metav1.ObjectMeta{Name: ep.Name, Namespace: ep.Namespace}, ea.TargetRef, host)
-			if pod == nil && expectedPod {
-				continue
-			}
-			builder := NewEndpointBuilder(e.c, pod)
+		endpoints = append(endpoints, e.buildIstioEndpointFromAddress(ep, ss, ss.Addresses, host, discoverabilityPolicy, model.Healthy)...)
+		if features.SendUnhealthyEndpoints {
+			endpoints = append(endpoints, e.buildIstioEndpointFromAddress(ep, ss, ss.NotReadyAddresses, host, discoverabilityPolicy, model.UnHealthy)...)
+		}
+	}
+	return endpoints
+}
 
-			// EDS and ServiceEntry use name for service port - ADS will need to map to numbers.
-			for _, port := range ss.Ports {
-				istioEndpoint := builder.buildIstioEndpoint(ea.IP, port.Port, port.Name, discoverabilityPolicy)
-				endpoints = append(endpoints, istioEndpoint)
+func (e *endpointsController) buildServiceInstances(ep *v1.Endpoints, ss v1.EndpointSubset, endpoints []v1.EndpointAddress,
+	svc *model.Service, discoverabilityPolicy model.EndpointDiscoverabilityPolicy, labelsList labels.Collection,
+	svcPort *model.Port, health model.HealthStatus) []*model.ServiceInstance {
+	var out []*model.ServiceInstance
+	for _, ea := range endpoints {
+		var podLabels labels.Instance
+		pod, expectedPod := getPod(e.c, ea.IP, &metav1.ObjectMeta{Name: ep.Name, Namespace: ep.Namespace}, ea.TargetRef, svc.Hostname)
+		if pod == nil && expectedPod {
+			continue
+		}
+		if pod != nil {
+			podLabels = pod.Labels
+		}
+		// check that one of the input labels is a subset of the labels
+		if !labelsList.HasSubsetOf(podLabels) {
+			continue
+		}
+
+		builder := NewEndpointBuilder(e.c, pod)
+
+		// identify the port by name. K8S EndpointPort uses the service port name
+		for _, port := range ss.Ports {
+			if port.Name == "" || // 'name optional if single port is defined'
+				svcPort.Name == port.Name {
+				istioEndpoint := builder.buildIstioEndpoint(ea.IP, port.Port, svcPort.Name, discoverabilityPolicy)
+				istioEndpoint.HealthStatus = health
+				out = append(out, &model.ServiceInstance{
+					Endpoint:    istioEndpoint,
+					ServicePort: svcPort,
+					Service:     svc,
+				})
 			}
 		}
 	}
-	return endpoints
+	return out
+}
+
+func (e *endpointsController) buildIstioEndpointFromAddress(ep *v1.Endpoints, ss v1.EndpointSubset, endpoints []v1.EndpointAddress,
+	host host.Name, discoverabilityPolicy model.EndpointDiscoverabilityPolicy, health model.HealthStatus) []*model.IstioEndpoint {
+	var istioEndpoints []*model.IstioEndpoint
+	for _, ea := range endpoints {
+		pod, expectedPod := getPod(e.c, ea.IP, &metav1.ObjectMeta{Name: ep.Name, Namespace: ep.Namespace}, ea.TargetRef, host)
+		if pod == nil && expectedPod {
+			continue
+		}
+		builder := NewEndpointBuilder(e.c, pod)
+		// EDS and ServiceEntry use name for service port - ADS will need to map to numbers.
+		for _, port := range ss.Ports {
+			istioEndpoint := builder.buildIstioEndpoint(ea.IP, port.Port, port.Name, discoverabilityPolicy)
+			istioEndpoint.HealthStatus = health
+			istioEndpoints = append(istioEndpoints, istioEndpoint)
+		}
+	}
+	return istioEndpoints
 }
 
 func (e *endpointsController) buildIstioEndpointsWithService(name, namespace string, host host.Name, _ bool) []*model.IstioEndpoint {
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpointslice.go b/pilot/pkg/serviceregistry/kube/controller/endpointslice.go
index b7ce95e794..ad6ce73456 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpointslice.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpointslice.go
@@ -31,6 +31,7 @@
 	mcs "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
+	"istio.io/istio/pilot/pkg/features"
 	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/serviceregistry/kube"
 	"istio.io/istio/pilot/pkg/serviceregistry/kube/controller/filter"
@@ -227,10 +228,13 @@ func (esc *endpointSliceController) updateEndpointCacheForSlice(hostName host.Na
 	discoverabilityPolicy := esc.c.exports.EndpointDiscoverabilityPolicy(esc.c.GetService(hostName))
 
 	for _, e := range slice.Endpoints() {
-		if e.Conditions.Ready != nil && !*e.Conditions.Ready {
-			// Ignore not ready endpoints
-			continue
+		if !features.SendUnhealthyEndpoints {
+			if e.Conditions.Ready != nil && !*e.Conditions.Ready {
+				// Ignore not ready endpoints
+				continue
+			}
 		}
+		ready := e.Conditions.Ready == nil || *e.Conditions.Ready
 		for _, a := range e.Addresses {
 			pod, expectedPod := getPod(esc.c, a, &metav1.ObjectMeta{Name: slice.Name, Namespace: slice.Namespace}, e.TargetRef, hostName)
 			if pod == nil && expectedPod {
@@ -249,6 +253,11 @@ func (esc *endpointSliceController) updateEndpointCacheForSlice(hostName host.Na
 				}
 
 				istioEndpoint := builder.buildIstioEndpoint(a, portNum, portName, discoverabilityPolicy)
+				if ready {
+					istioEndpoint.HealthStatus = model.Healthy
+				} else {
+					istioEndpoint.HealthStatus = model.UnHealthy
+				}
 				endpoints = append(endpoints, istioEndpoint)
 			}
 		}
diff --git a/pilot/pkg/xds/eds.go b/pilot/pkg/xds/eds.go
index 81f6625890..6f92992cbc 100644
--- a/pilot/pkg/xds/eds.go
+++ b/pilot/pkg/xds/eds.go
@@ -35,6 +35,18 @@
 	"istio.io/istio/pkg/config/schema/gvk"
 )
 
+// PushType is an enumeration that decides what type push we should do when we get EDS update.
+type PushType int
+
+const (
+	// NoPush does not push any thing.
+	NoPush PushType = iota
+	// IncrementalPush just pushes endpoints.
+	IncrementalPush
+	// FullPush triggers full push - typically used for new services.
+	FullPush
+)
+
 // UpdateServiceShards will list the endpoints and create the shards.
 // This is used to reconcile and to support non-k8s registries (until they migrate).
 // Note that aggregated list is expensive (for large numbers) - we want to replace
@@ -94,17 +106,19 @@ func (s *DiscoveryServer) EDSUpdate(shard model.ShardKey, serviceName string, na
 	istioEndpoints []*model.IstioEndpoint) {
 	inboundEDSUpdates.Increment()
 	// Update the endpoint shards
-	fp := s.edsCacheUpdate(shard, serviceName, namespace, istioEndpoints)
-	// Trigger a push
-	s.ConfigUpdate(&model.PushRequest{
-		Full: fp,
-		ConfigsUpdated: map[model.ConfigKey]struct{}{{
-			Kind:      gvk.ServiceEntry,
-			Name:      serviceName,
-			Namespace: namespace,
-		}: {}},
-		Reason: []model.TriggerReason{model.EndpointUpdate},
-	})
+	pushType := s.edsCacheUpdate(shard, serviceName, namespace, istioEndpoints)
+	if pushType == IncrementalPush || pushType == FullPush {
+		// Trigger a push
+		s.ConfigUpdate(&model.PushRequest{
+			Full: pushType == FullPush,
+			ConfigsUpdated: map[model.ConfigKey]struct{}{{
+				Kind:      gvk.ServiceEntry,
+				Name:      serviceName,
+				Namespace: namespace,
+			}: {}},
+			Reason: []model.TriggerReason{model.EndpointUpdate},
+		})
+	}
 }
 
 // EDSCacheUpdate computes destination address membership across all clusters and networks.
@@ -122,9 +136,10 @@ func (s *DiscoveryServer) EDSCacheUpdate(shard model.ShardKey, serviceName strin
 }
 
 // edsCacheUpdate updates EndpointShards data by clusterID, hostname, IstioEndpoints.
-// It also tracks the changes to ServiceAccounts. It returns whether a full push
-// is needed or incremental push is sufficient.
-func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string, namespace string, istioEndpoints []*model.IstioEndpoint) bool {
+// It also tracks the changes to ServiceAccounts. It returns whether endpoints need to be pushed and
+// it also returns if they need to be pushed whether a full push is needed or incremental push is sufficient.
+func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string, namespace string,
+	istioEndpoints []*model.IstioEndpoint) PushType {
 	if len(istioEndpoints) == 0 {
 		// Should delete the service EndpointShards when endpoints become zero to prevent memory leak,
 		// but we should not delete the keys from EndpointShardsByService map - that will trigger
@@ -132,22 +147,29 @@ func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string,
 		// flip flopping between 1 and 0.
 		s.deleteEndpointShards(shard, hostname, namespace)
 		log.Infof("Incremental push, service %s at shard %v has no endpoints", hostname, shard)
-		return false
+		return IncrementalPush
 	}
 
-	fullPush := false
+	pushType := IncrementalPush
 	// Find endpoint shard for this service, if it is available - otherwise create a new one.
 	ep, created := s.getOrCreateEndpointShard(hostname, namespace)
 	// If we create a new endpoint shard, that means we have not seen the service earlier. We should do a full push.
 	if created {
 		log.Infof("Full push, new service %s/%s", namespace, hostname)
-		fullPush = true
+		pushType = FullPush
 	}
 
 	ep.mutex.Lock()
+	oldIstioEndpoints := ep.Shards[shard]
 	ep.Shards[shard] = istioEndpoints
 	// Check if ServiceAccounts have changed. We should do a full push if they have changed.
 	saUpdated := s.UpdateServiceAccount(ep, hostname)
+
+	// For existing endpoints, we need to do full push if service accounts change.
+	if saUpdated {
+		log.Infof("Full push, service accounts changed, %v", hostname)
+		pushType = FullPush
+	}
 	// Clear the cache here. While it would likely be cleared later when we trigger a push, a race
 	// condition is introduced where an XDS response may be generated before the update, but not
 	// completed until after a response after the update. Essentially, we transition from v0 -> v1 ->
@@ -161,13 +183,39 @@ func (s *DiscoveryServer) edsCacheUpdate(shard model.ShardKey, hostname string,
 		Namespace: namespace,
 	}: {}})
 	ep.mutex.Unlock()
+	if features.SendUnhealthyEndpoints && pushType == IncrementalPush {
+		// Check if new Endpoints are ready to be pushed. This check
+		// will ensure that if a new pod comes with a non ready endpoint,
+		// we do not unnecessarily push that config to Envoy.
+		// Please note that address is not a unique key. So this may not accurately
+		// identify based on health status and push too many times - which is ok since its an optimization.
+		emap := make(map[string]*model.IstioEndpoint, len(oldIstioEndpoints))
+		for _, oie := range oldIstioEndpoints {
+			emap[oie.Address] = oie
+		}
+		needPush := false
+		for _, nie := range istioEndpoints {
+			if oie, exists := emap[nie.Address]; exists {
+				// If endpoint exists already, we should push if it's health status changes.
+				needPush = oie.HealthStatus != nie.HealthStatus
+			} else {
+				// If the endpoint does not exist in shards that means it is a
+				// new endpoint. Only send if it is healthy to avoid pushing endpoints
+				// that are not ready to start with.
+				needPush = nie.HealthStatus == model.Healthy
+			}
+			if needPush {
+				break
+			}
+		}
 
-	// For existing endpoints, we need to do full push if service accounts change.
-	if saUpdated {
-		log.Infof("Full push, service accounts changed, %v", hostname)
-		fullPush = true
+		if !needPush {
+			log.Infof("No push, either old endpoint health status did not change or new endpoint came with unhealthy status, %v", hostname)
+			pushType = NoPush
+		}
 	}
-	return fullPush
+
+	return pushType
 }
 
 func (s *DiscoveryServer) RemoveShard(shardKey model.ShardKey) {
diff --git a/pilot/pkg/xds/eds_test.go b/pilot/pkg/xds/eds_test.go
index 71a68e0c6e..e0e885a605 100644
--- a/pilot/pkg/xds/eds_test.go
+++ b/pilot/pkg/xds/eds_test.go
@@ -31,6 +31,7 @@
 	"testing"
 	"time"
 
+	envoy_config_core_v3 "github.com/envoyproxy/go-control-plane/envoy/config/core/v3"
 	endpoint "github.com/envoyproxy/go-control-plane/envoy/config/endpoint/v3"
 	uatomic "go.uber.org/atomic"
 
@@ -338,6 +339,113 @@ func TestEDSOverlapping(t *testing.T) {
 	testOverlappingPorts(s, adscon, t)
 }
 
+func TestEDSUnhealthyEndpoints(t *testing.T) {
+	s := xds.NewFakeDiscoveryServer(t, xds.FakeOptions{})
+	addUnhealthyCluster(s)
+	adscon := s.Connect(nil, nil, watchEds)
+	_, err := adscon.Wait(5 * time.Second)
+	if err != nil {
+		t.Fatalf("Error in push %v", err)
+	}
+
+	// Validate that there are  no endpoints.
+	lbe := adscon.GetEndpoints()["outbound|53||unhealthy.svc.cluster.local"]
+	if lbe != nil && len(lbe.Endpoints) == 1 && len(lbe.Endpoints[0].LbEndpoints) > 1 {
+		t.Fatalf("one endpoint is expected for  %s,  but got %v", "unhealthy.svc.cluster.local", adscon.EndpointsJSON())
+	}
+
+	adscon.WaitClear()
+
+	// Set the unhealthy endpoint and validate Eds update is not triggered.
+	s.Discovery.MemRegistry.SetEndpoints("unhealthy.svc.cluster.local", "",
+		[]*model.IstioEndpoint{
+			{
+				Address:         "10.0.0.53",
+				EndpointPort:    53,
+				ServicePortName: "tcp-dns",
+				HealthStatus:    model.Healthy,
+			},
+			{
+				Address:         "10.0.0.54",
+				EndpointPort:    53,
+				ServicePortName: "tcp-dns",
+				HealthStatus:    model.UnHealthy,
+			},
+		})
+
+	// Validate that endpoint is not pushed.
+	lbe = adscon.GetEndpoints()["outbound|53||unhealthy.svc.cluster.local"]
+	if lbe != nil && len(lbe.Endpoints) == 1 && len(lbe.Endpoints[0].LbEndpoints) > 1 {
+		t.Fatalf("one endpoint is expected for  %s,  but got %v", "unhealthy.svc.cluster.local", adscon.EndpointsJSON())
+	}
+
+	// Change the status of endpoint to Healthy and validate Eds is pushed.
+	s.Discovery.MemRegistry.SetEndpoints("unhealthy.svc.cluster.local", "",
+		[]*model.IstioEndpoint{
+			{
+				Address:         "10.0.0.53",
+				EndpointPort:    53,
+				ServicePortName: "tcp-dns",
+				HealthStatus:    model.Healthy,
+			},
+			{
+				Address:         "10.0.0.54",
+				EndpointPort:    53,
+				ServicePortName: "tcp-dns",
+				HealthStatus:    model.Healthy,
+			},
+		})
+
+	upd, _ := adscon.Wait(5*time.Second, v3.EndpointType)
+
+	if len(upd) > 0 && !contains(upd, v3.EndpointType) {
+		t.Fatalf("Expecting EDS push as endpoint health is changed. But received %v", upd)
+	}
+
+	// Validate that endpoints are pushed.
+	lbe = adscon.GetEndpoints()["outbound|53||unhealthy.svc.cluster.local"]
+	if lbe != nil && len(lbe.Endpoints[0].LbEndpoints) != 2 {
+		t.Fatalf("two endpoints expected for  %s,  but got %v", "unhealthy.svc.cluster.local", adscon.EndpointsJSON())
+	}
+
+	// Now change the status of endpoint to UnHealthy and validate Eds is pushed.
+	s.Discovery.MemRegistry.SetEndpoints("unhealthy.svc.cluster.local", "",
+		[]*model.IstioEndpoint{
+			{
+				Address:         "10.0.0.53",
+				EndpointPort:    53,
+				ServicePortName: "tcp-dns",
+				HealthStatus:    model.UnHealthy,
+			},
+			{
+				Address:         "10.0.0.54",
+				EndpointPort:    53,
+				ServicePortName: "tcp-dns",
+				HealthStatus:    model.Healthy,
+			},
+		})
+
+	upd, _ = adscon.Wait(5*time.Second, v3.EndpointType)
+
+	if len(upd) > 0 && !contains(upd, v3.EndpointType) {
+		t.Fatalf("Expecting EDS push as endpoint health is changed. But received %v", upd)
+	}
+
+	// Validate that endpoints are pushed.
+	lbe = adscon.GetEndpoints()["outbound|53||unhealthy.svc.cluster.local"]
+	if lbe != nil && len(lbe.Endpoints[0].LbEndpoints) != 2 {
+		t.Fatalf("two endpoints expected for  %s,  but got %v", "unhealthy.svc.cluster.local", adscon.EndpointsJSON())
+	}
+
+	// Validate that health status is updated correctly.
+	lbendpoints := lbe.Endpoints[0].LbEndpoints
+	for _, lbe := range lbendpoints {
+		if lbe.GetEndpoint().Address.GetSocketAddress().Address == "10.0.0.53" && lbe.HealthStatus != envoy_config_core_v3.HealthStatus_UNHEALTHY {
+			t.Fatal("expected endpoint to be unhealthy, but got healthy")
+		}
+	}
+}
+
 // Validates the behavior when Service resolution type is updated after initial EDS push.
 // See https://github.com/istio/istio/issues/18355 for more details.
 func TestEDSServiceResolutionUpdate(t *testing.T) {
@@ -1120,6 +1228,32 @@ func addOverlappingEndpoints(s *xds.FakeDiscoveryServer) {
 	fullPush(s)
 }
 
+func addUnhealthyCluster(s *xds.FakeDiscoveryServer) {
+	s.Discovery.MemRegistry.AddService("unhealthy.svc.cluster.local", &model.Service{
+		Hostname: "unhealthy.svc.cluster.local",
+		Ports: model.PortList{
+			{
+				Name:     "tcp-dns",
+				Port:     53,
+				Protocol: protocol.TCP,
+			},
+		},
+	})
+	s.Discovery.MemRegistry.AddInstance("unhealthy.svc.cluster.local", &model.ServiceInstance{
+		Endpoint: &model.IstioEndpoint{
+			Address:         "10.0.0.53",
+			EndpointPort:    53,
+			ServicePortName: "tcp-dns",
+		},
+		ServicePort: &model.Port{
+			Name:     "tcp-dns",
+			Port:     53,
+			Protocol: protocol.TCP,
+		},
+	})
+	fullPush(s)
+}
+
 // Verify the endpoint debug interface is installed and returns some string.
 // TODO: parse response, check if data captured matches what we expect.
 // TODO: use this in integration tests.
diff --git a/pilot/pkg/xds/endpoint_builder.go b/pilot/pkg/xds/endpoint_builder.go
index 6b061ca646..f5f7600ef5 100644
--- a/pilot/pkg/xds/endpoint_builder.go
+++ b/pilot/pkg/xds/endpoint_builder.go
@@ -398,8 +398,13 @@ func (b *EndpointBuilder) createClusterLoadAssignment(llbOpts []*LocLbEndpointsA
 // buildEnvoyLbEndpoint packs the endpoint based on istio info.
 func buildEnvoyLbEndpoint(e *model.IstioEndpoint) *endpoint.LbEndpoint {
 	addr := util.BuildAddress(e.Address, e.EndpointPort)
+	healthStatus := core.HealthStatus_HEALTHY
+	if e.HealthStatus == model.UnHealthy {
+		healthStatus = core.HealthStatus_UNHEALTHY
+	}
 
 	ep := &endpoint.LbEndpoint{
+		HealthStatus: healthStatus,
 		LoadBalancingWeight: &wrappers.UInt32Value{
 			Value: e.GetLoadBalancingWeight(),
 		},
diff --git a/releasenotes/notes/36274.yaml b/releasenotes/notes/36274.yaml
new file mode 100644
index 0000000000..a9c28d2f74
--- /dev/null
+++ b/releasenotes/notes/36274.yaml
@@ -0,0 +1,7 @@
+apiVersion: release-notes/v2
+kind: feature
+area: traffic-management
+releaseNotes:
+- |
+  **Added** support for sending unready endpoints also to Envoy. This will be useful when slow start mode in Envoy is enabled.
+  This can be disabled by setting PILOT_SEND_UNHEALTHY_ENDPOINTS to false.
-- 
2.35.3

