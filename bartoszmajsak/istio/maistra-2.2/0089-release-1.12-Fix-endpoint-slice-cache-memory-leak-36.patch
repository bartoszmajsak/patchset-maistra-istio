From 18c0ed2d70619d3e2153dfb5d9773f27003bb195 Mon Sep 17 00:00:00 2001
From: Istio Automation <istio-testing-bot@google.com>
Date: Wed, 22 Dec 2021 02:00:53 -0800
Subject: [release-1.12] Fix endpoint slice cache memory leak (#36558)

* fix endpoint slice cache memory leak

Signed-off-by: dddddai <dddwq@foxmail.com>

* add unit test

Signed-off-by: dddddai <dddwq@foxmail.com>

Co-authored-by: dddddai <dddwq@foxmail.com>
---
 .../kube/controller/endpointslice.go          | 54 +++++-------
 .../kube/controller/endpointslice_test.go     | 82 +++++++++++++++++++
 2 files changed, 104 insertions(+), 32 deletions(-)

diff --git a/pilot/pkg/serviceregistry/kube/controller/endpointslice.go b/pilot/pkg/serviceregistry/kube/controller/endpointslice.go
index 8e199cf496..f563a9eab0 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpointslice.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpointslice.go
@@ -379,15 +379,13 @@ type endpointKey struct {
 }
 
 type endpointSliceCache struct {
-	mu                            sync.RWMutex
-	endpointKeysByServiceAndSlice map[host.Name]map[string][]endpointKey
-	endpointByKey                 map[endpointKey]*model.IstioEndpoint
+	mu                         sync.RWMutex
+	endpointsByServiceAndSlice map[host.Name]map[string][]*model.IstioEndpoint
 }
 
 func newEndpointSliceCache() *endpointSliceCache {
 	out := &endpointSliceCache{
-		endpointKeysByServiceAndSlice: make(map[host.Name]map[string][]endpointKey),
-		endpointByKey:                 make(map[endpointKey]*model.IstioEndpoint),
+		endpointsByServiceAndSlice: make(map[host.Name]map[string][]*model.IstioEndpoint),
 	}
 	return out
 }
@@ -396,36 +394,27 @@ func (e *endpointSliceCache) Update(hostname host.Name, slice string, endpoints
 	e.mu.Lock()
 	defer e.mu.Unlock()
 	if len(endpoints) == 0 {
-		for _, ip := range e.endpointKeysByServiceAndSlice[hostname][slice] {
-			delete(e.endpointByKey, ip)
-		}
-		delete(e.endpointKeysByServiceAndSlice[hostname], slice)
-	}
-	if _, f := e.endpointKeysByServiceAndSlice[hostname]; !f {
-		e.endpointKeysByServiceAndSlice[hostname] = make(map[string][]endpointKey)
-	}
-	keys := make([]endpointKey, 0, len(endpoints))
-	for _, ep := range endpoints {
-		key := endpointKey{ep.Address, ep.ServicePortName}
-		keys = append(keys, key)
-		// We will always overwrite. A conflict here means an endpoint is transitioning
-		// from one slice to another See
-		// https://github.com/kubernetes/website/blob/master/content/en/docs/concepts/services-networking/endpoint-slices.md#duplicate-endpoints
-		// In this case, we can always assume and update is fresh, although older slices
-		// we have not gotten updates may be stale; therefor we always take the new
-		// update.
-		e.endpointByKey[key] = ep
-	}
-	e.endpointKeysByServiceAndSlice[hostname][slice] = keys
+		delete(e.endpointsByServiceAndSlice[hostname], slice)
+	}
+	if _, f := e.endpointsByServiceAndSlice[hostname]; !f {
+		e.endpointsByServiceAndSlice[hostname] = make(map[string][]*model.IstioEndpoint)
+	}
+	// We will always overwrite. A conflict here means an endpoint is transitioning
+	// from one slice to another See
+	// https://github.com/kubernetes/website/blob/master/content/en/docs/concepts/services-networking/endpoint-slices.md#duplicate-endpoints
+	// In this case, we can always assume and update is fresh, although older slices
+	// we have not gotten updates may be stale; therefor we always take the new
+	// update.
+	e.endpointsByServiceAndSlice[hostname][slice] = endpoints
 }
 
 func (e *endpointSliceCache) Delete(hostname host.Name, slice string) {
 	e.mu.Lock()
 	defer e.mu.Unlock()
 
-	delete(e.endpointKeysByServiceAndSlice[hostname], slice)
-	if len(e.endpointKeysByServiceAndSlice[hostname]) == 0 {
-		delete(e.endpointKeysByServiceAndSlice, hostname)
+	delete(e.endpointsByServiceAndSlice[hostname], slice)
+	if len(e.endpointsByServiceAndSlice[hostname]) == 0 {
+		delete(e.endpointsByServiceAndSlice, hostname)
 	}
 }
 
@@ -434,15 +423,16 @@ func (e *endpointSliceCache) Get(hostname host.Name) []*model.IstioEndpoint {
 	defer e.mu.RUnlock()
 	var endpoints []*model.IstioEndpoint
 	found := map[endpointKey]struct{}{}
-	for _, keys := range e.endpointKeysByServiceAndSlice[hostname] {
-		for _, key := range keys {
+	for _, eps := range e.endpointsByServiceAndSlice[hostname] {
+		for _, ep := range eps {
+			key := endpointKey{ep.Address, ep.ServicePortName}
 			if _, f := found[key]; f {
 				// This a duplicate. Update() already handles conflict resolution, so we don't
 				// need to pick the "right" one here.
 				continue
 			}
 			found[key] = struct{}{}
-			endpoints = append(endpoints, e.endpointByKey[key])
+			endpoints = append(endpoints, ep)
 		}
 	}
 	return endpoints
diff --git a/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go b/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
index 8a35ac2c5a..3b687456ca 100644
--- a/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
+++ b/pilot/pkg/serviceregistry/kube/controller/endpointslice_test.go
@@ -23,7 +23,9 @@
 	mcs "sigs.k8s.io/mcs-api/pkg/apis/v1alpha1"
 
 	"istio.io/api/label"
+	"istio.io/istio/pilot/pkg/model"
 	"istio.io/istio/pilot/pkg/serviceregistry/kube"
+	"istio.io/istio/pkg/config/host"
 	"istio.io/istio/pkg/config/labels"
 )
 
@@ -119,3 +121,83 @@ func TestEndpointSliceFromMCSShouldBeIgnored(t *testing.T) {
 		t.Fatalf("should be 0 instances: len(instances) = %v", len(instances))
 	}
 }
+
+func TestEndpointSliceCache(t *testing.T) {
+	cache := newEndpointSliceCache()
+	hostname := host.Name("foo")
+
+	// add a endpoint
+	ep1 := &model.IstioEndpoint{
+		Address:         "1.2.3.4",
+		ServicePortName: "http",
+	}
+	cache.Update(hostname, "slice1", []*model.IstioEndpoint{ep1})
+	if !testEndpointsEqual(cache.Get(hostname), []*model.IstioEndpoint{ep1}) {
+		t.Fatalf("unexpected endpoints")
+	}
+
+	// add a new endpoint
+	ep2 := &model.IstioEndpoint{
+		Address:         "2.3.4.5",
+		ServicePortName: "http",
+	}
+	cache.Update(hostname, "slice1", []*model.IstioEndpoint{ep1, ep2})
+	if !testEndpointsEqual(cache.Get(hostname), []*model.IstioEndpoint{ep1, ep2}) {
+		t.Fatalf("unexpected endpoints")
+	}
+
+	// change service port name
+	ep1 = &model.IstioEndpoint{
+		Address:         "1.2.3.4",
+		ServicePortName: "http2",
+	}
+	ep2 = &model.IstioEndpoint{
+		Address:         "2.3.4.5",
+		ServicePortName: "http2",
+	}
+	cache.Update(hostname, "slice1", []*model.IstioEndpoint{ep1, ep2})
+	if !testEndpointsEqual(cache.Get(hostname), []*model.IstioEndpoint{ep1, ep2}) {
+		t.Fatalf("unexpected endpoints")
+	}
+
+	// add a new slice
+	ep3 := &model.IstioEndpoint{
+		Address:         "3.4.5.6",
+		ServicePortName: "http2",
+	}
+	cache.Update(hostname, "slice2", []*model.IstioEndpoint{ep3})
+	if !testEndpointsEqual(cache.Get(hostname), []*model.IstioEndpoint{ep1, ep2, ep3}) {
+		t.Fatalf("unexpected endpoints")
+	}
+
+	// dedup when transitioning
+	cache.Update(hostname, "slice2", []*model.IstioEndpoint{ep2, ep3})
+	if !testEndpointsEqual(cache.Get(hostname), []*model.IstioEndpoint{ep1, ep2, ep3}) {
+		t.Fatalf("unexpected endpoints")
+	}
+
+	cache.Delete(hostname, "slice1")
+	if !testEndpointsEqual(cache.Get(hostname), []*model.IstioEndpoint{ep2, ep3}) {
+		t.Fatalf("unexpected endpoints")
+	}
+
+	cache.Delete(hostname, "slice2")
+	if cache.Get(hostname) != nil {
+		t.Fatalf("unexpected endpoints")
+	}
+}
+
+func testEndpointsEqual(a, b []*model.IstioEndpoint) bool {
+	if len(a) != len(b) {
+		return false
+	}
+	m1 := make(map[endpointKey]int)
+	m2 := make(map[endpointKey]int)
+	for _, i := range a {
+		m1[endpointKey{i.Address, i.ServicePortName}]++
+	}
+	for _, i := range b {
+		m2[endpointKey{i.Address, i.ServicePortName}]++
+	}
+	return reflect.DeepEqual(m1, m2)
+}
-- 
2.35.3

