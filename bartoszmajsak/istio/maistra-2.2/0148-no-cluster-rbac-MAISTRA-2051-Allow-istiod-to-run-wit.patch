From f6f379c1ffaab2afd519dad39ea2fe4bff6ef51f Mon Sep 17 00:00:00 2001
From: Brad Ison <brad.ison@redhat.com>
Date: Wed, 3 Feb 2021 19:06:51 +0100
Subject: [no-cluster-rbac] MAISTRA-2051: Allow istiod to run without Node
 access (#255)

This is a reimplementation of the following for the Maistra 2.1 /
Istio 1.8 rebase:

 - MAISTRA-1723: Allow disabling NodePort gateway support (#180)
 - Ensure Pilot can run without privileges for reading nodes (#125)

A new istiod flag is added, `disableNodeAccess`, that will disable any
feature relying on access to Node objects.
---
 pilot/cmd/pilot-discovery/app/cmd.go          |  2 ++
 pilot/pkg/bootstrap/configcontroller.go       |  4 +--
 pilot/pkg/config/kube/ingress/status.go       | 12 ++++---
 pilot/pkg/config/kube/ingress/status_test.go  |  2 +-
 pilot/pkg/config/kube/ingressv1/status.go     | 10 ++++--
 .../pkg/config/kube/ingressv1/status_test.go  |  2 +-
 .../kube/controller/controller.go             | 34 ++++++++++++++++---
 7 files changed, 50 insertions(+), 16 deletions(-)

diff --git a/pilot/cmd/pilot-discovery/app/cmd.go b/pilot/cmd/pilot-discovery/app/cmd.go
index 4d4e025969..d5ff9b31e4 100644
--- a/pilot/cmd/pilot-discovery/app/cmd.go
+++ b/pilot/cmd/pilot-discovery/app/cmd.go
@@ -154,6 +154,8 @@ func addFlags(c *cobra.Command) {
 		"The name of the MemberRoll resource")
 	c.PersistentFlags().BoolVar(&serverArgs.RegistryOptions.KubeOptions.EnableCRDScan, "enableCRDScan", true,
 		"Whether to scan CRDs at startup")
+	c.PersistentFlags().BoolVar(&serverArgs.RegistryOptions.KubeOptions.DisableNodeAccess, "disableNodeAccess", false,
+		"Whether to prevent istiod watching Node objects")
 
 	// using address, so it can be configured as localhost:.. (possibly UDS in future)
 	c.PersistentFlags().StringVar(&serverArgs.ServerOptions.HTTPAddr, "httpAddr", ":8080",
diff --git a/pilot/pkg/bootstrap/configcontroller.go b/pilot/pkg/bootstrap/configcontroller.go
index 4514f30f11..007cecd8c4 100644
--- a/pilot/pkg/bootstrap/configcontroller.go
+++ b/pilot/pkg/bootstrap/configcontroller.go
@@ -104,7 +104,7 @@ func (s *Server) initConfigController(args *PilotArgs) error {
 				NewLeaderElection(args.Namespace, args.PodName, leaderelection.IngressController, args.Revision, s.kubeClient).
 				AddRunFunction(func(leaderStop <-chan struct{}) {
 					if ingressV1 {
-						ingressSyncer := ingressv1.NewStatusSyncer(s.environment.Watcher, s.kubeClient)
+						ingressSyncer := ingressv1.NewStatusSyncer(s.environment.Watcher, s.kubeClient, args.RegistryOptions.KubeOptions.DisableNodeAccess)
 						// Start informers again. This fixes the case where informers for namespace do not start,
 						// as we create them only after acquiring the leader lock
 						// Note: stop here should be the overall pilot stop, NOT the leader election stop. We are
@@ -114,7 +114,7 @@ func (s *Server) initConfigController(args *PilotArgs) error {
 						log.Infof("Starting ingress controller")
 						ingressSyncer.Run(leaderStop)
 					} else {
-						ingressSyncer := ingress.NewStatusSyncer(s.environment.Watcher, s.kubeClient)
+						ingressSyncer := ingress.NewStatusSyncer(s.environment.Watcher, s.kubeClient, args.RegistryOptions.KubeOptions.DisableNodeAccess)
 						// Start informers again. This fixes the case where informers for namespace do not start,
 						// as we create them only after acquiring the leader lock
 						// Note: stop here should be the overall pilot stop, NOT the leader election stop. We are
diff --git a/pilot/pkg/config/kube/ingress/status.go b/pilot/pkg/config/kube/ingress/status.go
index 5a7982698a..4e32f401aa 100644
--- a/pilot/pkg/config/kube/ingress/status.go
+++ b/pilot/pkg/config/kube/ingress/status.go
@@ -61,7 +61,7 @@ func (s *StatusSyncer) Run(stopCh <-chan struct{}) {
 }
 
 // NewStatusSyncer creates a new instance
-func NewStatusSyncer(meshHolder mesh.Holder, client kubelib.Client) *StatusSyncer {
+func NewStatusSyncer(meshHolder mesh.Holder, client kubelib.Client, noNodeAccess bool) *StatusSyncer {
 	// as in controller, ingressClassListener can be nil since not supported in k8s version <1.18
 	var ingressClassLister listerv1beta1.IngressClassLister
 	if NetworkingIngressAvailable(client) {
@@ -71,16 +71,20 @@ func NewStatusSyncer(meshHolder mesh.Holder, client kubelib.Client) *StatusSynce
 	// queue requires a time duration for a retry delay after a handler error
 	q := queue.NewQueue(5 * time.Second)
 
-	return &StatusSyncer{
+	s := &StatusSyncer{
 		meshHolder:         meshHolder,
 		client:             client,
 		ingressLister:      client.KubeInformer().Networking().V1beta1().Ingresses().Lister(),
 		podLister:          client.KubeInformer().Core().V1().Pods().Lister(),
 		serviceLister:      client.KubeInformer().Core().V1().Services().Lister(),
-		nodeLister:         client.KubeInformer().Core().V1().Nodes().Lister(),
 		ingressClassLister: ingressClassLister,
 		queue:              q,
 	}
+
+	if !noNodeAccess {
+		s.nodeLister = client.KubeInformer().Core().V1().Nodes().Lister()
+	}
+	return s
 }
 
 func (s *StatusSyncer) onEvent() error {
@@ -193,7 +197,7 @@ func (s *StatusSyncer) runningAddresses(ingressNs string) ([]string, error) {
 
 	for _, pod := range igPods {
 		// only Running pods are valid
-		if pod.Status.Phase != coreV1.PodRunning {
+		if pod.Status.Phase != coreV1.PodRunning || s.nodeLister == nil {
 			continue
 		}
 
diff --git a/pilot/pkg/config/kube/ingress/status_test.go b/pilot/pkg/config/kube/ingress/status_test.go
index c426079d9a..7ada38b5b9 100644
--- a/pilot/pkg/config/kube/ingress/status_test.go
+++ b/pilot/pkg/config/kube/ingress/status_test.go
@@ -115,7 +115,7 @@ func makeStatusSyncer(t *testing.T) *StatusSyncer {
 
 	client := kubelib.NewFakeClient()
 	setupFake(t, client)
-	sync := NewStatusSyncer(fakeMeshHolder("istio-ingress"), client)
+	sync := NewStatusSyncer(fakeMeshHolder("istio-ingress"), client, false)
 	stop := make(chan struct{})
 	client.RunAndWait(stop)
 	t.Cleanup(func() {
diff --git a/pilot/pkg/config/kube/ingressv1/status.go b/pilot/pkg/config/kube/ingressv1/status.go
index 6fff2a9ba8..f3d6e9ec91 100644
--- a/pilot/pkg/config/kube/ingressv1/status.go
+++ b/pilot/pkg/config/kube/ingressv1/status.go
@@ -61,20 +61,24 @@ func (s *StatusSyncer) Run(stopCh <-chan struct{}) {
 }
 
 // NewStatusSyncer creates a new instance
-func NewStatusSyncer(meshHolder mesh.Holder, client kubelib.Client) *StatusSyncer {
+func NewStatusSyncer(meshHolder mesh.Holder, client kubelib.Client, noNodeAccess bool) *StatusSyncer {
 	// queue requires a time duration for a retry delay after a handler error
 	q := queue.NewQueue(5 * time.Second)
 
-	return &StatusSyncer{
+	s := &StatusSyncer{
 		meshHolder:         meshHolder,
 		client:             client,
 		ingressLister:      client.KubeInformer().Networking().V1().Ingresses().Lister(),
 		podLister:          client.KubeInformer().Core().V1().Pods().Lister(),
 		serviceLister:      client.KubeInformer().Core().V1().Services().Lister(),
-		nodeLister:         client.KubeInformer().Core().V1().Nodes().Lister(),
 		ingressClassLister: client.KubeInformer().Networking().V1().IngressClasses().Lister(),
 		queue:              q,
 	}
+
+	if !noNodeAccess {
+		s.nodeLister = client.KubeInformer().Core().V1().Nodes().Lister()
+	}
+	return s
 }
 
 func (s *StatusSyncer) onEvent() error {
diff --git a/pilot/pkg/config/kube/ingressv1/status_test.go b/pilot/pkg/config/kube/ingressv1/status_test.go
index c426079d9a..7ada38b5b9 100644
--- a/pilot/pkg/config/kube/ingressv1/status_test.go
+++ b/pilot/pkg/config/kube/ingressv1/status_test.go
@@ -115,7 +115,7 @@ func makeStatusSyncer(t *testing.T) *StatusSyncer {
 
 	client := kubelib.NewFakeClient()
 	setupFake(t, client)
-	sync := NewStatusSyncer(fakeMeshHolder("istio-ingress"), client)
+	sync := NewStatusSyncer(fakeMeshHolder("istio-ingress"), client, false)
 	stop := make(chan struct{})
 	client.RunAndWait(stop)
 	t.Cleanup(func() {
diff --git a/pilot/pkg/serviceregistry/kube/controller/controller.go b/pilot/pkg/serviceregistry/kube/controller/controller.go
index 2e6ba1aecb..dd15a9312c 100644
--- a/pilot/pkg/serviceregistry/kube/controller/controller.go
+++ b/pilot/pkg/serviceregistry/kube/controller/controller.go
@@ -163,6 +163,12 @@ type Options struct {
 	// that are. If this is set to false, all CRDs defined in the schema must be
 	// present for istiod to function.
 	EnableCRDScan bool
+
+	// DisableNodeAccess determines whether the controller should attempt to
+	// watch and/or list Node objects.  If this is true, some features will not
+	// be available, e.g. NodePort gateways and determining locality information
+	// based on Nodes.
+	DisableNodeAccess bool
 }
 
 func (o Options) GetSyncInterval() time.Duration {
@@ -386,10 +392,12 @@ func NewController(kubeClient kubelib.Client, options Options) *Controller {
 		c.endpoints = newEndpointSliceController(c)
 	}
 
-	// This is for getting the node IPs of a selected set of nodes
-	c.nodeInformer = kubeClient.KubeInformer().Core().V1().Nodes().Informer()
-	c.nodeLister = kubeClient.KubeInformer().Core().V1().Nodes().Lister()
-	c.registerHandlers(c.nodeInformer, "Nodes", c.onNodeEvent, nil)
+	if !options.DisableNodeAccess {
+		// This is for getting the node IPs of a selected set of nodes
+		c.nodeInformer = kubeClient.KubeInformer().Core().V1().Nodes().Informer()
+		c.nodeLister = kubeClient.KubeInformer().Core().V1().Nodes().Lister()
+		c.registerHandlers(c.nodeInformer, "Nodes", c.onNodeEvent, nil)
+	}
 
 	podInformer := filter.NewFilteredSharedIndexInformer(c.opts.DiscoveryNamespacesFilter.Filter, kubeClient.KubeInformer().Core().V1().Pods().Informer())
 	c.pods = newPodCache(c, podInformer, func(key string) {
@@ -760,7 +768,7 @@ func (c *Controller) informersSynced() bool {
 		!c.serviceInformer.HasSynced() ||
 		!c.endpoints.HasSynced() ||
 		!c.pods.informer.HasSynced() ||
-		!c.nodeInformer.HasSynced() ||
+		(c.nodeInformer != nil && !c.nodeInformer.HasSynced()) ||
 		!c.exports.HasSynced() ||
 		!c.imports.HasSynced() {
 		return false
@@ -797,6 +805,9 @@ func (c *Controller) syncSystemNamespace() error {
 }
 
 func (c *Controller) syncNodes() error {
+	if c.nodeInformer == nil {
+		return nil
+	}
 	var err *multierror.Error
 	nodes := c.nodeInformer.GetIndexer().List()
 	log.Debugf("initializing %d nodes", len(nodes))
@@ -892,6 +903,19 @@ func (c *Controller) getPodLocality(pod *v1.Pod) string {
 		return model.GetLocalityLabelOrDefault(pod.Labels[model.LocalityLabel], "")
 	}
 
+	if c.nodeLister == nil {
+		// Maistra compatibility. Try Node labels copied to Pod.
+		region := getLabelValue(pod, NodeRegionLabel, NodeRegionLabelGA)
+		zone := getLabelValue(pod, NodeZoneLabel, NodeZoneLabelGA)
+		subzone := getLabelValue(pod, label.TopologySubzone.Name, "")
+
+		if region == "" && zone == "" && subzone == "" {
+			return ""
+		}
+
+		return region + "/" + zone + "/" + subzone // Format: "%s/%s/%s"
+	}
+
 	// NodeName is set by the scheduler after the pod is created
 	// https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#late-initialization
 	raw, err := c.nodeLister.Get(pod.Spec.NodeName)
-- 
2.35.3

